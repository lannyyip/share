&&&& RUNNING TensorRT.trtexec [TensorRT v8001] # ./trtexec --onnx=my_model.onnx --optShapes=input0:1x1x1024x500 --int8 --calib=calibration.cache --workspace=5000 --verbose
[11/01/2021-22:22:37] [I] === Model Options ===
[11/01/2021-22:22:37] [I] Format: ONNX
[11/01/2021-22:22:37] [I] Model: my_model.onnx
[11/01/2021-22:22:37] [I] Output:
[11/01/2021-22:22:37] [I] === Build Options ===
[11/01/2021-22:22:37] [I] Max batch: explicit
[11/01/2021-22:22:37] [I] Workspace: 5000 MiB
[11/01/2021-22:22:37] [I] minTiming: 1
[11/01/2021-22:22:37] [I] avgTiming: 8
[11/01/2021-22:22:37] [I] Precision: FP32+INT8
[11/01/2021-22:22:37] [I] Calibration: calibration.cache
[11/01/2021-22:22:37] [I] Refit: Disabled
[11/01/2021-22:22:37] [I] Sparsity: Disabled
[11/01/2021-22:22:37] [I] Safe mode: Disabled
[11/01/2021-22:22:37] [I] Restricted mode: Disabled
[11/01/2021-22:22:37] [I] Save engine: 
[11/01/2021-22:22:37] [I] Load engine: 
[11/01/2021-22:22:37] [I] NVTX verbosity: 0
[11/01/2021-22:22:37] [I] Tactic sources: Using default tactic sources
[11/01/2021-22:22:37] [I] timingCacheMode: local
[11/01/2021-22:22:37] [I] timingCacheFile: 
[11/01/2021-22:22:37] [I] Input(s)s format: fp32:CHW
[11/01/2021-22:22:37] [I] Output(s)s format: fp32:CHW
[11/01/2021-22:22:37] [I] Input build shape: input0=1x1x1024x500+1x1x1024x500+1x1x1024x500
[11/01/2021-22:22:37] [I] Input calibration shape: input0=1x1x1024x500+1x1x1024x500+1x1x1024x500
[11/01/2021-22:22:37] [I] === System Options ===
[11/01/2021-22:22:37] [I] Device: 0
[11/01/2021-22:22:37] [I] DLACore: 
[11/01/2021-22:22:37] [I] Plugins:
[11/01/2021-22:22:37] [I] === Inference Options ===
[11/01/2021-22:22:37] [I] Batch: Explicit
[11/01/2021-22:22:37] [I] Input inference shape: input0=1x1x1024x500
[11/01/2021-22:22:37] [I] Iterations: 10
[11/01/2021-22:22:37] [I] Duration: 3s (+ 200ms warm up)
[11/01/2021-22:22:37] [I] Sleep time: 0ms
[11/01/2021-22:22:37] [I] Streams: 1
[11/01/2021-22:22:37] [I] ExposeDMA: Disabled
[11/01/2021-22:22:37] [I] Data transfers: Enabled
[11/01/2021-22:22:37] [I] Spin-wait: Disabled
[11/01/2021-22:22:37] [I] Multithreading: Disabled
[11/01/2021-22:22:37] [I] CUDA Graph: Disabled
[11/01/2021-22:22:37] [I] Separate profiling: Disabled
[11/01/2021-22:22:37] [I] Time Deserialize: Disabled
[11/01/2021-22:22:37] [I] Time Refit: Disabled
[11/01/2021-22:22:37] [I] Skip inference: Disabled
[11/01/2021-22:22:37] [I] Inputs:
[11/01/2021-22:22:37] [I] === Reporting Options ===
[11/01/2021-22:22:37] [I] Verbose: Enabled
[11/01/2021-22:22:37] [I] Averages: 10 inferences
[11/01/2021-22:22:37] [I] Percentile: 99
[11/01/2021-22:22:37] [I] Dump refittable layers:Disabled
[11/01/2021-22:22:37] [I] Dump output: Disabled
[11/01/2021-22:22:37] [I] Profile: Disabled
[11/01/2021-22:22:37] [I] Export timing to JSON file: 
[11/01/2021-22:22:37] [I] Export output to JSON file: 
[11/01/2021-22:22:37] [I] Export profile to JSON file: 
[11/01/2021-22:22:37] [I] 
[11/01/2021-22:22:37] [I] === Device Information ===
[11/01/2021-22:22:37] [I] Selected Device: NVIDIA GeForce RTX 3070
[11/01/2021-22:22:37] [I] Compute Capability: 8.6
[11/01/2021-22:22:37] [I] SMs: 46
[11/01/2021-22:22:37] [I] Compute Clock Rate: 1.755 GHz
[11/01/2021-22:22:37] [I] Device Global Memory: 7973 MiB
[11/01/2021-22:22:37] [I] Shared Memory per SM: 100 KiB
[11/01/2021-22:22:37] [I] Memory Bus Width: 256 bits (ECC disabled)
[11/01/2021-22:22:37] [I] Memory Clock Rate: 7.001 GHz
[11/01/2021-22:22:37] [I] 
[11/01/2021-22:22:37] [I] TensorRT version: 8001
[11/01/2021-22:22:37] [V] [TRT] Registered plugin creator - ::GridAnchor_TRT version 1
[11/01/2021-22:22:37] [V] [TRT] Registered plugin creator - ::GridAnchorRect_TRT version 1
[11/01/2021-22:22:37] [V] [TRT] Registered plugin creator - ::NMS_TRT version 1
[11/01/2021-22:22:37] [V] [TRT] Registered plugin creator - ::Reorg_TRT version 1
[11/01/2021-22:22:37] [V] [TRT] Registered plugin creator - ::Region_TRT version 1
[11/01/2021-22:22:37] [V] [TRT] Registered plugin creator - ::Clip_TRT version 1
[11/01/2021-22:22:37] [V] [TRT] Registered plugin creator - ::LReLU_TRT version 1
[11/01/2021-22:22:37] [V] [TRT] Registered plugin creator - ::PriorBox_TRT version 1
[11/01/2021-22:22:37] [V] [TRT] Registered plugin creator - ::Normalize_TRT version 1
[11/01/2021-22:22:37] [V] [TRT] Registered plugin creator - ::ScatterND version 1
[11/01/2021-22:22:37] [V] [TRT] Registered plugin creator - ::RPROI_TRT version 1
[11/01/2021-22:22:37] [V] [TRT] Registered plugin creator - ::BatchedNMS_TRT version 1
[11/01/2021-22:22:37] [V] [TRT] Registered plugin creator - ::BatchedNMSDynamic_TRT version 1
[11/01/2021-22:22:37] [V] [TRT] Registered plugin creator - ::FlattenConcat_TRT version 1
[11/01/2021-22:22:37] [V] [TRT] Registered plugin creator - ::CropAndResize version 1
[11/01/2021-22:22:37] [V] [TRT] Registered plugin creator - ::DetectionLayer_TRT version 1
[11/01/2021-22:22:37] [V] [TRT] Registered plugin creator - ::EfficientNMS_ONNX_TRT version 1
[11/01/2021-22:22:37] [V] [TRT] Registered plugin creator - ::EfficientNMS_TRT version 1
[11/01/2021-22:22:37] [V] [TRT] Registered plugin creator - ::Proposal version 1
[11/01/2021-22:22:37] [V] [TRT] Registered plugin creator - ::ProposalLayer_TRT version 1
[11/01/2021-22:22:37] [V] [TRT] Registered plugin creator - ::PyramidROIAlign_TRT version 1
[11/01/2021-22:22:37] [V] [TRT] Registered plugin creator - ::ResizeNearest_TRT version 1
[11/01/2021-22:22:37] [V] [TRT] Registered plugin creator - ::Split version 1
[11/01/2021-22:22:37] [V] [TRT] Registered plugin creator - ::SpecialSlice_TRT version 1
[11/01/2021-22:22:37] [V] [TRT] Registered plugin creator - ::InstanceNormalization_TRT version 1
[11/01/2021-22:22:38] [I] [TRT] [MemUsageChange] Init CUDA: CPU +532, GPU +0, now: CPU 539, GPU 598 (MiB)
[11/01/2021-22:22:38] [I] Start parsing network model
[11/01/2021-22:22:38] [I] [TRT] ----------------------------------------------------------------
[11/01/2021-22:22:38] [I] [TRT] Input filename:   my_model.onnx
[11/01/2021-22:22:38] [I] [TRT] ONNX IR version:  0.0.6
[11/01/2021-22:22:38] [I] [TRT] Opset version:    11
[11/01/2021-22:22:38] [I] [TRT] Producer name:    pytorch
[11/01/2021-22:22:38] [I] [TRT] Producer version: 1.7
[11/01/2021-22:22:38] [I] [TRT] Domain:           
[11/01/2021-22:22:38] [I] [TRT] Model version:    0
[11/01/2021-22:22:38] [I] [TRT] Doc string:       
[11/01/2021-22:22:38] [I] [TRT] ----------------------------------------------------------------
[11/01/2021-22:22:38] [V] [TRT] Plugin creator already registered - ::GridAnchor_TRT version 1
[11/01/2021-22:22:38] [V] [TRT] Plugin creator already registered - ::GridAnchorRect_TRT version 1
[11/01/2021-22:22:38] [V] [TRT] Plugin creator already registered - ::NMS_TRT version 1
[11/01/2021-22:22:38] [V] [TRT] Plugin creator already registered - ::Reorg_TRT version 1
[11/01/2021-22:22:38] [V] [TRT] Plugin creator already registered - ::Region_TRT version 1
[11/01/2021-22:22:38] [V] [TRT] Plugin creator already registered - ::Clip_TRT version 1
[11/01/2021-22:22:38] [V] [TRT] Plugin creator already registered - ::LReLU_TRT version 1
[11/01/2021-22:22:38] [V] [TRT] Plugin creator already registered - ::PriorBox_TRT version 1
[11/01/2021-22:22:38] [V] [TRT] Plugin creator already registered - ::Normalize_TRT version 1
[11/01/2021-22:22:38] [V] [TRT] Plugin creator already registered - ::ScatterND version 1
[11/01/2021-22:22:38] [V] [TRT] Plugin creator already registered - ::RPROI_TRT version 1
[11/01/2021-22:22:38] [V] [TRT] Plugin creator already registered - ::BatchedNMS_TRT version 1
[11/01/2021-22:22:38] [V] [TRT] Plugin creator already registered - ::BatchedNMSDynamic_TRT version 1
[11/01/2021-22:22:38] [V] [TRT] Plugin creator already registered - ::FlattenConcat_TRT version 1
[11/01/2021-22:22:38] [V] [TRT] Plugin creator already registered - ::CropAndResize version 1
[11/01/2021-22:22:38] [V] [TRT] Plugin creator already registered - ::DetectionLayer_TRT version 1
[11/01/2021-22:22:38] [V] [TRT] Plugin creator already registered - ::EfficientNMS_ONNX_TRT version 1
[11/01/2021-22:22:38] [V] [TRT] Plugin creator already registered - ::EfficientNMS_TRT version 1
[11/01/2021-22:22:38] [V] [TRT] Plugin creator already registered - ::Proposal version 1
[11/01/2021-22:22:38] [V] [TRT] Plugin creator already registered - ::ProposalLayer_TRT version 1
[11/01/2021-22:22:38] [V] [TRT] Plugin creator already registered - ::PyramidROIAlign_TRT version 1
[11/01/2021-22:22:38] [V] [TRT] Plugin creator already registered - ::ResizeNearest_TRT version 1
[11/01/2021-22:22:38] [V] [TRT] Plugin creator already registered - ::Split version 1
[11/01/2021-22:22:38] [V] [TRT] Plugin creator already registered - ::SpecialSlice_TRT version 1
[11/01/2021-22:22:38] [V] [TRT] Plugin creator already registered - ::InstanceNormalization_TRT version 1
[11/01/2021-22:22:38] [V] [TRT] Adding network input: input0 with dtype: float32, dimensions: (1, 1, 1024, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: input0 for ONNX tensor: input0
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: 240
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: 241
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: 242
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: 243
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: 244
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: 245
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: 246
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: 247
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: 248
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: 249
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: 250
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: 251
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: 252
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: 253
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: 254
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: 255
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: 256
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: 257
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: 258
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Conv1.conv.0.bias
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Conv1.conv.0.weight
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Conv1.conv.3.bias
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Conv1.conv.3.weight
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Conv1.res_con.bias
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Conv1.res_con.weight
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Conv2.conv.0.bias
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Conv2.conv.0.weight
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Conv2.conv.3.bias
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Conv2.conv.3.weight
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Conv3.conv.0.bias
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Conv3.conv.0.weight
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Conv3.conv.3.bias
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Conv3.conv.3.weight
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Conv4.conv.0.bias
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Conv4.conv.0.weight
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Conv4.conv.3.bias
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Conv4.conv.3.weight
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Conv_1x1_1.bias
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Conv_1x1_1.weight
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Conv_1x1_2.bias
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Conv_1x1_2.weight
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Up_conv1.conv.0.bias
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Up_conv1.conv.0.weight
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Up_conv1.conv.3.bias
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Up_conv1.conv.3.weight
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Up_conv1.res_con.bias
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Up_conv1.res_con.weight
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Up_conv2.conv.0.bias
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Up_conv2.conv.0.weight
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Up_conv2.conv.3.bias
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Up_conv2.conv.3.weight
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Up_conv2.res_con.bias
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Up_conv2.res_con.weight
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Up_conv3.conv.0.bias
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Up_conv3.conv.0.weight
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Up_conv3.conv.3.bias
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Up_conv3.conv.3.weight
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Up_conv3.res_con.bias
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Up_conv3.res_con.weight
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Up_conv4.conv.0.bias
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Up_conv4.conv.0.weight
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Up_conv4.conv.3.bias
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Up_conv4.conv.3.weight
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Up_conv4.res_con.bias
[11/01/2021-22:22:38] [V] [TRT] Importing initializer: network.Up_conv4.res_con.weight
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Shape_0 [Shape]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: input0
[11/01/2021-22:22:38] [V] [TRT] Shape_0 [Shape] inputs: [input0 -> (1, 1, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Shape_0 for ONNX node: Shape_0
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 63 for ONNX tensor: 63
[11/01/2021-22:22:38] [V] [TRT] Shape_0 [Shape] outputs: [63 -> (4)[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_1 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_1 [Constant] inputs: 
[11/01/2021-22:22:38] [11/01/2021-22:22:38] [V] [TRT] Constant_1 [Constant] outputs: [64 -> ()[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Gather_2 [Gather]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 63
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 64
[11/01/2021-22:22:38] [V] [TRT] Gather_2 [Gather] inputs: [63 -> (4)[INT32]], [64 -> ()[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: 64 for ONNX node: 64
[11/01/2021-22:22:38] [V] [TRT] Using Gather axis: 0
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Gather_2 for ONNX node: Gather_2
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 65 for ONNX tensor: 65
[11/01/2021-22:22:38] [V] [TRT] Gather_2 [Gather] outputs: [65 -> ()[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Shape_3 [Shape]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: input0
[11/01/2021-22:22:38] [V] [TRT] Shape_3 [Shape] inputs: [input0 -> (1, 1, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Shape_3 for ONNX node: Shape_3
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 66 for ONNX tensor: 66
[11/01/2021-22:22:38] [V] [TRT] Shape_3 [Shape] outputs: [66 -> (4)[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_4 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_4 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_4 [Constant] outputs: [67 -> ()[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Gather_5 [Gather]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 66
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 67
[11/01/2021-22:22:38] [V] [TRT] Gather_5 [Gather] inputs: [66 -> (4)[INT32]], [67 -> ()[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: 67 for ONNX node: 67
[11/01/2021-22:22:38] [V] [TRT] Using Gather axis: 0
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Gather_5 for ONNX node: Gather_5
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 68 for ONNX tensor: 68
[11/01/2021-22:22:38] [V] [TRT] Gather_5 [Gather] outputs: [68 -> ()[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_6 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_6 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_6 [Constant] outputs: [69 -> ()[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Div_7 [Div]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 65
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 69
[11/01/2021-22:22:38] [V] [TRT] Div_7 [Div] inputs: [65 -> ()[INT32]], [69 -> ()[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: 69 for ONNX node: 69
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Div_7 for ONNX node: Div_7
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 70 for ONNX tensor: 70
[11/01/2021-22:22:38] [V] [TRT] Div_7 [Div] outputs: [70 -> ()[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Cast_8 [Cast]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 70
[11/01/2021-22:22:38] [V] [TRT] Cast_8 [Cast] inputs: [70 -> ()[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Casting to type: int32
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Cast_8 for ONNX node: Cast_8
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 71 for ONNX tensor: 71
[11/01/2021-22:22:38] [V] [TRT] Cast_8 [Cast] outputs: [71 -> ()[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Cast_9 [Cast]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 71
[11/01/2021-22:22:38] [V] [TRT] Cast_9 [Cast] inputs: [71 -> ()[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Casting to type: int32
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Cast_9 for ONNX node: Cast_9
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 72 for ONNX tensor: 72
[11/01/2021-22:22:38] [V] [TRT] Cast_9 [Cast] outputs: [72 -> ()[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_10 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_10 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_10 [Constant] outputs: [73 -> ()[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Mul_11 [Mul]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 72
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 73
[11/01/2021-22:22:38] [V] [TRT] Mul_11 [Mul] inputs: [72 -> ()[INT32]], [73 -> ()[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: 73 for ONNX node: 73
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Mul_11 for ONNX node: Mul_11
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 74 for ONNX tensor: 74
[11/01/2021-22:22:38] [V] [TRT] Mul_11 [Mul] outputs: [74 -> ()[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_12 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_12 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_12 [Constant] outputs: [75 -> ()[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Div_13 [Div]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 68
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 75
[11/01/2021-22:22:38] [V] [TRT] Div_13 [Div] inputs: [68 -> ()[INT32]], [75 -> ()[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: 75 for ONNX node: 75
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Div_13 for ONNX node: Div_13
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 76 for ONNX tensor: 76
[11/01/2021-22:22:38] [V] [TRT] Div_13 [Div] outputs: [76 -> ()[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Cast_14 [Cast]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 76
[11/01/2021-22:22:38] [V] [TRT] Cast_14 [Cast] inputs: [76 -> ()[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Casting to type: int32
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Cast_14 for ONNX node: Cast_14
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 77 for ONNX tensor: 77
[11/01/2021-22:22:38] [V] [TRT] Cast_14 [Cast] outputs: [77 -> ()[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Cast_15 [Cast]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 77
[11/01/2021-22:22:38] [V] [TRT] Cast_15 [Cast] inputs: [77 -> ()[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Casting to type: int32
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Cast_15 for ONNX node: Cast_15
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 78 for ONNX tensor: 78
[11/01/2021-22:22:38] [V] [TRT] Cast_15 [Cast] outputs: [78 -> ()[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_16 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_16 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_16 [Constant] outputs: [79 -> ()[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Mul_17 [Mul]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 78
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 79
[11/01/2021-22:22:38] [V] [TRT] Mul_17 [Mul] inputs: [78 -> ()[INT32]], [79 -> ()[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: 79 for ONNX node: 79
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Mul_17 for ONNX node: Mul_17
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 80 for ONNX tensor: 80
[11/01/2021-22:22:38] [V] [TRT] Mul_17 [Mul] outputs: [80 -> ()[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Unsqueeze_18 [Unsqueeze]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 74
[11/01/2021-22:22:38] [V] [TRT] Unsqueeze_18 [Unsqueeze] inputs: [74 -> ()[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Original shape: (), unsqueezing to: (1,)
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Unsqueeze_18 for ONNX node: Unsqueeze_18
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 81 for ONNX tensor: 81
[11/01/2021-22:22:38] [V] [TRT] Unsqueeze_18 [Unsqueeze] outputs: [81 -> (1)[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Unsqueeze_19 [Unsqueeze]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 80
[11/01/2021-22:22:38] [V] [TRT] Unsqueeze_19 [Unsqueeze] inputs: [80 -> ()[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Original shape: (), unsqueezing to: (1,)
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Unsqueeze_19 for ONNX node: Unsqueeze_19
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 82 for ONNX tensor: 82
[11/01/2021-22:22:38] [V] [TRT] Unsqueeze_19 [Unsqueeze] outputs: [82 -> (1)[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Concat_20 [Concat]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 81
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 82
[11/01/2021-22:22:38] [V] [TRT] Concat_20 [Concat] inputs: [81 -> (1)[INT32]], [82 -> (1)[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Concat_20 for ONNX node: Concat_20
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 83 for ONNX tensor: 83
[11/01/2021-22:22:38] [V] [TRT] Concat_20 [Concat] outputs: [83 -> (2)[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_21 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_21 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_21 [Constant] outputs: [84 -> ()[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Shape_22 [Shape]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: input0
[11/01/2021-22:22:38] [V] [TRT] Shape_22 [Shape] inputs: [input0 -> (1, 1, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Shape_22 for ONNX node: Shape_22
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 85 for ONNX tensor: 85
[11/01/2021-22:22:38] [V] [TRT] Shape_22 [Shape] outputs: [85 -> (4)[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_23 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_23 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_23 [Constant] outputs: [86 -> (1)[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_24 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_24 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_24 [Constant] outputs: [87 -> (1)[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_25 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_25 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_25 [Constant] outputs: [88 -> (1)[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Slice_26 [Slice]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 85
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 87
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 88
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 86
[11/01/2021-22:22:38] [V] [TRT] Slice_26 [Slice] inputs: [85 -> (4)[INT32]], [87 -> (1)[INT32]], [88 -> (1)[INT32]], [86 -> (1)[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Slice_26 for ONNX node: Slice_26
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 89 for ONNX tensor: 89
[11/01/2021-22:22:38] [V] [TRT] Slice_26 [Slice] outputs: [89 -> (2)[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Cast_27 [Cast]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 83
[11/01/2021-22:22:38] [V] [TRT] Cast_27 [Cast] inputs: [83 -> (2)[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Casting to type: int32
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Cast_27 for ONNX node: Cast_27
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 90 for ONNX tensor: 90
[11/01/2021-22:22:38] [V] [TRT] Cast_27 [Cast] outputs: [90 -> (2)[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Concat_28 [Concat]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 89
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 90
[11/01/2021-22:22:38] [V] [TRT] Concat_28 [Concat] inputs: [89 -> (2)[INT32]], [90 -> (2)[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Concat_28 for ONNX node: Concat_28
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 91 for ONNX tensor: 91
[11/01/2021-22:22:38] [V] [TRT] Concat_28 [Concat] outputs: [91 -> (4)[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_29 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_29 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_29 [Constant] outputs: [92 -> ()[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Resize_30 [Resize]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: input0
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 84
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 92
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 91
[11/01/2021-22:22:38] [V] [TRT] Resize_30 [Resize] inputs: [input0 -> (1, 1, 1024, -1)[FLOAT]], [84 -> ()[FLOAT]], [92 -> ()[FLOAT]], [91 -> (4)[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Resize_30 for ONNX node: Resize_30
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 93 for ONNX tensor: 93
[11/01/2021-22:22:38] [V] [TRT] Resize_30 [Resize] outputs: [93 -> (1, 1, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Conv_31 [Conv]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 93
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Conv1.res_con.weight
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Conv1.res_con.bias
[11/01/2021-22:22:38] [V] [TRT] Conv_31 [Conv] inputs: [93 -> (1, 1, 1024, -1)[FLOAT]], [network.Conv1.res_con.weight -> (64, 1, 1, 1)[FLOAT]], [network.Conv1.res_con.bias -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Convolution input dimensions: (1, 1, 1024, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Conv_31 for ONNX node: Conv_31
[11/01/2021-22:22:38] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 64
[11/01/2021-22:22:38] [V] [TRT] Convolution output dimensions: (1, 64, 1024, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 94 for ONNX tensor: 94
[11/01/2021-22:22:38] [V] [TRT] Conv_31 [Conv] outputs: [94 -> (1, 64, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Conv_32 [Conv]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 93
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Conv1.conv.0.weight
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Conv1.conv.0.bias
[11/01/2021-22:22:38] [V] [TRT] Conv_32 [Conv] inputs: [93 -> (1, 1, 1024, -1)[FLOAT]], [network.Conv1.conv.0.weight -> (64, 1, 3, 3)[FLOAT]], [network.Conv1.conv.0.bias -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Convolution input dimensions: (1, 1, 1024, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Conv_32 for ONNX node: Conv_32
[11/01/2021-22:22:38] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[11/01/2021-22:22:38] [V] [TRT] Convolution output dimensions: (1, 64, 1024, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 95 for ONNX tensor: 95
[11/01/2021-22:22:38] [V] [TRT] Conv_32 [Conv] outputs: [95 -> (1, 64, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_33 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_33 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_33 [Constant] outputs: [96 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_34 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_34 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_34 [Constant] outputs: [97 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: InstanceNormalization_35 [InstanceNormalization]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 95
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 96
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 97
[11/01/2021-22:22:38] [V] [TRT] InstanceNormalization_35 [InstanceNormalization] inputs: [95 -> (1, 64, 1024, -1)[FLOAT]], [96 -> (64)[FLOAT]], [97 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: InstanceNormalization_35 for ONNX node: InstanceNormalization_35
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 98 for ONNX tensor: 98
[11/01/2021-22:22:38] [V] [TRT] InstanceNormalization_35 [InstanceNormalization] outputs: [98 -> (1, 64, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: PRelu_36 [PRelu]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 98
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 240
[11/01/2021-22:22:38] [V] [TRT] PRelu_36 [PRelu] inputs: [98 -> (1, 64, 1024, -1)[FLOAT]], [240 -> (1, 1, 1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: 240 for ONNX node: 240
[11/01/2021-22:22:38] [V] [TRT] Registering layer: PRelu_36 for ONNX node: PRelu_36
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 100 for ONNX tensor: 100
[11/01/2021-22:22:38] [V] [TRT] PRelu_36 [PRelu] outputs: [100 -> (1, 64, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Conv_37 [Conv]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 100
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Conv1.conv.3.weight
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Conv1.conv.3.bias
[11/01/2021-22:22:38] [V] [TRT] Conv_37 [Conv] inputs: [100 -> (1, 64, 1024, -1)[FLOAT]], [network.Conv1.conv.3.weight -> (64, 64, 3, 3)[FLOAT]], [network.Conv1.conv.3.bias -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Convolution input dimensions: (1, 64, 1024, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Conv_37 for ONNX node: Conv_37
[11/01/2021-22:22:38] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[11/01/2021-22:22:38] [V] [TRT] Convolution output dimensions: (1, 64, 1024, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 101 for ONNX tensor: 101
[11/01/2021-22:22:38] [V] [TRT] Conv_37 [Conv] outputs: [101 -> (1, 64, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_38 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_38 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_38 [Constant] outputs: [102 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_39 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_39 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_39 [Constant] outputs: [103 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: InstanceNormalization_40 [InstanceNormalization]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 101
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 102
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 103
[11/01/2021-22:22:38] [V] [TRT] InstanceNormalization_40 [InstanceNormalization] inputs: [101 -> (1, 64, 1024, -1)[FLOAT]], [102 -> (64)[FLOAT]], [103 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: InstanceNormalization_40 for ONNX node: InstanceNormalization_40
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 104 for ONNX tensor: 104
[11/01/2021-22:22:38] [V] [TRT] InstanceNormalization_40 [InstanceNormalization] outputs: [104 -> (1, 64, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Add_41 [Add]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 94
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 104
[11/01/2021-22:22:38] [V] [TRT] Add_41 [Add] inputs: [94 -> (1, 64, 1024, -1)[FLOAT]], [104 -> (1, 64, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Add_41 for ONNX node: Add_41
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 105 for ONNX tensor: 105
[11/01/2021-22:22:38] [V] [TRT] Add_41 [Add] outputs: [105 -> (1, 64, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: PRelu_42 [PRelu]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 105
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 241
[11/01/2021-22:22:38] [V] [TRT] PRelu_42 [PRelu] inputs: [105 -> (1, 64, 1024, -1)[FLOAT]], [241 -> (1, 1, 1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: 241 for ONNX node: 241
[11/01/2021-22:22:38] [V] [TRT] Registering layer: PRelu_42 for ONNX node: PRelu_42
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 107 for ONNX tensor: 107
[11/01/2021-22:22:38] [V] [TRT] PRelu_42 [PRelu] outputs: [107 -> (1, 64, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: MaxPool_43 [MaxPool]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 107
[11/01/2021-22:22:38] [V] [TRT] MaxPool_43 [MaxPool] inputs: [107 -> (1, 64, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: MaxPool_43 for ONNX node: MaxPool_43
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 108 for ONNX tensor: 108
[11/01/2021-22:22:38] [V] [TRT] MaxPool_43 [MaxPool] outputs: [108 -> (1, 64, 512, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Conv_44 [Conv]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 108
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Conv2.conv.0.weight
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Conv2.conv.0.bias
[11/01/2021-22:22:38] [V] [TRT] Conv_44 [Conv] inputs: [108 -> (1, 64, 512, -1)[FLOAT]], [network.Conv2.conv.0.weight -> (64, 64, 3, 3)[FLOAT]], [network.Conv2.conv.0.bias -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Convolution input dimensions: (1, 64, 512, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Conv_44 for ONNX node: Conv_44
[11/01/2021-22:22:38] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[11/01/2021-22:22:38] [V] [TRT] Convolution output dimensions: (1, 64, 512, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 109 for ONNX tensor: 109
[11/01/2021-22:22:38] [V] [TRT] Conv_44 [Conv] outputs: [109 -> (1, 64, 512, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_45 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_45 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_45 [Constant] outputs: [110 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_46 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_46 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_46 [Constant] outputs: [111 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: InstanceNormalization_47 [InstanceNormalization]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 109
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 110
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 111
[11/01/2021-22:22:38] [V] [TRT] InstanceNormalization_47 [InstanceNormalization] inputs: [109 -> (1, 64, 512, -1)[FLOAT]], [110 -> (64)[FLOAT]], [111 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: InstanceNormalization_47 for ONNX node: InstanceNormalization_47
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 112 for ONNX tensor: 112
[11/01/2021-22:22:38] [V] [TRT] InstanceNormalization_47 [InstanceNormalization] outputs: [112 -> (1, 64, 512, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: PRelu_48 [PRelu]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 112
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 242
[11/01/2021-22:22:38] [V] [TRT] PRelu_48 [PRelu] inputs: [112 -> (1, 64, 512, -1)[FLOAT]], [242 -> (1, 1, 1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: 242 for ONNX node: 242
[11/01/2021-22:22:38] [V] [TRT] Registering layer: PRelu_48 for ONNX node: PRelu_48
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 114 for ONNX tensor: 114
[11/01/2021-22:22:38] [V] [TRT] PRelu_48 [PRelu] outputs: [114 -> (1, 64, 512, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Conv_49 [Conv]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 114
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Conv2.conv.3.weight
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Conv2.conv.3.bias
[11/01/2021-22:22:38] [V] [TRT] Conv_49 [Conv] inputs: [114 -> (1, 64, 512, -1)[FLOAT]], [network.Conv2.conv.3.weight -> (64, 64, 3, 3)[FLOAT]], [network.Conv2.conv.3.bias -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Convolution input dimensions: (1, 64, 512, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Conv_49 for ONNX node: Conv_49
[11/01/2021-22:22:38] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[11/01/2021-22:22:38] [V] [TRT] Convolution output dimensions: (1, 64, 512, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 115 for ONNX tensor: 115
[11/01/2021-22:22:38] [V] [TRT] Conv_49 [Conv] outputs: [115 -> (1, 64, 512, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_50 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_50 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_50 [Constant] outputs: [116 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_51 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_51 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_51 [Constant] outputs: [117 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: InstanceNormalization_52 [InstanceNormalization]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 115
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 116
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 117
[11/01/2021-22:22:38] [V] [TRT] InstanceNormalization_52 [InstanceNormalization] inputs: [115 -> (1, 64, 512, -1)[FLOAT]], [116 -> (64)[FLOAT]], [117 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: InstanceNormalization_52 for ONNX node: InstanceNormalization_52
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 118 for ONNX tensor: 118
[11/01/2021-22:22:38] [V] [TRT] InstanceNormalization_52 [InstanceNormalization] outputs: [118 -> (1, 64, 512, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Add_53 [Add]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 108
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 118
[11/01/2021-22:22:38] [V] [TRT] Add_53 [Add] inputs: [108 -> (1, 64, 512, -1)[FLOAT]], [118 -> (1, 64, 512, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Add_53 for ONNX node: Add_53
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 119 for ONNX tensor: 119
[11/01/2021-22:22:38] [V] [TRT] Add_53 [Add] outputs: [119 -> (1, 64, 512, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: PRelu_54 [PRelu]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 119
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 243
[11/01/2021-22:22:38] [V] [TRT] PRelu_54 [PRelu] inputs: [119 -> (1, 64, 512, -1)[FLOAT]], [243 -> (1, 1, 1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: 243 for ONNX node: 243
[11/01/2021-22:22:38] [V] [TRT] Registering layer: PRelu_54 for ONNX node: PRelu_54
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 121 for ONNX tensor: 121
[11/01/2021-22:22:38] [V] [TRT] PRelu_54 [PRelu] outputs: [121 -> (1, 64, 512, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: MaxPool_55 [MaxPool]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 121
[11/01/2021-22:22:38] [V] [TRT] MaxPool_55 [MaxPool] inputs: [121 -> (1, 64, 512, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: MaxPool_55 for ONNX node: MaxPool_55
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 122 for ONNX tensor: 122
[11/01/2021-22:22:38] [V] [TRT] MaxPool_55 [MaxPool] outputs: [122 -> (1, 64, 256, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Conv_56 [Conv]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 122
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Conv3.conv.0.weight
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Conv3.conv.0.bias
[11/01/2021-22:22:38] [V] [TRT] Conv_56 [Conv] inputs: [122 -> (1, 64, 256, -1)[FLOAT]], [network.Conv3.conv.0.weight -> (64, 64, 3, 3)[FLOAT]], [network.Conv3.conv.0.bias -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Convolution input dimensions: (1, 64, 256, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Conv_56 for ONNX node: Conv_56
[11/01/2021-22:22:38] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[11/01/2021-22:22:38] [V] [TRT] Convolution output dimensions: (1, 64, 256, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 123 for ONNX tensor: 123
[11/01/2021-22:22:38] [V] [TRT] Conv_56 [Conv] outputs: [123 -> (1, 64, 256, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_57 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_57 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_57 [Constant] outputs: [124 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_58 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_58 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_58 [Constant] outputs: [125 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: InstanceNormalization_59 [InstanceNormalization]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 123
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 124
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 125
[11/01/2021-22:22:38] [V] [TRT] InstanceNormalization_59 [InstanceNormalization] inputs: [123 -> (1, 64, 256, -1)[FLOAT]], [124 -> (64)[FLOAT]], [125 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: InstanceNormalization_59 for ONNX node: InstanceNormalization_59
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 126 for ONNX tensor: 126
[11/01/2021-22:22:38] [V] [TRT] InstanceNormalization_59 [InstanceNormalization] outputs: [126 -> (1, 64, 256, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: PRelu_60 [PRelu]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 126
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 244
[11/01/2021-22:22:38] [V] [TRT] PRelu_60 [PRelu] inputs: [126 -> (1, 64, 256, -1)[FLOAT]], [244 -> (1, 1, 1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: 244 for ONNX node: 244
[11/01/2021-22:22:38] [V] [TRT] Registering layer: PRelu_60 for ONNX node: PRelu_60
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 128 for ONNX tensor: 128
[11/01/2021-22:22:38] [V] [TRT] PRelu_60 [PRelu] outputs: [128 -> (1, 64, 256, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Conv_61 [Conv]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 128
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Conv3.conv.3.weight
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Conv3.conv.3.bias
[11/01/2021-22:22:38] [V] [TRT] Conv_61 [Conv] inputs: [128 -> (1, 64, 256, -1)[FLOAT]], [network.Conv3.conv.3.weight -> (64, 64, 3, 3)[FLOAT]], [network.Conv3.conv.3.bias -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Convolution input dimensions: (1, 64, 256, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Conv_61 for ONNX node: Conv_61
[11/01/2021-22:22:38] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[11/01/2021-22:22:38] [V] [TRT] Convolution output dimensions: (1, 64, 256, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 129 for ONNX tensor: 129
[11/01/2021-22:22:38] [V] [TRT] Conv_61 [Conv] outputs: [129 -> (1, 64, 256, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_62 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_62 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_62 [Constant] outputs: [130 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_63 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_63 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_63 [Constant] outputs: [131 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: InstanceNormalization_64 [InstanceNormalization]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 129
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 130
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 131
[11/01/2021-22:22:38] [V] [TRT] InstanceNormalization_64 [InstanceNormalization] inputs: [129 -> (1, 64, 256, -1)[FLOAT]], [130 -> (64)[FLOAT]], [131 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: InstanceNormalization_64 for ONNX node: InstanceNormalization_64
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 132 for ONNX tensor: 132
[11/01/2021-22:22:38] [V] [TRT] InstanceNormalization_64 [InstanceNormalization] outputs: [132 -> (1, 64, 256, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Add_65 [Add]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 122
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 132
[11/01/2021-22:22:38] [V] [TRT] Add_65 [Add] inputs: [122 -> (1, 64, 256, -1)[FLOAT]], [132 -> (1, 64, 256, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Add_65 for ONNX node: Add_65
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 133 for ONNX tensor: 133
[11/01/2021-22:22:38] [V] [TRT] Add_65 [Add] outputs: [133 -> (1, 64, 256, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: PRelu_66 [PRelu]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 133
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 245
[11/01/2021-22:22:38] [V] [TRT] PRelu_66 [PRelu] inputs: [133 -> (1, 64, 256, -1)[FLOAT]], [245 -> (1, 1, 1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: 245 for ONNX node: 245
[11/01/2021-22:22:38] [V] [TRT] Registering layer: PRelu_66 for ONNX node: PRelu_66
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 135 for ONNX tensor: 135
[11/01/2021-22:22:38] [V] [TRT] PRelu_66 [PRelu] outputs: [135 -> (1, 64, 256, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: MaxPool_67 [MaxPool]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 135
[11/01/2021-22:22:38] [V] [TRT] MaxPool_67 [MaxPool] inputs: [135 -> (1, 64, 256, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: MaxPool_67 for ONNX node: MaxPool_67
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 136 for ONNX tensor: 136
[11/01/2021-22:22:38] [V] [TRT] MaxPool_67 [MaxPool] outputs: [136 -> (1, 64, 128, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Conv_68 [Conv]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 136
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Conv4.conv.0.weight
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Conv4.conv.0.bias
[11/01/2021-22:22:38] [V] [TRT] Conv_68 [Conv] inputs: [136 -> (1, 64, 128, -1)[FLOAT]], [network.Conv4.conv.0.weight -> (64, 64, 3, 3)[FLOAT]], [network.Conv4.conv.0.bias -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Convolution input dimensions: (1, 64, 128, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Conv_68 for ONNX node: Conv_68
[11/01/2021-22:22:38] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[11/01/2021-22:22:38] [V] [TRT] Convolution output dimensions: (1, 64, 128, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 137 for ONNX tensor: 137
[11/01/2021-22:22:38] [V] [TRT] Conv_68 [Conv] outputs: [137 -> (1, 64, 128, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_69 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_69 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_69 [Constant] outputs: [138 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_70 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_70 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_70 [Constant] outputs: [139 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: InstanceNormalization_71 [InstanceNormalization]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 137
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 138
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 139
[11/01/2021-22:22:38] [V] [TRT] InstanceNormalization_71 [InstanceNormalization] inputs: [137 -> (1, 64, 128, -1)[FLOAT]], [138 -> (64)[FLOAT]], [139 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: InstanceNormalization_71 for ONNX node: InstanceNormalization_71
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 140 for ONNX tensor: 140
[11/01/2021-22:22:38] [V] [TRT] InstanceNormalization_71 [InstanceNormalization] outputs: [140 -> (1, 64, 128, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: PRelu_72 [PRelu]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 140
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 246
[11/01/2021-22:22:38] [V] [TRT] PRelu_72 [PRelu] inputs: [140 -> (1, 64, 128, -1)[FLOAT]], [246 -> (1, 1, 1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: 246 for ONNX node: 246
[11/01/2021-22:22:38] [V] [TRT] Registering layer: PRelu_72 for ONNX node: PRelu_72
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 142 for ONNX tensor: 142
[11/01/2021-22:22:38] [V] [TRT] PRelu_72 [PRelu] outputs: [142 -> (1, 64, 128, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Conv_73 [Conv]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 142
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Conv4.conv.3.weight
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Conv4.conv.3.bias
[11/01/2021-22:22:38] [V] [TRT] Conv_73 [Conv] inputs: [142 -> (1, 64, 128, -1)[FLOAT]], [network.Conv4.conv.3.weight -> (64, 64, 3, 3)[FLOAT]], [network.Conv4.conv.3.bias -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Convolution input dimensions: (1, 64, 128, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Conv_73 for ONNX node: Conv_73
[11/01/2021-22:22:38] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[11/01/2021-22:22:38] [V] [TRT] Convolution output dimensions: (1, 64, 128, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 143 for ONNX tensor: 143
[11/01/2021-22:22:38] [V] [TRT] Conv_73 [Conv] outputs: [143 -> (1, 64, 128, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_74 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_74 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_74 [Constant] outputs: [144 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_75 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_75 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_75 [Constant] outputs: [145 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: InstanceNormalization_76 [InstanceNormalization]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 143
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 144
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 145
[11/01/2021-22:22:38] [V] [TRT] InstanceNormalization_76 [InstanceNormalization] inputs: [143 -> (1, 64, 128, -1)[FLOAT]], [144 -> (64)[FLOAT]], [145 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: InstanceNormalization_76 for ONNX node: InstanceNormalization_76
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 146 for ONNX tensor: 146
[11/01/2021-22:22:38] [V] [TRT] InstanceNormalization_76 [InstanceNormalization] outputs: [146 -> (1, 64, 128, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Add_77 [Add]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 136
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 146
[11/01/2021-22:22:38] [V] [TRT] Add_77 [Add] inputs: [136 -> (1, 64, 128, -1)[FLOAT]], [146 -> (1, 64, 128, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Add_77 for ONNX node: Add_77
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 147 for ONNX tensor: 147
[11/01/2021-22:22:38] [V] [TRT] Add_77 [Add] outputs: [147 -> (1, 64, 128, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: PRelu_78 [PRelu]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 147
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 247
[11/01/2021-22:22:38] [V] [TRT] PRelu_78 [PRelu] inputs: [147 -> (1, 64, 128, -1)[FLOAT]], [247 -> (1, 1, 1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: 247 for ONNX node: 247
[11/01/2021-22:22:38] [V] [TRT] Registering layer: PRelu_78 for ONNX node: PRelu_78
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 149 for ONNX tensor: 149
[11/01/2021-22:22:38] [V] [TRT] PRelu_78 [PRelu] outputs: [149 -> (1, 64, 128, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_79 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_79 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_79 [Constant] outputs: [153 -> ()[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Resize_80 [Resize]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 149
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 153
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 248
[11/01/2021-22:22:38] [V] [TRT] Resize_80 [Resize] inputs: [149 -> (1, 64, 128, -1)[FLOAT]], [153 -> ()[FLOAT]], [248 -> (4)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Resize_80 for ONNX node: Resize_80
[11/01/2021-22:22:38] [V] [TRT] Running resize layer with: 
Transformation mode: asymmetric
Resize mode: nearest

[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 154 for ONNX tensor: 154
[11/01/2021-22:22:38] [V] [TRT] Resize_80 [Resize] outputs: [154 -> (1, 64, 256, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Concat_81 [Concat]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 135
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 154
[11/01/2021-22:22:38] [V] [TRT] Concat_81 [Concat] inputs: [135 -> (1, 64, 256, -1)[FLOAT]], [154 -> (1, 64, 256, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Concat_81 for ONNX node: Concat_81
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 155 for ONNX tensor: 155
[11/01/2021-22:22:38] [V] [TRT] Concat_81 [Concat] outputs: [155 -> (1, 128, 256, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Conv_82 [Conv]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 155
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Up_conv4.res_con.weight
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Up_conv4.res_con.bias
[11/01/2021-22:22:38] [V] [TRT] Conv_82 [Conv] inputs: [155 -> (1, 128, 256, -1)[FLOAT]], [network.Up_conv4.res_con.weight -> (64, 128, 1, 1)[FLOAT]], [network.Up_conv4.res_con.bias -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Convolution input dimensions: (1, 128, 256, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Conv_82 for ONNX node: Conv_82
[11/01/2021-22:22:38] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 64
[11/01/2021-22:22:38] [V] [TRT] Convolution output dimensions: (1, 64, 256, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 156 for ONNX tensor: 156
[11/01/2021-22:22:38] [V] [TRT] Conv_82 [Conv] outputs: [156 -> (1, 64, 256, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Conv_83 [Conv]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 155
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Up_conv4.conv.0.weight
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Up_conv4.conv.0.bias
[11/01/2021-22:22:38] [V] [TRT] Conv_83 [Conv] inputs: [155 -> (1, 128, 256, -1)[FLOAT]], [network.Up_conv4.conv.0.weight -> (64, 128, 3, 3)[FLOAT]], [network.Up_conv4.conv.0.bias -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Convolution input dimensions: (1, 128, 256, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Conv_83 for ONNX node: Conv_83
[11/01/2021-22:22:38] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[11/01/2021-22:22:38] [V] [TRT] Convolution output dimensions: (1, 64, 256, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 157 for ONNX tensor: 157
[11/01/2021-22:22:38] [V] [TRT] Conv_83 [Conv] outputs: [157 -> (1, 64, 256, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_84 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_84 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_84 [Constant] outputs: [158 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_85 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_85 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_85 [Constant] outputs: [159 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: InstanceNormalization_86 [InstanceNormalization]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 157
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 158
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 159
[11/01/2021-22:22:38] [V] [TRT] InstanceNormalization_86 [InstanceNormalization] inputs: [157 -> (1, 64, 256, -1)[FLOAT]], [158 -> (64)[FLOAT]], [159 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: InstanceNormalization_86 for ONNX node: InstanceNormalization_86
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 160 for ONNX tensor: 160
[11/01/2021-22:22:38] [V] [TRT] InstanceNormalization_86 [InstanceNormalization] outputs: [160 -> (1, 64, 256, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: PRelu_87 [PRelu]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 160
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 249
[11/01/2021-22:22:38] [V] [TRT] PRelu_87 [PRelu] inputs: [160 -> (1, 64, 256, -1)[FLOAT]], [249 -> (1, 1, 1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: 249 for ONNX node: 249
[11/01/2021-22:22:38] [V] [TRT] Registering layer: PRelu_87 for ONNX node: PRelu_87
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 162 for ONNX tensor: 162
[11/01/2021-22:22:38] [V] [TRT] PRelu_87 [PRelu] outputs: [162 -> (1, 64, 256, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Conv_88 [Conv]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 162
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Up_conv4.conv.3.weight
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Up_conv4.conv.3.bias
[11/01/2021-22:22:38] [V] [TRT] Conv_88 [Conv] inputs: [162 -> (1, 64, 256, -1)[FLOAT]], [network.Up_conv4.conv.3.weight -> (64, 64, 3, 3)[FLOAT]], [network.Up_conv4.conv.3.bias -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Convolution input dimensions: (1, 64, 256, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Conv_88 for ONNX node: Conv_88
[11/01/2021-22:22:38] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[11/01/2021-22:22:38] [V] [TRT] Convolution output dimensions: (1, 64, 256, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 163 for ONNX tensor: 163
[11/01/2021-22:22:38] [V] [TRT] Conv_88 [Conv] outputs: [163 -> (1, 64, 256, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_89 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_89 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_89 [Constant] outputs: [164 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_90 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_90 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_90 [Constant] outputs: [165 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: InstanceNormalization_91 [InstanceNormalization]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 163
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 164
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 165
[11/01/2021-22:22:38] [V] [TRT] InstanceNormalization_91 [InstanceNormalization] inputs: [163 -> (1, 64, 256, -1)[FLOAT]], [164 -> (64)[FLOAT]], [165 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: InstanceNormalization_91 for ONNX node: InstanceNormalization_91
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 166 for ONNX tensor: 166
[11/01/2021-22:22:38] [V] [TRT] InstanceNormalization_91 [InstanceNormalization] outputs: [166 -> (1, 64, 256, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Add_92 [Add]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 156
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 166
[11/01/2021-22:22:38] [V] [TRT] Add_92 [Add] inputs: [156 -> (1, 64, 256, -1)[FLOAT]], [166 -> (1, 64, 256, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Add_92 for ONNX node: Add_92
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 167 for ONNX tensor: 167
[11/01/2021-22:22:38] [V] [TRT] Add_92 [Add] outputs: [167 -> (1, 64, 256, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: PRelu_93 [PRelu]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 167
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 250
[11/01/2021-22:22:38] [V] [TRT] PRelu_93 [PRelu] inputs: [167 -> (1, 64, 256, -1)[FLOAT]], [250 -> (1, 1, 1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: 250 for ONNX node: 250
[11/01/2021-22:22:38] [V] [TRT] Registering layer: PRelu_93 for ONNX node: PRelu_93
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 169 for ONNX tensor: 169
[11/01/2021-22:22:38] [V] [TRT] PRelu_93 [PRelu] outputs: [169 -> (1, 64, 256, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_94 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_94 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_94 [Constant] outputs: [173 -> ()[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Resize_95 [Resize]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 169
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 173
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 251
[11/01/2021-22:22:38] [V] [TRT] Resize_95 [Resize] inputs: [169 -> (1, 64, 256, -1)[FLOAT]], [173 -> ()[FLOAT]], [251 -> (4)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Resize_95 for ONNX node: Resize_95
[11/01/2021-22:22:38] [V] [TRT] Running resize layer with: 
Transformation mode: asymmetric
Resize mode: nearest

[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 174 for ONNX tensor: 174
[11/01/2021-22:22:38] [V] [TRT] Resize_95 [Resize] outputs: [174 -> (1, 64, 512, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Concat_96 [Concat]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 121
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 174
[11/01/2021-22:22:38] [V] [TRT] Concat_96 [Concat] inputs: [121 -> (1, 64, 512, -1)[FLOAT]], [174 -> (1, 64, 512, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Concat_96 for ONNX node: Concat_96
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 175 for ONNX tensor: 175
[11/01/2021-22:22:38] [V] [TRT] Concat_96 [Concat] outputs: [175 -> (1, 128, 512, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Conv_97 [Conv]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 175
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Up_conv3.res_con.weight
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Up_conv3.res_con.bias
[11/01/2021-22:22:38] [V] [TRT] Conv_97 [Conv] inputs: [175 -> (1, 128, 512, -1)[FLOAT]], [network.Up_conv3.res_con.weight -> (64, 128, 1, 1)[FLOAT]], [network.Up_conv3.res_con.bias -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Convolution input dimensions: (1, 128, 512, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Conv_97 for ONNX node: Conv_97
[11/01/2021-22:22:38] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 64
[11/01/2021-22:22:38] [V] [TRT] Convolution output dimensions: (1, 64, 512, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 176 for ONNX tensor: 176
[11/01/2021-22:22:38] [V] [TRT] Conv_97 [Conv] outputs: [176 -> (1, 64, 512, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Conv_98 [Conv]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 175
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Up_conv3.conv.0.weight
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Up_conv3.conv.0.bias
[11/01/2021-22:22:38] [V] [TRT] Conv_98 [Conv] inputs: [175 -> (1, 128, 512, -1)[FLOAT]], [network.Up_conv3.conv.0.weight -> (64, 128, 3, 3)[FLOAT]], [network.Up_conv3.conv.0.bias -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Convolution input dimensions: (1, 128, 512, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Conv_98 for ONNX node: Conv_98
[11/01/2021-22:22:38] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[11/01/2021-22:22:38] [V] [TRT] Convolution output dimensions: (1, 64, 512, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 177 for ONNX tensor: 177
[11/01/2021-22:22:38] [V] [TRT] Conv_98 [Conv] outputs: [177 -> (1, 64, 512, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_99 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_99 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_99 [Constant] outputs: [178 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_100 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_100 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_100 [Constant] outputs: [179 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: InstanceNormalization_101 [InstanceNormalization]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 177
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 178
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 179
[11/01/2021-22:22:38] [V] [TRT] InstanceNormalization_101 [InstanceNormalization] inputs: [177 -> (1, 64, 512, -1)[FLOAT]], [178 -> (64)[FLOAT]], [179 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: InstanceNormalization_101 for ONNX node: InstanceNormalization_101
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 180 for ONNX tensor: 180
[11/01/2021-22:22:38] [V] [TRT] InstanceNormalization_101 [InstanceNormalization] outputs: [180 -> (1, 64, 512, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: PRelu_102 [PRelu]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 180
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 252
[11/01/2021-22:22:38] [V] [TRT] PRelu_102 [PRelu] inputs: [180 -> (1, 64, 512, -1)[FLOAT]], [252 -> (1, 1, 1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: 252 for ONNX node: 252
[11/01/2021-22:22:38] [V] [TRT] Registering layer: PRelu_102 for ONNX node: PRelu_102
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 182 for ONNX tensor: 182
[11/01/2021-22:22:38] [V] [TRT] PRelu_102 [PRelu] outputs: [182 -> (1, 64, 512, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Conv_103 [Conv]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 182
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Up_conv3.conv.3.weight
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Up_conv3.conv.3.bias
[11/01/2021-22:22:38] [V] [TRT] Conv_103 [Conv] inputs: [182 -> (1, 64, 512, -1)[FLOAT]], [network.Up_conv3.conv.3.weight -> (64, 64, 3, 3)[FLOAT]], [network.Up_conv3.conv.3.bias -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Convolution input dimensions: (1, 64, 512, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Conv_103 for ONNX node: Conv_103
[11/01/2021-22:22:38] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[11/01/2021-22:22:38] [V] [TRT] Convolution output dimensions: (1, 64, 512, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 183 for ONNX tensor: 183
[11/01/2021-22:22:38] [V] [TRT] Conv_103 [Conv] outputs: [183 -> (1, 64, 512, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_104 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_104 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_104 [Constant] outputs: [184 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_105 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_105 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_105 [Constant] outputs: [185 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: InstanceNormalization_106 [InstanceNormalization]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 183
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 184
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 185
[11/01/2021-22:22:38] [V] [TRT] InstanceNormalization_106 [InstanceNormalization] inputs: [183 -> (1, 64, 512, -1)[FLOAT]], [184 -> (64)[FLOAT]], [185 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: InstanceNormalization_106 for ONNX node: InstanceNormalization_106
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 186 for ONNX tensor: 186
[11/01/2021-22:22:38] [V] [TRT] InstanceNormalization_106 [InstanceNormalization] outputs: [186 -> (1, 64, 512, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Add_107 [Add]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 176
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 186
[11/01/2021-22:22:38] [V] [TRT] Add_107 [Add] inputs: [176 -> (1, 64, 512, -1)[FLOAT]], [186 -> (1, 64, 512, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Add_107 for ONNX node: Add_107
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 187 for ONNX tensor: 187
[11/01/2021-22:22:38] [V] [TRT] Add_107 [Add] outputs: [187 -> (1, 64, 512, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: PRelu_108 [PRelu]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 187
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 253
[11/01/2021-22:22:38] [V] [TRT] PRelu_108 [PRelu] inputs: [187 -> (1, 64, 512, -1)[FLOAT]], [253 -> (1, 1, 1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: 253 for ONNX node: 253
[11/01/2021-22:22:38] [V] [TRT] Registering layer: PRelu_108 for ONNX node: PRelu_108
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 189 for ONNX tensor: 189
[11/01/2021-22:22:38] [V] [TRT] PRelu_108 [PRelu] outputs: [189 -> (1, 64, 512, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_109 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_109 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_109 [Constant] outputs: [193 -> ()[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Resize_110 [Resize]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 189
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 193
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 254
[11/01/2021-22:22:38] [V] [TRT] Resize_110 [Resize] inputs: [189 -> (1, 64, 512, -1)[FLOAT]], [193 -> ()[FLOAT]], [254 -> (4)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Resize_110 for ONNX node: Resize_110
[11/01/2021-22:22:38] [V] [TRT] Running resize layer with: 
Transformation mode: asymmetric
Resize mode: nearest

[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 194 for ONNX tensor: 194
[11/01/2021-22:22:38] [V] [TRT] Resize_110 [Resize] outputs: [194 -> (1, 64, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Concat_111 [Concat]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 107
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 194
[11/01/2021-22:22:38] [V] [TRT] Concat_111 [Concat] inputs: [107 -> (1, 64, 1024, -1)[FLOAT]], [194 -> (1, 64, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Concat_111 for ONNX node: Concat_111
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 195 for ONNX tensor: 195
[11/01/2021-22:22:38] [V] [TRT] Concat_111 [Concat] outputs: [195 -> (1, 128, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Conv_112 [Conv]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 195
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Up_conv2.res_con.weight
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Up_conv2.res_con.bias
[11/01/2021-22:22:38] [V] [TRT] Conv_112 [Conv] inputs: [195 -> (1, 128, 1024, -1)[FLOAT]], [network.Up_conv2.res_con.weight -> (64, 128, 1, 1)[FLOAT]], [network.Up_conv2.res_con.bias -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Convolution input dimensions: (1, 128, 1024, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Conv_112 for ONNX node: Conv_112
[11/01/2021-22:22:38] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 64
[11/01/2021-22:22:38] [V] [TRT] Convolution output dimensions: (1, 64, 1024, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 196 for ONNX tensor: 196
[11/01/2021-22:22:38] [V] [TRT] Conv_112 [Conv] outputs: [196 -> (1, 64, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Conv_113 [Conv]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 195
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Up_conv2.conv.0.weight
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Up_conv2.conv.0.bias
[11/01/2021-22:22:38] [V] [TRT] Conv_113 [Conv] inputs: [195 -> (1, 128, 1024, -1)[FLOAT]], [network.Up_conv2.conv.0.weight -> (64, 128, 3, 3)[FLOAT]], [network.Up_conv2.conv.0.bias -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Convolution input dimensions: (1, 128, 1024, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Conv_113 for ONNX node: Conv_113
[11/01/2021-22:22:38] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[11/01/2021-22:22:38] [V] [TRT] Convolution output dimensions: (1, 64, 1024, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 197 for ONNX tensor: 197
[11/01/2021-22:22:38] [V] [TRT] Conv_113 [Conv] outputs: [197 -> (1, 64, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_114 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_114 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_114 [Constant] outputs: [198 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_115 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_115 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_115 [Constant] outputs: [199 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: InstanceNormalization_116 [InstanceNormalization]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 197
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 198
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 199
[11/01/2021-22:22:38] [V] [TRT] InstanceNormalization_116 [InstanceNormalization] inputs: [197 -> (1, 64, 1024, -1)[FLOAT]], [198 -> (64)[FLOAT]], [199 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: InstanceNormalization_116 for ONNX node: InstanceNormalization_116
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 200 for ONNX tensor: 200
[11/01/2021-22:22:38] [V] [TRT] InstanceNormalization_116 [InstanceNormalization] outputs: [200 -> (1, 64, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: PRelu_117 [PRelu]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 200
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 255
[11/01/2021-22:22:38] [V] [TRT] PRelu_117 [PRelu] inputs: [200 -> (1, 64, 1024, -1)[FLOAT]], [255 -> (1, 1, 1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: 255 for ONNX node: 255
[11/01/2021-22:22:38] [V] [TRT] Registering layer: PRelu_117 for ONNX node: PRelu_117
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 202 for ONNX tensor: 202
[11/01/2021-22:22:38] [V] [TRT] PRelu_117 [PRelu] outputs: [202 -> (1, 64, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Conv_118 [Conv]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 202
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Up_conv2.conv.3.weight
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Up_conv2.conv.3.bias
[11/01/2021-22:22:38] [V] [TRT] Conv_118 [Conv] inputs: [202 -> (1, 64, 1024, -1)[FLOAT]], [network.Up_conv2.conv.3.weight -> (64, 64, 3, 3)[FLOAT]], [network.Up_conv2.conv.3.bias -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Convolution input dimensions: (1, 64, 1024, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Conv_118 for ONNX node: Conv_118
[11/01/2021-22:22:38] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64
[11/01/2021-22:22:38] [V] [TRT] Convolution output dimensions: (1, 64, 1024, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 203 for ONNX tensor: 203
[11/01/2021-22:22:38] [V] [TRT] Conv_118 [Conv] outputs: [203 -> (1, 64, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_119 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_119 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_119 [Constant] outputs: [204 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_120 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_120 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_120 [Constant] outputs: [205 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: InstanceNormalization_121 [InstanceNormalization]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 203
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 204
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 205
[11/01/2021-22:22:38] [V] [TRT] InstanceNormalization_121 [InstanceNormalization] inputs: [203 -> (1, 64, 1024, -1)[FLOAT]], [204 -> (64)[FLOAT]], [205 -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: InstanceNormalization_121 for ONNX node: InstanceNormalization_121
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 206 for ONNX tensor: 206
[11/01/2021-22:22:38] [V] [TRT] InstanceNormalization_121 [InstanceNormalization] outputs: [206 -> (1, 64, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Add_122 [Add]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 196
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 206
[11/01/2021-22:22:38] [V] [TRT] Add_122 [Add] inputs: [196 -> (1, 64, 1024, -1)[FLOAT]], [206 -> (1, 64, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Add_122 for ONNX node: Add_122
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 207 for ONNX tensor: 207
[11/01/2021-22:22:38] [V] [TRT] Add_122 [Add] outputs: [207 -> (1, 64, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: PRelu_123 [PRelu]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 207
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 256
[11/01/2021-22:22:38] [V] [TRT] PRelu_123 [PRelu] inputs: [207 -> (1, 64, 1024, -1)[FLOAT]], [256 -> (1, 1, 1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: 256 for ONNX node: 256
[11/01/2021-22:22:38] [V] [TRT] Registering layer: PRelu_123 for ONNX node: PRelu_123
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 209 for ONNX tensor: 209
[11/01/2021-22:22:38] [V] [TRT] PRelu_123 [PRelu] outputs: [209 -> (1, 64, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Conv_124 [Conv]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 209
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Conv_1x1_1.weight
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Conv_1x1_1.bias
[11/01/2021-22:22:38] [V] [TRT] Conv_124 [Conv] inputs: [209 -> (1, 64, 1024, -1)[FLOAT]], [network.Conv_1x1_1.weight -> (64, 64, 1, 1)[FLOAT]], [network.Conv_1x1_1.bias -> (64)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Convolution input dimensions: (1, 64, 1024, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Conv_124 for ONNX node: Conv_124
[11/01/2021-22:22:38] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 64
[11/01/2021-22:22:38] [V] [TRT] Convolution output dimensions: (1, 64, 1024, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 210 for ONNX tensor: 210
[11/01/2021-22:22:38] [V] [TRT] Conv_124 [Conv] outputs: [210 -> (1, 64, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Conv_125 [Conv]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 210
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Up_conv1.res_con.weight
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Up_conv1.res_con.bias
[11/01/2021-22:22:38] [V] [TRT] Conv_125 [Conv] inputs: [210 -> (1, 64, 1024, -1)[FLOAT]], [network.Up_conv1.res_con.weight -> (32, 64, 1, 1)[FLOAT]], [network.Up_conv1.res_con.bias -> (32)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Convolution input dimensions: (1, 64, 1024, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Conv_125 for ONNX node: Conv_125
[11/01/2021-22:22:38] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 32
[11/01/2021-22:22:38] [V] [TRT] Convolution output dimensions: (1, 32, 1024, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 211 for ONNX tensor: 211
[11/01/2021-22:22:38] [V] [TRT] Conv_125 [Conv] outputs: [211 -> (1, 32, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Conv_126 [Conv]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 210
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Up_conv1.conv.0.weight
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Up_conv1.conv.0.bias
[11/01/2021-22:22:38] [V] [TRT] Conv_126 [Conv] inputs: [210 -> (1, 64, 1024, -1)[FLOAT]], [network.Up_conv1.conv.0.weight -> (32, 64, 3, 3)[FLOAT]], [network.Up_conv1.conv.0.bias -> (32)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Convolution input dimensions: (1, 64, 1024, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Conv_126 for ONNX node: Conv_126
[11/01/2021-22:22:38] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 32
[11/01/2021-22:22:38] [V] [TRT] Convolution output dimensions: (1, 32, 1024, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 212 for ONNX tensor: 212
[11/01/2021-22:22:38] [V] [TRT] Conv_126 [Conv] outputs: [212 -> (1, 32, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_127 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_127 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_127 [Constant] outputs: [213 -> (32)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_128 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_128 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_128 [Constant] outputs: [214 -> (32)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: InstanceNormalization_129 [InstanceNormalization]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 212
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 213
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 214
[11/01/2021-22:22:38] [V] [TRT] InstanceNormalization_129 [InstanceNormalization] inputs: [212 -> (1, 32, 1024, -1)[FLOAT]], [213 -> (32)[FLOAT]], [214 -> (32)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: InstanceNormalization_129 for ONNX node: InstanceNormalization_129
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 215 for ONNX tensor: 215
[11/01/2021-22:22:38] [V] [TRT] InstanceNormalization_129 [InstanceNormalization] outputs: [215 -> (1, 32, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: PRelu_130 [PRelu]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 215
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 257
[11/01/2021-22:22:38] [V] [TRT] PRelu_130 [PRelu] inputs: [215 -> (1, 32, 1024, -1)[FLOAT]], [257 -> (1, 1, 1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: 257 for ONNX node: 257
[11/01/2021-22:22:38] [V] [TRT] Registering layer: PRelu_130 for ONNX node: PRelu_130
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 217 for ONNX tensor: 217
[11/01/2021-22:22:38] [V] [TRT] PRelu_130 [PRelu] outputs: [217 -> (1, 32, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Conv_131 [Conv]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 217
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Up_conv1.conv.3.weight
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Up_conv1.conv.3.bias
[11/01/2021-22:22:38] [V] [TRT] Conv_131 [Conv] inputs: [217 -> (1, 32, 1024, -1)[FLOAT]], [network.Up_conv1.conv.3.weight -> (32, 32, 3, 3)[FLOAT]], [network.Up_conv1.conv.3.bias -> (32)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Convolution input dimensions: (1, 32, 1024, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Conv_131 for ONNX node: Conv_131
[11/01/2021-22:22:38] [V] [TRT] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 32
[11/01/2021-22:22:38] [V] [TRT] Convolution output dimensions: (1, 32, 1024, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 218 for ONNX tensor: 218
[11/01/2021-22:22:38] [V] [TRT] Conv_131 [Conv] outputs: [218 -> (1, 32, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_132 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_132 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_132 [Constant] outputs: [219 -> (32)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_133 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_133 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_133 [Constant] outputs: [220 -> (32)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: InstanceNormalization_134 [InstanceNormalization]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 218
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 219
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 220
[11/01/2021-22:22:38] [V] [TRT] InstanceNormalization_134 [InstanceNormalization] inputs: [218 -> (1, 32, 1024, -1)[FLOAT]], [219 -> (32)[FLOAT]], [220 -> (32)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: InstanceNormalization_134 for ONNX node: InstanceNormalization_134
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 221 for ONNX tensor: 221
[11/01/2021-22:22:38] [V] [TRT] InstanceNormalization_134 [InstanceNormalization] outputs: [221 -> (1, 32, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Add_135 [Add]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 211
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 221
[11/01/2021-22:22:38] [V] [TRT] Add_135 [Add] inputs: [211 -> (1, 32, 1024, -1)[FLOAT]], [221 -> (1, 32, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Add_135 for ONNX node: Add_135
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 222 for ONNX tensor: 222
[11/01/2021-22:22:38] [V] [TRT] Add_135 [Add] outputs: [222 -> (1, 32, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: PRelu_136 [PRelu]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 222
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 258
[11/01/2021-22:22:38] [V] [TRT] PRelu_136 [PRelu] inputs: [222 -> (1, 32, 1024, -1)[FLOAT]], [258 -> (1, 1, 1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: 258 for ONNX node: 258
[11/01/2021-22:22:38] [V] [TRT] Registering layer: PRelu_136 for ONNX node: PRelu_136
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 224 for ONNX tensor: 224
[11/01/2021-22:22:38] [V] [TRT] PRelu_136 [PRelu] outputs: [224 -> (1, 32, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Conv_137 [Conv]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 224
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Conv_1x1_2.weight
[11/01/2021-22:22:38] [V] [TRT] Searching for input: network.Conv_1x1_2.bias
[11/01/2021-22:22:38] [V] [TRT] Conv_137 [Conv] inputs: [224 -> (1, 32, 1024, -1)[FLOAT]], [network.Conv_1x1_2.weight -> (1, 32, 1, 1)[FLOAT]], [network.Conv_1x1_2.bias -> (1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Convolution input dimensions: (1, 32, 1024, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Conv_137 for ONNX node: Conv_137
[11/01/2021-22:22:38] [V] [TRT] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 1
[11/01/2021-22:22:38] [V] [TRT] Convolution output dimensions: (1, 1, 1024, -1)
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 225 for ONNX tensor: 225
[11/01/2021-22:22:38] [V] [TRT] Conv_137 [Conv] outputs: [225 -> (1, 1, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Sub_138 [Sub]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 93
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 225
[11/01/2021-22:22:38] [V] [TRT] Sub_138 [Sub] inputs: [93 -> (1, 1, 1024, -1)[FLOAT]], [225 -> (1, 1, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Sub_138 for ONNX node: Sub_138
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 226 for ONNX tensor: 226
[11/01/2021-22:22:38] [V] [TRT] Sub_138 [Sub] outputs: [226 -> (1, 1, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Unsqueeze_139 [Unsqueeze]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 65
[11/01/2021-22:22:38] [V] [TRT] Unsqueeze_139 [Unsqueeze] inputs: [65 -> ()[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Original shape: (), unsqueezing to: (1,)
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Unsqueeze_139 for ONNX node: Unsqueeze_139
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 227 for ONNX tensor: 227
[11/01/2021-22:22:38] [V] [TRT] Unsqueeze_139 [Unsqueeze] outputs: [227 -> (1)[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Unsqueeze_140 [Unsqueeze]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 68
[11/01/2021-22:22:38] [V] [TRT] Unsqueeze_140 [Unsqueeze] inputs: [68 -> ()[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Original shape: (), unsqueezing to: (1,)
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Unsqueeze_140 for ONNX node: Unsqueeze_140
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 228 for ONNX tensor: 228
[11/01/2021-22:22:38] [V] [TRT] Unsqueeze_140 [Unsqueeze] outputs: [228 -> (1)[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Concat_141 [Concat]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 227
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 228
[11/01/2021-22:22:38] [V] [TRT] Concat_141 [Concat] inputs: [227 -> (1)[INT32]], [228 -> (1)[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Concat_141 for ONNX node: Concat_141
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 229 for ONNX tensor: 229
[11/01/2021-22:22:38] [V] [TRT] Concat_141 [Concat] outputs: [229 -> (2)[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_142 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_142 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_142 [Constant] outputs: [230 -> ()[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Shape_143 [Shape]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 226
[11/01/2021-22:22:38] [V] [TRT] Shape_143 [Shape] inputs: [226 -> (1, 1, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Shape_143 for ONNX node: Shape_143
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 231 for ONNX tensor: 231
[11/01/2021-22:22:38] [V] [TRT] Shape_143 [Shape] outputs: [231 -> (4)[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_144 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_144 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_144 [Constant] outputs: [232 -> (1)[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_145 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_145 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_145 [Constant] outputs: [233 -> (1)[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_146 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_146 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_146 [Constant] outputs: [234 -> (1)[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Slice_147 [Slice]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 231
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 233
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 234
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 232
[11/01/2021-22:22:38] [V] [TRT] Slice_147 [Slice] inputs: [231 -> (4)[INT32]], [233 -> (1)[INT32]], [234 -> (1)[INT32]], [232 -> (1)[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Slice_147 for ONNX node: Slice_147
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 235 for ONNX tensor: 235
[11/01/2021-22:22:38] [V] [TRT] Slice_147 [Slice] outputs: [235 -> (2)[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Cast_148 [Cast]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 229
[11/01/2021-22:22:38] [V] [TRT] Cast_148 [Cast] inputs: [229 -> (2)[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Casting to type: int32
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Cast_148 for ONNX node: Cast_148
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 236 for ONNX tensor: 236
[11/01/2021-22:22:38] [V] [TRT] Cast_148 [Cast] outputs: [236 -> (2)[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Concat_149 [Concat]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 235
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 236
[11/01/2021-22:22:38] [V] [TRT] Concat_149 [Concat] inputs: [235 -> (2)[INT32]], [236 -> (2)[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Concat_149 for ONNX node: Concat_149
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: 237 for ONNX tensor: 237
[11/01/2021-22:22:38] [V] [TRT] Concat_149 [Concat] outputs: [237 -> (4)[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Constant_150 [Constant]
[11/01/2021-22:22:38] [V] [TRT] Constant_150 [Constant] inputs: 
[11/01/2021-22:22:38] [V] [TRT] Constant_150 [Constant] outputs: [238 -> ()[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Parsing node: Resize_151 [Resize]
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 226
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 230
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 238
[11/01/2021-22:22:38] [V] [TRT] Searching for input: 237
[11/01/2021-22:22:38] [V] [TRT] Resize_151 [Resize] inputs: [226 -> (1, 1, 1024, -1)[FLOAT]], [230 -> ()[FLOAT]], [238 -> ()[FLOAT]], [237 -> (4)[INT32]], 
[11/01/2021-22:22:38] [V] [TRT] Registering layer: Resize_151 for ONNX node: Resize_151
[11/01/2021-22:22:38] [V] [TRT] Registering tensor: output0_0 for ONNX tensor: output0
[11/01/2021-22:22:38] [V] [TRT] Resize_151 [Resize] outputs: [output0 -> (1, 1, 1024, -1)[FLOAT]], 
[11/01/2021-22:22:38] [V] [TRT] Marking output0_0 as output: output0
[11/01/2021-22:22:38] [I] Finish parsing network model
[11/01/2021-22:22:38] [I] [TRT] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 1225, GPU 1023 (MiB)
[11/01/2021-22:22:38] [I] FP32 and INT8 precisions have been specified - more performance might be enabled by additionally specifying --fp16 or --best
[11/01/2021-22:22:38] [I] [TRT] [MemUsageSnapshot] Builder begin: CPU 1225 MiB, GPU 1023 MiB
[11/01/2021-22:22:38] [V] [TRT] Original: 107 layers
[11/01/2021-22:22:38] [V] [TRT] After dead-layer removal: 107 layers
[11/01/2021-22:22:38] [V] [TRT] After scale fusion: 107 layers
[11/01/2021-22:22:38] [V] [TRT] After vertical fusions: 107 layers
[11/01/2021-22:22:38] [V] [TRT] After final dead-layer removal: 107 layers
[11/01/2021-22:22:38] [V] [TRT] Eliminating concatenation Concat_81
[11/01/2021-22:22:38] [V] [TRT] Generating copy for 135 to 155 because copy elision is disabled for concat.
[11/01/2021-22:22:38] [V] [TRT] Generating copy for 154 to 155 because copy elision is disabled for concat.
[11/01/2021-22:22:38] [V] [TRT] Eliminating concatenation Concat_96
[11/01/2021-22:22:38] [V] [TRT] Generating copy for 121 to 175 because copy elision is disabled for concat.
[11/01/2021-22:22:38] [V] [TRT] Generating copy for 174 to 175 because copy elision is disabled for concat.
[11/01/2021-22:22:38] [V] [TRT] Eliminating concatenation Concat_111
[11/01/2021-22:22:38] [V] [TRT] Generating copy for 107 to 195 because copy elision is disabled for concat.
[11/01/2021-22:22:38] [V] [TRT] Generating copy for 194 to 195 because copy elision is disabled for concat.
[11/01/2021-22:22:38] [V] [TRT] After concat removal: 110 layers
[11/01/2021-22:22:38] [V] [TRT] After tensor merging: 110 layers
[11/01/2021-22:22:38] [I] [TRT] Reading Calibration Cache for calibrator: EntropyCalibration2
[11/01/2021-22:22:38] [I] [TRT] Generated calibration scales using calibration cache. Make sure that calibration cache has latest scales.
[11/01/2021-22:22:38] [I] [TRT] To regenerate calibration cache, please delete the existing one. TensorRT will generate a new calibration cache.
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: input0 scale and zero-point Quantization(scale: {0.00787594,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: 101 scale and zero-point Quantization(scale: {0.00787594,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: 108 scale and zero-point Quantization(scale: {0.0430067,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: 109 scale and zero-point Quantization(scale: {0.130314,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: 112 scale and zero-point Quantization(scale: {0.0646021,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: 115 scale and zero-point Quantization(scale: {0.0633853,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: 122 scale and zero-point Quantization(scale: {0.047405,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: 123 scale and zero-point Quantization(scale: {0.0794215,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: 126 scale and zero-point Quantization(scale: {0.0586894,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: 129 scale and zero-point Quantization(scale: {0.0630733,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: 136 scale and zero-point Quantization(scale: {0.0366824,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: 137 scale and zero-point Quantization(scale: {0.0913664,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: 140 scale and zero-point Quantization(scale: {0.0527293,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: 143 scale and zero-point Quantization(scale: {0.0921628,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: 154 scale and zero-point Quantization(scale: {0.0549096,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: 155 scale and zero-point Quantization(scale: {0.103852,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: 157 scale and zero-point Quantization(scale: {0.0921628,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: (Unnamed Layer* 79) [Constant]_output scale and zero-point Quantization(scale: {0.00151182,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: (Unnamed Layer* 80) [Shuffle]_output scale and zero-point Quantization(scale: {0.00151182,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: 162 scale and zero-point Quantization(scale: {0.0921628,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: 163 scale and zero-point Quantization(scale: {0.0921628,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: 174 scale and zero-point Quantization(scale: {0.0506564,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: 175 scale and zero-point Quantization(scale: {0.0580222,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: 177 scale and zero-point Quantization(scale: {0.0507433,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: (Unnamed Layer* 93) [Constant]_output scale and zero-point Quantization(scale: {0.00181085,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: (Unnamed Layer* 94) [Shuffle]_output scale and zero-point Quantization(scale: {0.00181085,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: 182 scale and zero-point Quantization(scale: {0.0630733,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: 183 scale and zero-point Quantization(scale: {0.0630733,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: 194 scale and zero-point Quantization(scale: {0.0522642,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: 195 scale and zero-point Quantization(scale: {0.0801867,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: 197 scale and zero-point Quantization(scale: {0.073055,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: (Unnamed Layer* 107) [Constant]_output scale and zero-point Quantization(scale: {0.00195244,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: (Unnamed Layer* 108) [Shuffle]_output scale and zero-point Quantization(scale: {0.00195244,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: 202 scale and zero-point Quantization(scale: {0.0633853,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: 203 scale and zero-point Quantization(scale: {0.0633853,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: 210 scale and zero-point Quantization(scale: {0.0530092,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: 211 scale and zero-point Quantization(scale: {0.134363,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: 215 scale and zero-point Quantization(scale: {0.10407,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: 217 scale and zero-point Quantization(scale: {0.10407,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: 218 scale and zero-point Quantization(scale: {0.264837,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: 225 scale and zero-point Quantization(scale: {0.0411201,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: 226 scale and zero-point Quantization(scale: {0.0780024,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] INT8 Inference Tensor scales and zero-points: output0 scale and zero-point Quantization(scale: {0.44438,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] Configuring builder for Int8 Mode completed in 0.0119384 seconds.
[11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [11/01/2021-22:22:38] [V] [TRT] Applying generic optimizations to the graph for inference.
[11/01/2021-22:22:38] [V] [TRT] Original: 107 layers
[11/01/2021-22:22:38] [V] [TRT] After dead-layer removal: 107 layers
[11/01/2021-22:22:38] [V] [TRT] ConstShuffleFusion: Fusing 240 with (Unnamed Layer* 30) [Shuffle]
[11/01/2021-22:22:38] [V] [TRT] ConstShuffleFusion: Fusing 241 with (Unnamed Layer* 36) [Shuffle]
[11/01/2021-22:22:38] [V] [TRT] ConstShuffleFusion: Fusing 242 with (Unnamed Layer* 42) [Shuffle]
[11/01/2021-22:22:38] [V] [TRT] ConstShuffleFusion: Fusing 243 with (Unnamed Layer* 48) [Shuffle]
[11/01/2021-22:22:38] [V] [TRT] ConstShuffleFusion: Fusing 244 with (Unnamed Layer* 54) [Shuffle]
[11/01/2021-22:22:38] [V] [TRT] ConstShuffleFusion: Fusing 245 with (Unnamed Layer* 60) [Shuffle]
[11/01/2021-22:22:38] [V] [TRT] ConstShuffleFusion: Fusing 246 with (Unnamed Layer* 66) [Shuffle]
[11/01/2021-22:22:38] [V] [TRT] ConstShuffleFusion: Fusing 247 with (Unnamed Layer* 72) [Shuffle]
[11/01/2021-22:22:38] [V] [TRT] ConstShuffleFusion: Fusing 249 with (Unnamed Layer* 80) [Shuffle]
[11/01/2021-22:22:38] [V] [TRT] ConstShuffleFusion: Fusing 250 with (Unnamed Layer* 86) [Shuffle]
[11/01/2021-22:22:38] [V] [TRT] ConstShuffleFusion: Fusing 252 with (Unnamed Layer* 94) [Shuffle]
[11/01/2021-22:22:38] [V] [TRT] ConstShuffleFusion: Fusing 253 with (Unnamed Layer* 100) [Shuffle]
[11/01/2021-22:22:38] [V] [TRT] ConstShuffleFusion: Fusing 255 with (Unnamed Layer* 108) [Shuffle]
[11/01/2021-22:22:38] [V] [TRT] ConstShuffleFusion: Fusing 256 with (Unnamed Layer* 114) [Shuffle]
[11/01/2021-22:22:38] [V] [TRT] ConstShuffleFusion: Fusing 257 with (Unnamed Layer* 121) [Shuffle]
[11/01/2021-22:22:38] [V] [TRT] ConstShuffleFusion: Fusing 258 with (Unnamed Layer* 127) [Shuffle]
[11/01/2021-22:22:38] [V] [TRT] After Myelin optimization: 91 layers
[11/01/2021-22:22:38] [V] [TRT] After scale fusion: 91 layers
[11/01/2021-22:22:38] [V] [TRT] ConvEltwiseSumFusion: Fusing Conv_31 with Add_41
[11/01/2021-22:22:38] [V] [TRT] ConvEltwiseSumFusion: Fusing Conv_82 with Add_92
[11/01/2021-22:22:38] [V] [TRT] ConvEltwiseSumFusion: Fusing Conv_97 with Add_107
[11/01/2021-22:22:38] [V] [TRT] ConvEltwiseSumFusion: Fusing Conv_112 with Add_122
[11/01/2021-22:22:38] [V] [TRT] ConvEltwiseSumFusion: Fusing Conv_125 with Add_135
[11/01/2021-22:22:38] [V] [TRT] PointWiseFusion: Fusing 240 + (Unnamed Layer* 30) [Shuffle] with PRelu_36
[11/01/2021-22:22:38] [V] [TRT] PointWiseFusion: Fusing 241 + (Unnamed Layer* 36) [Shuffle] with PRelu_42
[11/01/2021-22:22:38] [V] [TRT] PointWiseFusion: Fusing 242 + (Unnamed Layer* 42) [Shuffle] with PRelu_48
[11/01/2021-22:22:38] [V] [TRT] PointWiseFusion: Fusing 243 + (Unnamed Layer* 48) [Shuffle] with PRelu_54
[11/01/2021-22:22:38] [V] [TRT] PointWiseFusion: Fusing 244 + (Unnamed Layer* 54) [Shuffle] with PRelu_60
[11/01/2021-22:22:38] [V] [TRT] PointWiseFusion: Fusing 245 + (Unnamed Layer* 60) [Shuffle] with PRelu_66
[11/01/2021-22:22:38] [V] [TRT] PointWiseFusion: Fusing 246 + (Unnamed Layer* 66) [Shuffle] with PRelu_72
[11/01/2021-22:22:38] [V] [TRT] PointWiseFusion: Fusing 247 + (Unnamed Layer* 72) [Shuffle] with PRelu_78
[11/01/2021-22:22:38] [V] [TRT] PointWiseFusion: Fusing 249 + (Unnamed Layer* 80) [Shuffle] with PRelu_87
[11/01/2021-22:22:38] [V] [TRT] PointWiseFusion: Fusing 250 + (Unnamed Layer* 86) [Shuffle] with PRelu_93
[11/01/2021-22:22:38] [V] [TRT] PointWiseFusion: Fusing 252 + (Unnamed Layer* 94) [Shuffle] with PRelu_102
[11/01/2021-22:22:38] [V] [TRT] PointWiseFusion: Fusing 253 + (Unnamed Layer* 100) [Shuffle] with PRelu_108
[11/01/2021-22:22:38] [V] [TRT] PointWiseFusion: Fusing 255 + (Unnamed Layer* 108) [Shuffle] with PRelu_117
[11/01/2021-22:22:38] [V] [TRT] PointWiseFusion: Fusing 256 + (Unnamed Layer* 114) [Shuffle] with PRelu_123
[11/01/2021-22:22:38] [V] [TRT] PointWiseFusion: Fusing 257 + (Unnamed Layer* 121) [Shuffle] with PRelu_130
[11/01/2021-22:22:38] [V] [TRT] PointWiseFusion: Fusing 258 + (Unnamed Layer* 127) [Shuffle] with PRelu_136
[11/01/2021-22:22:38] [V] [TRT] PointWiseFusion: Fusing Add_53 with PWN(243 + (Unnamed Layer* 48) [Shuffle], PRelu_54)
[11/01/2021-22:22:38] [V] [TRT] PointWiseFusion: Fusing Add_65 with PWN(245 + (Unnamed Layer* 60) [Shuffle], PRelu_66)
[11/01/2021-22:22:38] [V] [TRT] PointWiseFusion: Fusing Add_77 with PWN(247 + (Unnamed Layer* 72) [Shuffle], PRelu_78)
[11/01/2021-22:22:38] [V] [TRT] GenericConvActFusionBase: Fusing Conv_82 + Add_92 with PWN(250 + (Unnamed Layer* 86) [Shuffle], PRelu_93)
[11/01/2021-22:22:38] [V] [TRT] GenericConvActFusionBase: Fusing Conv_97 + Add_107 with PWN(253 + (Unnamed Layer* 100) [Shuffle], PRelu_108)
[11/01/2021-22:22:38] [V] [TRT] GenericConvActFusionBase: Fusing Conv_112 + Add_122 with PWN(256 + (Unnamed Layer* 114) [Shuffle], PRelu_123)
[11/01/2021-22:22:38] [V] [TRT] GenericConvActFusionBase: Fusing Conv_125 + Add_135 with PWN(258 + (Unnamed Layer* 127) [Shuffle], PRelu_136)
[11/01/2021-22:22:38] [V] [TRT] After vertical fusions: 63 layers
[11/01/2021-22:22:38] [V] [TRT] After dupe layer removal: 63 layers
[11/01/2021-22:22:38] [V] [TRT] After final dead-layer removal: 63 layers
[11/01/2021-22:22:38] [V] [TRT] After tensor merging: 63 layers
[11/01/2021-22:22:38] [V] [TRT] Eliminating concatenation Concat_81
[11/01/2021-22:22:38] [V] [TRT] Retargeting 135 to 155
[11/01/2021-22:22:38] [V] [TRT] Assigning 135 scale and zero-point 
[11/01/2021-22:22:38] [V] [TRT] using 155 scale and zero-point Quantization(scale: {0.103852,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] Generating copy for 154 to 155 because input does not support striding.
[11/01/2021-22:22:38] [V] [TRT] Assigning 154 copy scale and zero-point 
[11/01/2021-22:22:38] [V] [TRT] using 155 scale and zero-point Quantization(scale: {0.103852,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] Eliminating concatenation Concat_96
[11/01/2021-22:22:38] [V] [TRT] Retargeting 121 to 175
[11/01/2021-22:22:38] [V] [TRT] Assigning 121 scale and zero-point 
[11/01/2021-22:22:38] [V] [TRT] using 175 scale and zero-point Quantization(scale: {0.0580222,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] Generating copy for 174 to 175 because input does not support striding.
[11/01/2021-22:22:38] [V] [TRT] Assigning 174 copy scale and zero-point 
[11/01/2021-22:22:38] [V] [TRT] using 175 scale and zero-point Quantization(scale: {0.0580222,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] Eliminating concatenation Concat_111
[11/01/2021-22:22:38] [V] [TRT] Retargeting 107 to 195
[11/01/2021-22:22:38] [V] [TRT] Assigning 107 scale and zero-point 
[11/01/2021-22:22:38] [V] [TRT] using 195 scale and zero-point Quantization(scale: {0.0801867,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] Generating copy for 194 to 195 because input does not support striding.
[11/01/2021-22:22:38] [V] [TRT] Assigning 194 copy scale and zero-point 
[11/01/2021-22:22:38] [V] [TRT] using 195 scale and zero-point Quantization(scale: {0.0801867,}, zero-point: {})
[11/01/2021-22:22:38] [V] [TRT] After concat removal: 63 layers
[11/01/2021-22:22:38] [V] [TRT] Graph construction and optimization completed in 0.133201 seconds.
[11/01/2021-22:22:39] [V] [TRT] Using cublasLt a tactic source
[11/01/2021-22:22:39] [11/01/2021-22:22:39] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +161, GPU +268, now: CPU 1389, GPU 1470 (MiB)
[11/01/2021-22:22:39] [V] [TRT] Using cuDNN as a tactic source
[11/01/2021-22:22:39] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 1389, GPU 1478 (MiB)
[11/01/2021-22:22:39] [11/01/2021-22:22:39] [V] [TRT] Constructing optimization profile number 0 [1/1].
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer Conv_32 due to missing int8 scales for tensor 93 at input index 0
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer InstanceNormalization_35 due to missing int8 scales for tensor 95 at input index 0
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36) due to missing int8 scales for tensor 98 at input index 0
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer Conv_37 due to missing int8 scales for tensor 100 at input index 0
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer InstanceNormalization_40 due to missing int8 scales for tensor 104 at output index 0
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer Conv_31 + Add_41 due to missing int8 scales for tensor 93 at input index 0
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer PWN(241 + (Unnamed Layer* 36) [Shuffle], PRelu_42) due to missing int8 scales for tensor 105 at input index 0
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer PWN(242 + (Unnamed Layer* 42) [Shuffle], PRelu_48) due to missing int8 scales for tensor 114 at output index 0
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer Conv_49 due to missing int8 scales for tensor 114 at input index 0
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer InstanceNormalization_52 due to missing int8 scales for tensor 118 at output index 0
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer PWN(Add_53, PWN(243 + (Unnamed Layer* 48) [Shuffle], PRelu_54)) due to missing int8 scales for tensor 118 at input index 1
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer PWN(244 + (Unnamed Layer* 54) [Shuffle], PRelu_60) due to missing int8 scales for tensor 128 at output index 0
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer Conv_61 due to missing int8 scales for tensor 128 at input index 0
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer InstanceNormalization_64 due to missing int8 scales for tensor 132 at output index 0
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer PWN(Add_65, PWN(245 + (Unnamed Layer* 60) [Shuffle], PRelu_66)) due to missing int8 scales for tensor 132 at input index 1
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer PWN(246 + (Unnamed Layer* 66) [Shuffle], PRelu_72) due to missing int8 scales for tensor 142 at output index 0
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer Conv_73 due to missing int8 scales for tensor 142 at input index 0
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer InstanceNormalization_76 due to missing int8 scales for tensor 146 at output index 0
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer PWN(Add_77, PWN(247 + (Unnamed Layer* 72) [Shuffle], PRelu_78)) due to missing int8 scales for tensor 146 at input index 1
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer InstanceNormalization_86 due to missing int8 scales for tensor 160 at output index 0
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer PWN(249 + (Unnamed Layer* 80) [Shuffle], PRelu_87) due to missing int8 scales for tensor 160 at input index 0
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer InstanceNormalization_91 due to missing int8 scales for tensor 166 at output index 0
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer Conv_82 + Add_92 + PWN(250 + (Unnamed Layer* 86) [Shuffle], PRelu_93) due to missing int8 scales for tensor 166 at input index 1
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer InstanceNormalization_101 due to missing int8 scales for tensor 180 at output index 0
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer PWN(252 + (Unnamed Layer* 94) [Shuffle], PRelu_102) due to missing int8 scales for tensor 180 at input index 0
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer InstanceNormalization_106 due to missing int8 scales for tensor 186 at output index 0
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer Conv_97 + Add_107 + PWN(253 + (Unnamed Layer* 100) [Shuffle], PRelu_108) due to missing int8 scales for tensor 186 at input index 1
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer InstanceNormalization_116 due to missing int8 scales for tensor 200 at output index 0
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer PWN(255 + (Unnamed Layer* 108) [Shuffle], PRelu_117) due to missing int8 scales for tensor 200 at input index 0
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer InstanceNormalization_121 due to missing int8 scales for tensor 206 at output index 0
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer Conv_112 + Add_122 + PWN(256 + (Unnamed Layer* 114) [Shuffle], PRelu_123) due to missing int8 scales for tensor 206 at input index 1
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer Conv_124 due to missing int8 scales for tensor 209 at input index 0
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer Conv_126 due to missing int8 scales for tensor 212 at output index 0
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer InstanceNormalization_129 due to missing int8 scales for tensor 212 at input index 0
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer InstanceNormalization_134 due to missing int8 scales for tensor 221 at output index 0
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer Conv_125 + Add_135 + PWN(258 + (Unnamed Layer* 127) [Shuffle], PRelu_136) due to missing int8 scales for tensor 221 at input index 1
[11/01/2021-22:22:39] [V] [TRT] Rejecting some int8 implementation of layer Conv_137 due to missing int8 scales for tensor 224 at input index 0
[11/01/2021-22:22:39] [V] [TRT] *************** Autotuning format combination: Float(512000,512000,500,1) -> Float(491520,491520,480,1) ***************
[11/01/2021-22:22:39] [V] [TRT] --------------- Timing Runner: Resize_30 (Resize)
[11/01/2021-22:22:39] [V] [TRT] Setting a default quantization params because quantization data is missing for Resize_30
[11/01/2021-22:22:39] [V] [TRT] Tactic: 0 is the only option, timing skipped
[11/01/2021-22:22:39] [V] [TRT] Fastest Tactic: 0 Time: 0
[11/01/2021-22:22:39] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: Resize Tactic: 0
[11/01/2021-22:22:39] [V] [TRT] *************** Autotuning Reformat:Float(491520,491520,480,1) -> Float(491520,1,480,1) ***************
[11/01/2021-22:22:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:22:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:39] [V] [TRT] Tactic: 1002 Time: 0.011264
[11/01/2021-22:22:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:39] [V] [TRT] Tactic: 0 Time: 0.009216
[11/01/2021-22:22:39] [V] [TRT] Fastest Tactic: 0 Time: 0.009216
[11/01/2021-22:22:39] [V] [TRT] *************** Autotuning Reformat:Float(491520,491520,480,1) -> Float(491520,1:4,480,1) ***************
[11/01/2021-22:22:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:22:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:39] [V] [TRT] Tactic: 1002 Time: 0.141512
[11/01/2021-22:22:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:39] [V] [TRT] Tactic: 0 Time: 0.051996
[11/01/2021-22:22:39] [V] [TRT] Fastest Tactic: 0 Time: 0.051996
[11/01/2021-22:22:39] [V] [TRT] *************** Autotuning Reformat:Float(491520,491520,480,1) -> Float(491520,491520:32,480,1) ***************
[11/01/2021-22:22:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:22:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:39] [V] [TRT] Tactic: 1002 Time: 0.284148
[11/01/2021-22:22:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:39] [V] [TRT] Tactic: 0 Time: 2.1833
[11/01/2021-22:22:39] [V] [TRT] Fastest Tactic: 1002 Time: 0.284148
[11/01/2021-22:22:39] [V] [TRT] *************** Autotuning Reformat:Float(491520,491520,480,1) -> Float(491520,1,480,1) ***************
[11/01/2021-22:22:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:22:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:39] [V] [TRT] Tactic: 1002 Time: 0.011264
[11/01/2021-22:22:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:39] [V] [TRT] Tactic: 0 Time: 0.009216
[11/01/2021-22:22:39] [V] [TRT] Fastest Tactic: 0 Time: 0.009216
[11/01/2021-22:22:39] [V] [TRT] *************** Autotuning Reformat:Float(491520,491520,480,1) -> Float(491520,1:4,480,1) ***************
[11/01/2021-22:22:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:22:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:39] [V] [TRT] Tactic: 1002 Time: 0.141312
[11/01/2021-22:22:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:39] [V] [TRT] Tactic: 0 Time: 0.052096
[11/01/2021-22:22:39] [V] [TRT] Fastest Tactic: 0 Time: 0.052096
[11/01/2021-22:22:39] [V] [TRT] *************** Autotuning Reformat:Float(491520,1,480,1) -> Float(491520,491520,480,1) ***************
[11/01/2021-22:22:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:22:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:39] [V] [TRT] Tactic: 1002 Time: 0.011136
[11/01/2021-22:22:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:39] [V] [TRT] Tactic: 0 Time: 0.009216
[11/01/2021-22:22:39] [V] [TRT] Fastest Tactic: 0 Time: 0.009216
[11/01/2021-22:22:39] [V] [TRT] *************** Autotuning Reformat:Float(491520,1,480,1) -> Float(491520,1:4,480,1) ***************
[11/01/2021-22:22:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:22:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:39] [V] [TRT] Tactic: 1002 Time: 0.16528
[11/01/2021-22:22:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:39] [V] [TRT] Tactic: 0 Time: 0.052104
[11/01/2021-22:22:39] [V] [TRT] Fastest Tactic: 0 Time: 0.052104
[11/01/2021-22:22:39] [V] [TRT] *************** Autotuning Reformat:Float(491520,1:4,480,1) -> Float(491520,491520,480,1) ***************
[11/01/2021-22:22:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:22:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:39] [V] [TRT] Tactic: 1002 Time: 0.282956
[11/01/2021-22:22:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:39] [V] [TRT] Tactic: 0 Time: 0.027768
[11/01/2021-22:22:39] [V] [TRT] Fastest Tactic: 0 Time: 0.027768
[11/01/2021-22:22:39] [V] [TRT] *************** Autotuning Reformat:Float(491520,1:4,480,1) -> Float(491520,1,480,1) ***************
[11/01/2021-22:22:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:22:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:39] [V] [TRT] Tactic: 1002 Time: 0.28278
[11/01/2021-22:22:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:39] [V] [TRT] Tactic: 0 Time: 0.02766
[11/01/2021-22:22:39] [V] [TRT] Fastest Tactic: 0 Time: 0.02766
[11/01/2021-22:22:39] [V] [TRT] *************** Autotuning Reformat:Float(491520,491520:32,480,1) -> Float(491520,491520,480,1) ***************
[11/01/2021-22:22:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:22:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:39] [V] [TRT] Tactic: 1002 Time: 0.28276
[11/01/2021-22:22:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:39] [V] [TRT] Tactic: 0 Time: 0.04692
[11/01/2021-22:22:39] [V] [TRT] Fastest Tactic: 0 Time: 0.04692
[11/01/2021-22:22:39] [V] [TRT] *************** Autotuning Reformat:Float(491520,491520:32,480,1) -> Float(491520,1,480,1) ***************
[11/01/2021-22:22:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:22:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:39] [V] [TRT] Tactic: 1002 Time: 0.282844
[11/01/2021-22:22:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:39] [V] [TRT] Tactic: 0 Time: 0.047092
[11/01/2021-22:22:39] [V] [TRT] Fastest Tactic: 0 Time: 0.047092
[11/01/2021-22:22:39] [V] [TRT] *************** Autotuning Reformat:Float(491520,491520:32,480,1) -> Float(491520,1:4,480,1) ***************
[11/01/2021-22:22:39] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:22:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:39] [V] [TRT] Tactic: 1002 Time: 0.178312
[11/01/2021-22:22:39] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:39] [V] [TRT] Tactic: 0 Time: 0.084152
[11/01/2021-22:22:39] [V] [TRT] Fastest Tactic: 0 Time: 0.084152
[11/01/2021-22:22:39] [V] [TRT] *************** Autotuning format combination: Float(491520,491520,480,1) -> Float(31457280,491520,480,1) ***************
[11/01/2021-22:22:39] [V] [TRT] --------------- Timing Runner: Conv_32 (CudaDepthwiseConvolution)
[11/01/2021-22:22:39] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[11/01/2021-22:22:39] [V] [TRT] --------------- Timing Runner: Conv_32 (FusedConvActConvolution)
[11/01/2021-22:22:39] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[11/01/2021-22:22:39] [V] [TRT] --------------- Timing Runner: Conv_32 (CudnnConvolution)
[11/01/2021-22:22:39] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: 0 Time: 1.13065
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: 1 Time: 0.992304
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: 2 Time: 1.22044
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: 5 Time: 5.70154
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: 6 Time: 1.17899
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: 56 Time: 1.07552
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: 57 Time: 0.985108
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: 58 Time: 1.15944
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: 61 Time: 5.72374
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: 62 Time: 1.17945
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: 112 Time: 1.07603
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: 113 Time: 0.987532
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: 114 Time: 1.15937
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: 117 Time: 5.66181
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: 118 Time: 1.18015
[11/01/2021-22:22:40] [V] [TRT] Fastest Tactic: 57 Time: 0.985108
[11/01/2021-22:22:40] [V] [TRT] --------------- Timing Runner: Conv_32 (CaskConvolution)
[11/01/2021-22:22:40] [V] [TRT] Conv_32 Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 4549827808004681195
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: 4549827808004681195 Time: 0.33858
[11/01/2021-22:22:40] [V] [TRT] Conv_32 Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 5779835512569528575
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: 5779835512569528575 Time: 0.501176
[11/01/2021-22:22:40] [V] [TRT] Conv_32 Set Tactic Name: ampere_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 6053873026024413720
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: 6053873026024413720 Time: 0.615844
[11/01/2021-22:22:40] [V] [TRT] Conv_32 Set Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 6767548733843469815
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: 6767548733843469815 Time: 0.343748
[11/01/2021-22:22:40] [V] [TRT] Conv_32 Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: -6313876406580483184
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: -6313876406580483184 Time: 0.34764
[11/01/2021-22:22:40] [V] [TRT] Conv_32 Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: -1123676555321336786
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: -1123676555321336786 Time: 0.501928
[11/01/2021-22:22:40] [V] [TRT] Conv_32 Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: -701551393537224327
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: -701551393537224327 Time: 0.338584
[11/01/2021-22:22:40] [V] [TRT] Fastest Tactic: 4549827808004681195 Time: 0.33858
[11/01/2021-22:22:40] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 4549827808004681195
[11/01/2021-22:22:40] [V] [TRT] *************** Autotuning format combination: Float(491520,1,480,1) -> Float(31457280,1,30720,64) ***************
[11/01/2021-22:22:40] [V] [TRT] --------------- Timing Runner: Conv_32 (CudnnConvolution)
[11/01/2021-22:22:40] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[11/01/2021-22:22:40] [V] [TRT] --------------- Timing Runner: Conv_32 (CaskConvolution)
[11/01/2021-22:22:40] [V] [TRT] Conv_32 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 5778138195697110003
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: 5778138195697110003 Time: 0.763332
[11/01/2021-22:22:40] [V] [TRT] Conv_32 Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: -3855385237722507464
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: -3855385237722507464 Time: 0.774296
[11/01/2021-22:22:40] [V] [TRT] Conv_32 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: -2809379259463049391
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: -2809379259463049391 Time: 0.766288
[11/01/2021-22:22:40] [V] [TRT] Fastest Tactic: 5778138195697110003 Time: 0.763332
[11/01/2021-22:22:40] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 5778138195697110003
[11/01/2021-22:22:40] [V] [TRT] *************** Autotuning format combination: Float(491520,1:4,480,1) -> Float(7864320,1:4,7680,16) ***************
[11/01/2021-22:22:40] [V] [TRT] --------------- Timing Runner: Conv_32 (CudnnConvolution)
[11/01/2021-22:22:40] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[11/01/2021-22:22:40] [V] [TRT] --------------- Timing Runner: Conv_32 (CaskConvolution)
[11/01/2021-22:22:40] [V] [TRT] Conv_32 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 2860655430572478466
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: 2860655430572478466 Time: 0.8192
[11/01/2021-22:22:40] [V] [TRT] Conv_32 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4474630279712975759
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: 4474630279712975759 Time: 1.72724
[11/01/2021-22:22:40] [V] [TRT] Conv_32 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 4479823862704990365
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: 4479823862704990365 Time: 1.70573
[11/01/2021-22:22:40] [V] [TRT] Conv_32 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4696204239951173149
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: 4696204239951173149 Time: 0.825772
[11/01/2021-22:22:40] [V] [TRT] Conv_32 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 5778138195697110003
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: 5778138195697110003 Time: 0.76916
[11/01/2021-22:22:40] [V] [TRT] Conv_32 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 7155825427510256858
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: 7155825427510256858 Time: 0.74832
[11/01/2021-22:22:40] [V] [TRT] Conv_32 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 7342025736444949634
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: 7342025736444949634 Time: 1.12217
[11/01/2021-22:22:40] [V] [TRT] Conv_32 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 8918020581761223752
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: 8918020581761223752 Time: 0.739452
[11/01/2021-22:22:40] [V] [TRT] Conv_32 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: -7377458734869418330
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: -7377458734869418330 Time: 1.10965
[11/01/2021-22:22:40] [V] [TRT] Conv_32 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: -5457304872213719461
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: -5457304872213719461 Time: 1.12587
[11/01/2021-22:22:40] [V] [TRT] Conv_32 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: -4756382386362004279
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:40] [V] [TRT] Tactic: -4756382386362004279 Time: 0.819984
[11/01/2021-22:22:40] [V] [TRT] Conv_32 Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: -3855385237722507464
[11/01/2021-22:22:40] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:41] [V] [TRT] Tactic: -3855385237722507464 Time: 0.779648
[11/01/2021-22:22:41] [V] [TRT] Conv_32 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: -2809379259463049391
[11/01/2021-22:22:41] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:41] [V] [TRT] Tactic: -2809379259463049391 Time: 0.77198
[11/01/2021-22:22:41] [V] [TRT] Conv_32 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: -504296718212024303
[11/01/2021-22:22:41] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_32
[11/01/2021-22:22:41] [V] [TRT] Tactic: -504296718212024303 Time: 0.741248
[11/01/2021-22:22:41] [V] [TRT] Fastest Tactic: 8918020581761223752 Time: 0.739452
[11/01/2021-22:22:41] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 8918020581761223752
[11/01/2021-22:22:41] [V] [TRT] *************** Autotuning Reformat:Float(31457280,1,30720,64) -> Float(31457280,491520,480,1) ***************
[11/01/2021-22:22:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:22:41] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:41] [V] [TRT] Tactic: 1002 Time: 0.62354
[11/01/2021-22:22:41] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:41] [V] [TRT] Tactic: 0 Time: 2.73708
[11/01/2021-22:22:41] [V] [TRT] Fastest Tactic: 1002 Time: 0.62354
[11/01/2021-22:22:41] [V] [TRT] *************** Autotuning Reformat:Float(7864320,1:4,7680,16) -> Float(31457280,491520,480,1) ***************
[11/01/2021-22:22:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:22:41] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:41] [V] [TRT] Tactic: 1002 Time: 0.625208
[11/01/2021-22:22:41] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:41] [V] [TRT] Tactic: 0 Time: 2.738
[11/01/2021-22:22:41] [V] [TRT] Fastest Tactic: 1002 Time: 0.625208
[11/01/2021-22:22:41] [V] [TRT] *************** Autotuning format combination: Float(31457280,491520,480,1) -> Float(31457280,491520,480,1) ***************
[11/01/2021-22:22:41] [V] [TRT] *************** Autotuning Reformat:Float(31457280,491520,480,1) -> Float(31457280,1,30720,64) ***************
[11/01/2021-22:22:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:22:41] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:41] [V] [TRT] Tactic: 1002 Time: 0.628128
[11/01/2021-22:22:41] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:41] [V] [TRT] Tactic: 0 Time: 1.02592
[11/01/2021-22:22:41] [V] [TRT] Fastest Tactic: 1002 Time: 0.628128
[11/01/2021-22:22:41] [V] [TRT] *************** Autotuning Reformat:Float(31457280,491520,480,1) -> Float(7864320,1:4,7680,16) ***************
[11/01/2021-22:22:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:22:41] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:41] [V] [TRT] Tactic: 1002 Time: 0.628216
[11/01/2021-22:22:41] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:41] [V] [TRT] Tactic: 0 Time: 1.01254
[11/01/2021-22:22:41] [V] [TRT] Fastest Tactic: 1002 Time: 0.628216
[11/01/2021-22:22:41] [V] [TRT] *************** Autotuning Reformat:Float(31457280,491520,480,1) -> Float(983040,491520:32,480,1) ***************
[11/01/2021-22:22:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:22:41] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:41] [V] [TRT] Tactic: 1002 Time: 0.627504
[11/01/2021-22:22:41] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:41] [V] [TRT] Tactic: 0 Time: 4.63399
[11/01/2021-22:22:41] [V] [TRT] Fastest Tactic: 1002 Time: 0.627504
[11/01/2021-22:22:41] [V] [TRT] *************** Autotuning Reformat:Float(31457280,491520,480,1) -> Float(31457280,1,30720,64) ***************
[11/01/2021-22:22:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:22:41] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:41] [V] [TRT] Tactic: 1002 Time: 0.628124
[11/01/2021-22:22:41] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:41] [V] [TRT] Tactic: 0 Time: 1.0252
[11/01/2021-22:22:41] [V] [TRT] Fastest Tactic: 1002 Time: 0.628124
[11/01/2021-22:22:41] [V] [TRT] *************** Autotuning Reformat:Float(31457280,491520,480,1) -> Float(7864320,1:4,7680,16) ***************
[11/01/2021-22:22:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:22:41] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:41] [V] [TRT] Tactic: 1002 Time: 0.627968
[11/01/2021-22:22:41] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:41] [V] [TRT] Tactic: 0 Time: 1.01247
[11/01/2021-22:22:41] [V] [TRT] Fastest Tactic: 1002 Time: 0.627968
[11/01/2021-22:22:41] [V] [TRT] *************** Autotuning Reformat:Float(31457280,491520,480,1) -> Float(983040,491520:32,480,1) ***************
[11/01/2021-22:22:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:22:41] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:41] [V] [TRT] Tactic: 1002 Time: 0.628372
[11/01/2021-22:22:41] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:41] [V] [TRT] Tactic: 0 Time: 4.63481
[11/01/2021-22:22:41] [V] [TRT] Fastest Tactic: 1002 Time: 0.628372
[11/01/2021-22:22:41] [V] [TRT] *************** Autotuning Reformat:Float(31457280,1,30720,64) -> Float(31457280,491520,480,1) ***************
[11/01/2021-22:22:41] [V] [TRT] *************** Autotuning Reformat:Float(31457280,1,30720,64) -> Float(7864320,1:4,7680,16) ***************
[11/01/2021-22:22:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:22:41] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:41] [V] [TRT] Tactic: 1002 Time: 0.624552
[11/01/2021-22:22:41] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:41] [V] [TRT] Tactic: 0 Time: 0.61568
[11/01/2021-22:22:41] [V] [TRT] Fastest Tactic: 0 Time: 0.61568
[11/01/2021-22:22:41] [V] [TRT] *************** Autotuning Reformat:Float(31457280,1,30720,64) -> Float(983040,491520:32,480,1) ***************
[11/01/2021-22:22:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:22:41] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:41] [V] [TRT] Tactic: 1002 Time: 0.623744
[11/01/2021-22:22:41] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:41] [V] [TRT] Tactic: 0 Time: 7.71569
[11/01/2021-22:22:41] [V] [TRT] Fastest Tactic: 1002 Time: 0.623744
[11/01/2021-22:22:41] [V] [TRT] *************** Autotuning Reformat:Float(7864320,1:4,7680,16) -> Float(31457280,491520,480,1) ***************
[11/01/2021-22:22:41] [V] [TRT] *************** Autotuning Reformat:Float(7864320,1:4,7680,16) -> Float(31457280,1,30720,64) ***************
[11/01/2021-22:22:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:22:41] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:41] [V] [TRT] Tactic: 1002 Time: 0.624296
[11/01/2021-22:22:41] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:41] [V] [TRT] Tactic: 0 Time: 0.616068
[11/01/2021-22:22:41] [V] [TRT] Fastest Tactic: 0 Time: 0.616068
[11/01/2021-22:22:41] [V] [TRT] *************** Autotuning Reformat:Float(7864320,1:4,7680,16) -> Float(983040,491520:32,480,1) ***************
[11/01/2021-22:22:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:22:41] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:41] [V] [TRT] Tactic: 1002 Time: 0.62464
[11/01/2021-22:22:41] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:41] [V] [TRT] Tactic: 0 Time: 7.7123
[11/01/2021-22:22:41] [V] [TRT] Fastest Tactic: 1002 Time: 0.62464
[11/01/2021-22:22:41] [V] [TRT] *************** Autotuning Reformat:Float(983040,491520:32,480,1) -> Float(31457280,491520,480,1) ***************
[11/01/2021-22:22:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:22:41] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:41] [V] [TRT] Tactic: 1002 Time: 0.626096
[11/01/2021-22:22:41] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:41] [V] [TRT] Tactic: 0 Time: 2.72722
[11/01/2021-22:22:41] [V] [TRT] Fastest Tactic: 1002 Time: 0.626096
[11/01/2021-22:22:41] [V] [TRT] *************** Autotuning Reformat:Float(983040,491520:32,480,1) -> Float(31457280,1,30720,64) ***************
[11/01/2021-22:22:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:22:41] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:41] [V] [TRT] Tactic: 1002 Time: 0.62534
[11/01/2021-22:22:41] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:41] [V] [TRT] Tactic: 0 Time: 0.617736
[11/01/2021-22:22:41] [V] [TRT] Fastest Tactic: 0 Time: 0.617736
[11/01/2021-22:22:41] [V] [TRT] *************** Autotuning Reformat:Float(983040,491520:32,480,1) -> Float(7864320,1:4,7680,16) ***************
[11/01/2021-22:22:41] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:22:41] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:41] [V] [TRT] Tactic: 1002 Time: 0.622908
[11/01/2021-22:22:41] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:22:41] [V] [TRT] Tactic: 0 Time: 0.62156
[11/01/2021-22:22:41] [V] [TRT] Fastest Tactic: 0 Time: 0.62156
[11/01/2021-22:22:41] [V] [TRT] *************** Autotuning format combination: Float(31457280,491520,480,1) -> Float(31457280,491520,480,1) ***************
[11/01/2021-22:22:41] [V] [TRT] --------------- Timing Runner: PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36) (PointWiseV2)
[11/01/2021-22:22:42] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:42] [V] [TRT] Tactic: 0 Time: 0.623284
[11/01/2021-22:22:42] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:42] [V] [TRT] Tactic: 1 Time: 0.621856
[11/01/2021-22:22:42] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:42] [V] [TRT] Tactic: 2 Time: 0.627532
[11/01/2021-22:22:42] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:42] [V] [TRT] Tactic: 3 Time: 0.624644
[11/01/2021-22:22:42] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:42] [V] [TRT] Tactic: 4 Time: 0.63054
[11/01/2021-22:22:43] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:43] [V] [TRT] Tactic: 5 Time: 0.625568
[11/01/2021-22:22:43] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:43] [V] [TRT] Tactic: 6 Time: 0.62244
[11/01/2021-22:22:43] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:43] [V] [TRT] Tactic: 7 Time: 0.62526
[11/01/2021-22:22:43] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:43] [V] [TRT] Tactic: 8 Time: 0.621872
[11/01/2021-22:22:43] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:43] [V] [TRT] Tactic: 9 Time: 0.629128
[11/01/2021-22:22:44] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:44] [V] [TRT] Tactic: 28 Time: 0.621488
[11/01/2021-22:22:44] [V] [TRT] Fastest Tactic: 28 Time: 0.621488
[11/01/2021-22:22:44] [V] [TRT] --------------- Timing Runner: PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36) (PointWise)
[11/01/2021-22:22:44] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:44] [V] [TRT] Tactic: 128 Time: 0.920576
[11/01/2021-22:22:44] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:44] [V] [TRT] Tactic: 256 Time: 0.928164
[11/01/2021-22:22:44] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:44] [V] [TRT] Tactic: 512 Time: 0.968056
[11/01/2021-22:22:44] [V] [TRT] Fastest Tactic: 128 Time: 0.920576
[11/01/2021-22:22:44] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 28
[11/01/2021-22:22:44] [V] [TRT] *************** Autotuning format combination: Float(31457280,1,30720,64) -> Float(31457280,1,30720,64) ***************
[11/01/2021-22:22:44] [V] [TRT] --------------- Timing Runner: PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36) (PointWiseV2)
[11/01/2021-22:22:44] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:44] [V] [TRT] Tactic: 0 Time: 0.620344
[11/01/2021-22:22:44] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:44] [V] [TRT] Tactic: 1 Time: 0.622116
[11/01/2021-22:22:44] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:44] [V] [TRT] Tactic: 2 Time: 0.627368
[11/01/2021-22:22:44] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:44] [V] [TRT] Tactic: 3 Time: 0.625196
[11/01/2021-22:22:44] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:44] [V] [TRT] Tactic: 4 Time: 0.628936
[11/01/2021-22:22:44] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:44] [V] [TRT] Tactic: 5 Time: 0.623928
[11/01/2021-22:22:44] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:44] [V] [TRT] Tactic: 6 Time: 0.621604
[11/01/2021-22:22:44] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:44] [V] [TRT] Tactic: 7 Time: 0.626248
[11/01/2021-22:22:44] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:44] [V] [TRT] Tactic: 8 Time: 0.6234
[11/01/2021-22:22:44] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:44] [V] [TRT] Tactic: 9 Time: 0.626976
[11/01/2021-22:22:44] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:44] [V] [TRT] Tactic: 28 Time: 0.62416
[11/01/2021-22:22:44] [V] [TRT] Fastest Tactic: 0 Time: 0.620344
[11/01/2021-22:22:44] [V] [TRT] --------------- Timing Runner: PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36) (PointWise)
[11/01/2021-22:22:44] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:44] [V] [TRT] Tactic: 128 Time: 0.827228
[11/01/2021-22:22:44] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:44] [V] [TRT] Tactic: 256 Time: 0.836236
[11/01/2021-22:22:44] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:44] [V] [TRT] Tactic: 512 Time: 0.87756
[11/01/2021-22:22:44] [V] [TRT] Fastest Tactic: 128 Time: 0.827228
[11/01/2021-22:22:44] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0
[11/01/2021-22:22:44] [V] [TRT] *************** Autotuning format combination: Float(7864320,1:4,7680,16) -> Float(7864320,1:4,7680,16) ***************
[11/01/2021-22:22:44] [V] [TRT] --------------- Timing Runner: PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36) (PointWiseV2)
[11/01/2021-22:22:44] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:44] [V] [TRT] Tactic: 0 Time: 0.626064
[11/01/2021-22:22:44] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:44] [V] [TRT] Tactic: 1 Time: 0.622324
[11/01/2021-22:22:45] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:45] [V] [TRT] Tactic: 2 Time: 0.62922
[11/01/2021-22:22:45] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:45] [V] [TRT] Tactic: 3 Time: 0.624028
[11/01/2021-22:22:45] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:45] [V] [TRT] Tactic: 4 Time: 0.625524
[11/01/2021-22:22:45] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:45] [V] [TRT] Tactic: 5 Time: 0.6298
[11/01/2021-22:22:45] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:45] [V] [TRT] Tactic: 6 Time: 0.626156
[11/01/2021-22:22:46] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:46] [V] [TRT] Tactic: 7 Time: 0.627592
[11/01/2021-22:22:46] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:46] [V] [TRT] Tactic: 8 Time: 0.625272
[11/01/2021-22:22:46] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:46] [V] [TRT] Tactic: 9 Time: 0.63188
[11/01/2021-22:22:46] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:46] [V] [TRT] Tactic: 10 Time: 0.629416
[11/01/2021-22:22:46] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:47] [V] [TRT] Tactic: 11 Time: 0.630596
[11/01/2021-22:22:47] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:47] [V] [TRT] Tactic: 12 Time: 0.625948
[11/01/2021-22:22:47] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:47] [V] [TRT] Tactic: 13 Time: 0.626364
[11/01/2021-22:22:47] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:47] [V] [TRT] Tactic: 14 Time: 0.624084
[11/01/2021-22:22:47] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:47] [V] [TRT] Tactic: 15 Time: 0.628712
[11/01/2021-22:22:48] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:48] [V] [TRT] Tactic: 16 Time: 0.632124
[11/01/2021-22:22:48] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:48] [V] [TRT] Tactic: 17 Time: 0.626072
[11/01/2021-22:22:48] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:48] [V] [TRT] Tactic: 18 Time: 0.627952
[11/01/2021-22:22:48] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:48] [V] [TRT] Tactic: 19 Time: 0.632604
[11/01/2021-22:22:48] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:48] [V] [TRT] Tactic: 20 Time: 0.619576
[11/01/2021-22:22:49] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:49] [V] [TRT] Tactic: 21 Time: 0.621732
[11/01/2021-22:22:49] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:49] [V] [TRT] Tactic: 22 Time: 0.624172
[11/01/2021-22:22:49] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:49] [V] [TRT] Tactic: 23 Time: 0.62262
[11/01/2021-22:22:49] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:49] [V] [TRT] Tactic: 28 Time: 0.627448
[11/01/2021-22:22:49] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:49] [V] [TRT] Tactic: 29 Time: 0.627784
[11/01/2021-22:22:50] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:50] [V] [TRT] Tactic: 30 Time: 0.618888
[11/01/2021-22:22:50] [V] [TRT] Fastest Tactic: 30 Time: 0.618888
[11/01/2021-22:22:50] [V] [TRT] --------------- Timing Runner: PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36) (PointWise)
[11/01/2021-22:22:50] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:50] [V] [TRT] Tactic: 128 Time: 0.920476
[11/01/2021-22:22:50] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:50] [V] [TRT] Tactic: 256 Time: 0.928128
[11/01/2021-22:22:50] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:50] [V] [TRT] Tactic: 512 Time: 0.968448
[11/01/2021-22:22:50] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:50] [V] [TRT] Tactic: -32 Time: 1.70509
[11/01/2021-22:22:50] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:50] [V] [TRT] Tactic: -64 Time: 1.06364
[11/01/2021-22:22:50] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:50] [V] [TRT] Tactic: -128 Time: 0.993612
[11/01/2021-22:22:50] [V] [TRT] Fastest Tactic: 128 Time: 0.920476
[11/01/2021-22:22:50] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 30
[11/01/2021-22:22:50] [V] [TRT] *************** Autotuning format combination: Float(983040,491520:32,480,1) -> Float(983040,491520:32,480,1) ***************
[11/01/2021-22:22:50] [V] [TRT] --------------- Timing Runner: PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36) (PointWiseV2)
[11/01/2021-22:22:50] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:50] [V] [TRT] Tactic: 24 Time: 0.62518
[11/01/2021-22:22:50] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:50] [V] [TRT] Tactic: 25 Time: 0.624132
[11/01/2021-22:22:50] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:50] [V] [TRT] Tactic: 26 Time: 0.625156
[11/01/2021-22:22:50] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:50] [V] [TRT] Tactic: 27 Time: 0.625656
[11/01/2021-22:22:51] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:51] [V] [TRT] Tactic: 31 Time: 0.624924
[11/01/2021-22:22:51] [V] [TRT] Fastest Tactic: 25 Time: 0.624132
[11/01/2021-22:22:51] [V] [TRT] --------------- Timing Runner: PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36) (PointWise)
[11/01/2021-22:22:51] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:51] [V] [TRT] Tactic: 128 Time: 0.920448
[11/01/2021-22:22:51] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:51] [V] [TRT] Tactic: 256 Time: 0.928204
[11/01/2021-22:22:51] [V] [TRT] Setting a default quantization params because quantization data is missing for PWN(240 + (Unnamed Layer* 30) [Shuffle], PRelu_36)
[11/01/2021-22:22:51] [V] [TRT] Tactic: 512 Time: 0.968144
[11/01/2021-22:22:51] [V] [TRT] Fastest Tactic: 128 Time: 0.920448
[11/01/2021-22:22:51] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 25
[11/01/2021-22:22:51] [V] [TRT] *************** Autotuning Reformat:Float(31457280,491520,480,1) -> Float(31457280,1,30720,64) ***************
[11/01/2021-22:22:51] [V] [TRT] *************** Autotuning Reformat:Float(31457280,491520,480,1) -> Float(7864320,1:4,7680,16) ***************
[11/01/2021-22:22:51] [V] [TRT] *************** Autotuning Reformat:Float(31457280,1,30720,64) -> Float(31457280,491520,480,1) ***************
[11/01/2021-22:22:51] [V] [TRT] *************** Autotuning Reformat:Float(31457280,1,30720,64) -> Float(7864320,1:4,7680,16) ***************
[11/01/2021-22:22:51] [V] [TRT] *************** Autotuning Reformat:Float(7864320,1:4,7680,16) -> Float(31457280,491520,480,1) ***************
[11/01/2021-22:22:51] [V] [TRT] *************** Autotuning Reformat:Float(7864320,1:4,7680,16) -> Float(31457280,1,30720,64) ***************
[11/01/2021-22:22:51] [V] [TRT] *************** Autotuning Reformat:Float(983040,491520:32,480,1) -> Float(31457280,491520,480,1) ***************
[11/01/2021-22:22:51] [V] [TRT] *************** Autotuning Reformat:Float(983040,491520:32,480,1) -> Float(31457280,1,30720,64) ***************
[11/01/2021-22:22:51] [V] [TRT] *************** Autotuning Reformat:Float(983040,491520:32,480,1) -> Float(7864320,1:4,7680,16) ***************
[11/01/2021-22:22:51] [V] [TRT] *************** Autotuning format combination: Float(31457280,491520,480,1) -> Float(31457280,491520,480,1) ***************
[11/01/2021-22:22:51] [V] [TRT] --------------- Timing Runner: Conv_37 (CudaDepthwiseConvolution)
[11/01/2021-22:22:51] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[11/01/2021-22:22:51] [V] [TRT] --------------- Timing Runner: Conv_37 (FusedConvActConvolution)
[11/01/2021-22:22:51] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[11/01/2021-22:22:51] [V] [TRT] --------------- Timing Runner: Conv_37 (CudnnConvolution)
[11/01/2021-22:22:51] [V] [TRT] Tactic: 0 Time: 7.45819
[11/01/2021-22:22:51] [V] [TRT] Tactic: 1 Time: 3.72825
[11/01/2021-22:22:51] [V] [TRT] Tactic: 2 Time: 9.96082
[11/01/2021-22:22:52] [V] [TRT] Tactic: 5 Time: 32.3641
[11/01/2021-22:22:52] [V] [TRT] Tactic: 6 Time: 2.75806
[11/01/2021-22:22:52] [V] [TRT] Tactic: 56 Time: 6.97153
[11/01/2021-22:22:52] [V] [TRT] Tactic: 57 Time: 3.75817
[11/01/2021-22:22:52] [V] [TRT] Tactic: 58 Time: 9.96563
[11/01/2021-22:22:53] [V] [TRT] Tactic: 61 Time: 32.3876
[11/01/2021-22:22:53] [V] [TRT] Tactic: 62 Time: 2.77354
[11/01/2021-22:22:53] [V] [TRT] Tactic: 112 Time: 7.00198
[11/01/2021-22:22:53] [V] [TRT] Tactic: 113 Time: 3.49995
[11/01/2021-22:22:53] [V] [TRT] Tactic: 114 Time: 10.0801
[11/01/2021-22:22:54] [V] [TRT] Tactic: 117 Time: 32.4557
[11/01/2021-22:22:54] [V] [TRT] Tactic: 118 Time: 2.75925
[11/01/2021-22:22:54] [V] [TRT] Fastest Tactic: 6 Time: 2.75806
[11/01/2021-22:22:54] [V] [TRT] --------------- Timing Runner: Conv_37 (CaskConvolution)
[11/01/2021-22:22:54] [V] [TRT] Conv_37 Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 4549827808004681195
[11/01/2021-22:22:54] [V] [TRT] Tactic: 4549827808004681195 Time: 2.92383
[11/01/2021-22:22:54] [V] [TRT] Conv_37 Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 5779835512569528575
[11/01/2021-22:22:54] [V] [TRT] Tactic: 5779835512569528575 Time: 5.33018
[11/01/2021-22:22:54] [V] [TRT] Conv_37 Set Tactic Name: ampere_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 6053873026024413720
[11/01/2021-22:22:54] [V] [TRT] Tactic: 6053873026024413720 Time: 5.59898
[11/01/2021-22:22:54] [V] [TRT] Conv_37 Set Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 6767548733843469815
[11/01/2021-22:22:54] [V] [TRT] Tactic: 6767548733843469815 Time: 2.81984
[11/01/2021-22:22:54] [V] [TRT] Conv_37 Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: -6313876406580483184
[11/01/2021-22:22:54] [V] [TRT] Tactic: -6313876406580483184 Time: 3.31912
[11/01/2021-22:22:54] [V] [TRT] Conv_37 Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: -1123676555321336786
[11/01/2021-22:22:54] [V] [TRT] Tactic: -1123676555321336786 Time: 5.35299
[11/01/2021-22:22:54] [V] [TRT] Conv_37 Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: -701551393537224327
[11/01/2021-22:22:54] [V] [TRT] Tactic: -701551393537224327 Time: 3.03597
[11/01/2021-22:22:54] [V] [TRT] Fastest Tactic: 6767548733843469815 Time: 2.81984
[11/01/2021-22:22:54] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 6
[11/01/2021-22:22:54] [V] [TRT] *************** Autotuning format combination: Float(31457280,1,30720,64) -> Float(31457280,1,30720,64) ***************
[11/01/2021-22:22:54] [V] [TRT] --------------- Timing Runner: Conv_37 (CudnnConvolution)
[11/01/2021-22:22:54] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[11/01/2021-22:22:54] [V] [TRT] --------------- Timing Runner: Conv_37 (CaskConvolution)
[11/01/2021-22:22:54] [V] [TRT] Conv_37 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 2860655430572478466
[11/01/2021-22:22:54] [V] [TRT] Tactic: 2860655430572478466 Time: 2.94142
[11/01/2021-22:22:54] [V] [TRT] Conv_37 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4474630279712975759
[11/01/2021-22:22:54] [V] [TRT] Tactic: 4474630279712975759 Time: 3.24131
[11/01/2021-22:22:54] [V] [TRT] Conv_37 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 4479823862704990365
[11/01/2021-22:22:54] [V] [TRT] Tactic: 4479823862704990365 Time: 3.19982
[11/01/2021-22:22:54] [V] [TRT] Conv_37 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4696204239951173149
[11/01/2021-22:22:54] [V] [TRT] Tactic: 4696204239951173149 Time: 2.97577
[11/01/2021-22:22:54] [V] [TRT] Conv_37 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 5778138195697110003
[11/01/2021-22:22:54] [V] [TRT] Tactic: 5778138195697110003 Time: 5.30008
[11/01/2021-22:22:54] [V] [TRT] Conv_37 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 7155825427510256858
[11/01/2021-22:22:55] [V] [TRT] Tactic: 7155825427510256858 Time: 5.13587
[11/01/2021-22:22:55] [V] [TRT] Conv_37 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 8918020581761223752
[11/01/2021-22:22:55] [V] [TRT] Tactic: 8918020581761223752 Time: 5.13253
[11/01/2021-22:22:55] [V] [TRT] Conv_37 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: -4756382386362004279
[11/01/2021-22:22:55] [V] [TRT] Tactic: -4756382386362004279 Time: 2.92055
[11/01/2021-22:22:55] [V] [TRT] Conv_37 Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: -3855385237722507464
[11/01/2021-22:22:55] [V] [TRT] Tactic: -3855385237722507464 Time: 5.33008
[11/01/2021-22:22:55] [V] [TRT] Conv_37 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: -2809379259463049391
[11/01/2021-22:22:55] [V] [TRT] Tactic: -2809379259463049391 Time: 5.3261
[11/01/2021-22:22:55] [V] [TRT] Conv_37 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: -504296718212024303
[11/01/2021-22:22:55] [V] [TRT] Tactic: -504296718212024303 Time: 5.07104
[11/01/2021-22:22:55] [V] [TRT] Fastest Tactic: -4756382386362004279 Time: 2.92055
[11/01/2021-22:22:55] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -4756382386362004279
[11/01/2021-22:22:55] [V] [TRT] *************** Autotuning format combination: Float(7864320,1:4,7680,16) -> Float(7864320,1:4,7680,16) ***************
[11/01/2021-22:22:55] [V] [TRT] --------------- Timing Runner: Conv_37 (CudnnConvolution)
[11/01/2021-22:22:55] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[11/01/2021-22:22:55] [V] [TRT] --------------- Timing Runner: Conv_37 (CaskConvolution)
[11/01/2021-22:22:55] [V] [TRT] Conv_37 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 2860655430572478466
[11/01/2021-22:22:55] [V] [TRT] Tactic: 2860655430572478466 Time: 2.92238
[11/01/2021-22:22:55] [V] [TRT] Conv_37 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4474630279712975759
[11/01/2021-22:22:55] [V] [TRT] Tactic: 4474630279712975759 Time: 3.23533
[11/01/2021-22:22:55] [V] [TRT] Conv_37 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 4479823862704990365
[11/01/2021-22:22:55] [V] [TRT] Tactic: 4479823862704990365 Time: 3.22476
[11/01/2021-22:22:55] [V] [TRT] Conv_37 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4696204239951173149
[11/01/2021-22:22:55] [V] [TRT] Tactic: 4696204239951173149 Time: 2.98383
[11/01/2021-22:22:55] [V] [TRT] Conv_37 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 5778138195697110003
[11/01/2021-22:22:55] [V] [TRT] Tactic: 5778138195697110003 Time: 5.2726
[11/01/2021-22:22:55] [V] [TRT] Conv_37 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 7155825427510256858
[11/01/2021-22:22:55] [V] [TRT] Tactic: 7155825427510256858 Time: 5.11726
[11/01/2021-22:22:55] [V] [TRT] Conv_37 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 7342025736444949634
[11/01/2021-22:22:55] [V] [TRT] Tactic: 7342025736444949634 Time: 3.67775
[11/01/2021-22:22:55] [V] [TRT] Conv_37 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 8918020581761223752
[11/01/2021-22:22:56] [V] [TRT] Tactic: 8918020581761223752 Time: 5.04973
[11/01/2021-22:22:56] [V] [TRT] Conv_37 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: -7377458734869418330
[11/01/2021-22:22:56] [V] [TRT] Tactic: -7377458734869418330 Time: 3.6433
[11/01/2021-22:22:56] [V] [TRT] Conv_37 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: -5457304872213719461
[11/01/2021-22:22:56] [V] [TRT] Tactic: -5457304872213719461 Time: 3.65942
[11/01/2021-22:22:56] [V] [TRT] Conv_37 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: -4756382386362004279
[11/01/2021-22:22:56] [V] [TRT] Tactic: -4756382386362004279 Time: 2.89487
[11/01/2021-22:22:56] [V] [TRT] Conv_37 Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: -3855385237722507464
[11/01/2021-22:22:56] [V] [TRT] Tactic: -3855385237722507464 Time: 5.36592
[11/01/2021-22:22:56] [V] [TRT] Conv_37 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: -2809379259463049391
[11/01/2021-22:22:56] [V] [TRT] Tactic: -2809379259463049391 Time: 5.3082
[11/01/2021-22:22:56] [V] [TRT] Conv_37 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: -504296718212024303
[11/01/2021-22:22:56] [V] [TRT] Tactic: -504296718212024303 Time: 5.07219
[11/01/2021-22:22:56] [V] [TRT] Fastest Tactic: -4756382386362004279 Time: 2.89487
[11/01/2021-22:22:56] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -4756382386362004279
[11/01/2021-22:22:56] [V] [TRT] *************** Autotuning Reformat:Float(31457280,1,30720,64) -> Float(31457280,491520,480,1) ***************
[11/01/2021-22:22:56] [V] [TRT] *************** Autotuning Reformat:Float(7864320,1:4,7680,16) -> Float(31457280,491520,480,1) ***************
[11/01/2021-22:22:56] [V] [TRT] *************** Autotuning format combination: Float(31457280,491520,480,1) -> Float(31457280,491520,480,1) ***************
[11/01/2021-22:22:56] [V] [TRT] *************** Autotuning Reformat:Float(31457280,491520,480,1) -> Float(31457280,1,30720,64) ***************
[11/01/2021-22:22:56] [V] [TRT] *************** Autotuning Reformat:Float(31457280,491520,480,1) -> Float(7864320,1:4,7680,16) ***************
[11/01/2021-22:22:56] [V] [TRT] *************** Autotuning Reformat:Float(491520,491520,480,1) -> Float(491520,1,480,1) ***************
[11/01/2021-22:22:56] [V] [TRT] *************** Autotuning Reformat:Float(491520,491520,480,1) -> Float(491520,1:4,480,1) ***************
[11/01/2021-22:22:56] [V] [TRT] *************** Autotuning Reformat:Float(491520,1,480,1) -> Float(491520,491520,480,1) ***************
[11/01/2021-22:22:56] [V] [TRT] *************** Autotuning Reformat:Float(491520,1,480,1) -> Float(491520,1:4,480,1) ***************
[11/01/2021-22:22:56] [V] [TRT] *************** Autotuning Reformat:Float(491520,1:4,480,1) -> Float(491520,491520,480,1) ***************
[11/01/2021-22:22:56] [V] [TRT] *************** Autotuning Reformat:Float(491520,1:4,480,1) -> Float(491520,1,480,1) ***************
[11/01/2021-22:22:56] [V] [TRT] *************** Autotuning Reformat:Float(491520,491520:32,480,1) -> Float(491520,491520,480,1) ***************
[11/01/2021-22:22:56] [V] [TRT] *************** Autotuning Reformat:Float(491520,491520:32,480,1) -> Float(491520,1,480,1) ***************
[11/01/2021-22:22:56] [V] [TRT] *************** Autotuning Reformat:Float(491520,491520:32,480,1) -> Float(491520,1:4,480,1) ***************
[11/01/2021-22:22:56] [V] [TRT] *************** Autotuning Reformat:Float(31457280,491520,480,1) -> Float(31457280,1,30720,64) ***************
[11/01/2021-22:22:56] [V] [TRT] *************** Autotuning Reformat:Float(31457280,491520,480,1) -> Float(7864320,1:4,7680,16) ***************
[11/01/2021-22:22:56] [V] [TRT] *************** Autotuning Reformat:Float(31457280,1,30720,64) -> Float(31457280,491520,480,1) ***************
[11/01/2021-22:22:56] [V] [TRT] *************** Autotuning Reformat:Float(31457280,1,30720,64) -> Float(7864320,1:4,7680,16) ***************
[11/01/2021-22:22:56] [V] [TRT] *************** Autotuning Reformat:Float(7864320,1:4,7680,16) -> Float(31457280,491520,480,1) ***************
[11/01/2021-22:22:56] [V] [TRT] *************** Autotuning Reformat:Float(7864320,1:4,7680,16) -> Float(31457280,1,30720,64) ***************
[11/01/2021-22:22:56] [V] [TRT] *************** Autotuning format combination: Float(491520,491520,480,1), Float(31457280,491520,480,1) -> Float(31457280,491520,480,1) ***************
[11/01/2021-22:22:56] [V] [TRT] --------------- Timing Runner: Conv_31 + Add_41 (CudaDepthwiseConvolution)
[11/01/2021-22:22:56] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[11/01/2021-22:22:56] [V] [TRT] --------------- Timing Runner: Conv_31 + Add_41 (FusedConvActConvolution)
[11/01/2021-22:22:56] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[11/01/2021-22:22:56] [V] [TRT] --------------- Timing Runner: Conv_31 + Add_41 (CudnnConvolution)
[11/01/2021-22:22:56] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:56] [V] [TRT] Tactic: 0 Time: 2.01789
[11/01/2021-22:22:56] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:56] [V] [TRT] Tactic: 1 Time: 1.92356
[11/01/2021-22:22:56] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:56] [V] [TRT] Tactic: 2 Time: 2.06892
[11/01/2021-22:22:56] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:56] [V] [TRT] Tactic: 5 Time: 2.57755
[11/01/2021-22:22:56] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:56] [V] [TRT] Tactic: 56 Time: 2.02068
[11/01/2021-22:22:56] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:56] [V] [TRT] Tactic: 57 Time: 1.93472
[11/01/2021-22:22:56] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:56] [V] [TRT] Tactic: 58 Time: 2.0652
[11/01/2021-22:22:56] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:56] [V] [TRT] Tactic: 61 Time: 2.57916
[11/01/2021-22:22:56] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:56] [V] [TRT] Tactic: 112 Time: 2.01576
[11/01/2021-22:22:56] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:56] [V] [TRT] Tactic: 113 Time: 1.91978
[11/01/2021-22:22:56] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:57] [V] [TRT] Tactic: 114 Time: 2.06381
[11/01/2021-22:22:57] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:57] [V] [TRT] Tactic: 117 Time: 2.61534
[11/01/2021-22:22:57] [V] [TRT] Fastest Tactic: 113 Time: 1.91978
[11/01/2021-22:22:57] [V] [TRT] --------------- Timing Runner: Conv_31 + Add_41 (CublasConvolution)
[11/01/2021-22:22:57] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[11/01/2021-22:22:57] [V] [TRT] --------------- Timing Runner: Conv_31 + Add_41 (CaskConvolution)
[11/01/2021-22:22:57] [V] [TRT] Conv_31 + Add_41 Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 4549827808004681195
[11/01/2021-22:22:57] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:57] [V] [TRT] Tactic: 4549827808004681195 Time: 0.65792
[11/01/2021-22:22:57] [V] [TRT] Conv_31 + Add_41 Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 5779835512569528575
[11/01/2021-22:22:57] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:57] [V] [TRT] Tactic: 5779835512569528575 Time: 0.656136
[11/01/2021-22:22:57] [V] [TRT] Conv_31 + Add_41 Set Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: -7491730084094677098
[11/01/2021-22:22:57] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:57] [V] [TRT] Tactic: -7491730084094677098 Time: 0.669236
[11/01/2021-22:22:57] [V] [TRT] Conv_31 + Add_41 Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: -6313876406580483184
[11/01/2021-22:22:57] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:57] [V] [TRT] Tactic: -6313876406580483184 Time: 0.670224
[11/01/2021-22:22:57] [V] [TRT] Conv_31 + Add_41 Set Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: -6273689210331812572
[11/01/2021-22:22:57] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:57] [V] [TRT] Tactic: -6273689210331812572 Time: 0.650984
[11/01/2021-22:22:57] [V] [TRT] Conv_31 + Add_41 Set Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: -4337126844824617177
[11/01/2021-22:22:57] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:57] [V] [TRT] Tactic: -4337126844824617177 Time: 0.657864
[11/01/2021-22:22:57] [V] [TRT] Conv_31 + Add_41 Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: -1123676555321336786
[11/01/2021-22:22:57] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:57] [V] [TRT] Tactic: -1123676555321336786 Time: 0.655344
[11/01/2021-22:22:57] [V] [TRT] Conv_31 + Add_41 Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: -701551393537224327
[11/01/2021-22:22:57] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:57] [V] [TRT] Tactic: -701551393537224327 Time: 0.658504
[11/01/2021-22:22:57] [V] [TRT] Fastest Tactic: -6273689210331812572 Time: 0.650984
[11/01/2021-22:22:57] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -6273689210331812572
[11/01/2021-22:22:57] [V] [TRT] *************** Autotuning format combination: Float(491520,1,480,1), Float(31457280,1,30720,64) -> Float(31457280,1,30720,64) ***************
[11/01/2021-22:22:57] [V] [TRT] --------------- Timing Runner: Conv_31 + Add_41 (CudnnConvolution)
[11/01/2021-22:22:57] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[11/01/2021-22:22:57] [V] [TRT] --------------- Timing Runner: Conv_31 + Add_41 (CublasConvolution)
[11/01/2021-22:22:57] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[11/01/2021-22:22:57] [V] [TRT] --------------- Timing Runner: Conv_31 + Add_41 (CaskConvolution)
[11/01/2021-22:22:57] [V] [TRT] Conv_31 + Add_41 Set Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 1663866669559596164
[11/01/2021-22:22:57] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:57] [V] [TRT] Tactic: 1663866669559596164 Time: 0.687
[11/01/2021-22:22:57] [V] [TRT] Conv_31 + Add_41 Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 4549827808004681195
[11/01/2021-22:22:57] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:57] [V] [TRT] Tactic: 4549827808004681195 Time: 5.9243
[11/01/2021-22:22:57] [V] [TRT] Conv_31 + Add_41 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 5778138195697110003
[11/01/2021-22:22:57] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:57] [V] [TRT] Tactic: 5778138195697110003 Time: 0.690584
[11/01/2021-22:22:57] [V] [TRT] Conv_31 + Add_41 Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 5779835512569528575
[11/01/2021-22:22:57] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:57] [V] [TRT] Tactic: 5779835512569528575 Time: 2.17019
[11/01/2021-22:22:57] [V] [TRT] Conv_31 + Add_41 Set Tactic Name: ampere_scudnn_128x32_relu_interior_nn_v1 Tactic: -7491730084094677098
[11/01/2021-22:22:57] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:57] [V] [TRT] Tactic: -7491730084094677098 Time: 5.62065
[11/01/2021-22:22:57] [V] [TRT] Conv_31 + Add_41 Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: -6313876406580483184
[11/01/2021-22:22:57] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:57] [V] [TRT] Tactic: -6313876406580483184 Time: 5.58137
[11/01/2021-22:22:57] [V] [TRT] Conv_31 + Add_41 Set Tactic Name: ampere_scudnn_128x128_relu_interior_nn_v1 Tactic: -6273689210331812572
[11/01/2021-22:22:57] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:57] [V] [TRT] Tactic: -6273689210331812572 Time: 2.18
[11/01/2021-22:22:57] [V] [TRT] Conv_31 + Add_41 Set Tactic Name: ampere_scudnn_128x64_relu_interior_nn_v1 Tactic: -4337126844824617177
[11/01/2021-22:22:57] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:57] [V] [TRT] Tactic: -4337126844824617177 Time: 6.17583
[11/01/2021-22:22:57] [V] [TRT] Conv_31 + Add_41 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: -2809379259463049391
[11/01/2021-22:22:57] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:57] [V] [TRT] Tactic: -2809379259463049391 Time: 0.690492
[11/01/2021-22:22:57] [V] [TRT] Conv_31 + Add_41 Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: -1123676555321336786
[11/01/2021-22:22:57] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:57] [V] [TRT] Tactic: -1123676555321336786 Time: 2.17223
[11/01/2021-22:22:57] [V] [TRT] Conv_31 + Add_41 Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: -701551393537224327
[11/01/2021-22:22:57] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:57] [V] [TRT] Tactic: -701551393537224327 Time: 5.88992
[11/01/2021-22:22:57] [V] [TRT] Fastest Tactic: 1663866669559596164 Time: 0.687
[11/01/2021-22:22:57] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 1663866669559596164
[11/01/2021-22:22:57] [V] [TRT] *************** Autotuning format combination: Float(491520,1:4,480,1), Float(7864320,1:4,7680,16) -> Float(7864320,1:4,7680,16) ***************
[11/01/2021-22:22:57] [V] [TRT] --------------- Timing Runner: Conv_31 + Add_41 (CudnnConvolution)
[11/01/2021-22:22:57] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[11/01/2021-22:22:57] [V] [TRT] --------------- Timing Runner: Conv_31 + Add_41 (CublasConvolution)
[11/01/2021-22:22:57] [V] [TRT] CublasConvolution has no valid tactics for this config, skipping
[11/01/2021-22:22:57] [V] [TRT] --------------- Timing Runner: Conv_31 + Add_41 (CaskConvolution)
[11/01/2021-22:22:57] [V] [TRT] Conv_31 + Add_41 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r1s1 Tactic: 1373022415249282411
[11/01/2021-22:22:57] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:57] [V] [TRT] Tactic: 1373022415249282411 Time: 0.68762
[11/01/2021-22:22:57] [V] [TRT] Conv_31 + Add_41 Set Tactic Name: ampere_scudnn_128x128_relu_exp_interior_nhwc_tn_v1 Tactic: 1663866669559596164
[11/01/2021-22:22:57] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:57] [V] [TRT] Tactic: 1663866669559596164 Time: 0.697624
[11/01/2021-22:22:57] [V] [TRT] Conv_31 + Add_41 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 2860655430572478466
[11/01/2021-22:22:57] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:57] [V] [TRT] Tactic: 2860655430572478466 Time: 0.702848
[11/01/2021-22:22:57] [V] [TRT] Conv_31 + Add_41 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4474630279712975759
[11/01/2021-22:22:57] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:57] [V] [TRT] Tactic: 4474630279712975759 Time: 0.830628
[11/01/2021-22:22:57] [V] [TRT] Conv_31 + Add_41 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 4479823862704990365
[11/01/2021-22:22:57] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:57] [V] [TRT] Tactic: 4479823862704990365 Time: 0.829596
[11/01/2021-22:22:57] [V] [TRT] Conv_31 + Add_41 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4696204239951173149
[11/01/2021-22:22:57] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:57] [V] [TRT] Tactic: 4696204239951173149 Time: 0.701276
[11/01/2021-22:22:57] [V] [TRT] Conv_31 + Add_41 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 5778138195697110003
[11/01/2021-22:22:57] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:57] [V] [TRT] Tactic: 5778138195697110003 Time: 0.700412
[11/01/2021-22:22:57] [V] [TRT] Conv_31 + Add_41 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 8918020581761223752
[11/01/2021-22:22:57] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:58] [V] [TRT] Tactic: 8918020581761223752 Time: 0.701048
[11/01/2021-22:22:58] [V] [TRT] Conv_31 + Add_41 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_simple_t1r1s1 Tactic: -7067026478815706014
[11/01/2021-22:22:58] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:58] [V] [TRT] Tactic: -7067026478815706014 Time: 0.684724
[11/01/2021-22:22:58] [V] [TRT] Conv_31 + Add_41 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: -5905193483742532701
[11/01/2021-22:22:58] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:58] [V] [TRT] Tactic: -5905193483742532701 Time: 0.699176
[11/01/2021-22:22:58] [V] [TRT] Conv_31 + Add_41 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: -4035591156787122265
[11/01/2021-22:22:58] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:58] [V] [TRT] Tactic: -4035591156787122265 Time: 0.821504
[11/01/2021-22:22:58] [V] [TRT] Conv_31 + Add_41 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: -2809379259463049391
[11/01/2021-22:22:58] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:58] [V] [TRT] Tactic: -2809379259463049391 Time: 0.701184
[11/01/2021-22:22:58] [V] [TRT] Conv_31 + Add_41 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_interior_nhwc_tn_v1 Tactic: -1985235291706575900
[11/01/2021-22:22:58] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:58] [V] [TRT] Tactic: -1985235291706575900 Time: 0.698148
[11/01/2021-22:22:58] [V] [TRT] Conv_31 + Add_41 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: -504296718212024303
[11/01/2021-22:22:58] [V] [TRT] Setting a default quantization params because quantization data is missing for Conv_31 + Add_41
[11/01/2021-22:22:58] [V] [TRT] Tactic: -504296718212024303 Time: 0.699864
[11/01/2021-22:22:58] [V] [TRT] Fastest Tactic: -7067026478815706014 Time: 0.684724
[11/01/2021-22:22:58] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -7067026478815706014
[11/01/2021-22:22:58] [V] [TRT] *************** Autotuning Reformat:Float(31457280,491520,480,1) -> Float(31457280,1,30720,64) ***************
[11/01/2021-22:22:58] [V] [TRT] *************** Autotuning Reformat:Float(31457280,491520,480,1) -> Float(7864320,1:4,7680,16) ***************
[11/01/2021-22:22:58] [V] [TRT] *************** Autotuning Reformat:Float(31457280,491520,480,1) -> Float(983040,491520:32,480,1) ***************
[11/01/2021-22:22:58] [V] [TRT] *************** Autotuning Reformat:Float(31457280,1,30720,64) -> Float(31457280,491520,480,1) ***************
[11/01/2021-22:22:58] [V] [TRT] *************** Autotuning Reformat:Float(31457280,1,30720,64) -> Float(7864320,1:4,7680,16) ***************
[11/01/2021-22:22:58] [V] [TRT] *************** Autotuning Reformat:Float(31457280,1,30720,64) -> Float(983040,491520:32,480,1) ***************
[11/01/2021-22:22:58] [V] [TRT] *************** Autotuning Reformat:Float(7864320,1:4,7680,16) -> Float(31457280,491520,480,1) ***************
[11/01/2021-22:22:58] [V] [TRT] *************** Autotuning Reformat:Float(7864320,1:4,7680,16) -> Float(31457280,1,30720,64) ***************
[11/01/2021-22:22:58] [V] [TRT] *************** Autotuning Reformat:Float(7864320,1:4,7680,16) -> Float(983040,491520:32,480,1) ***************
[11/01/2021-22:22:58] [V] [TRT] *************** Autotuning Reformat:Float(983040,491520:32,480,1) -> Float(31457280,491520,480,1) ***************
[11/01/2021-22:22:58] [V] [TRT] *************** Autotuning Reformat:Float(983040,491520:32,480,1) -> Float(31457280,1,30720,64) ***************
[11/01/2021-22:22:58] [V] [TRT] *************** Autotuning Reformat:Float(983040,491520:32,480,1) -> Float(7864320,1:4,7680,16) ***************
[11/01/2021-22:22:58] [V] [TRT] *************** Autotuning format combination: Float(31457280,491520,480,1) -> Float(62914560,491520,480,1) ***************
[11/01/2021-22:22:58] [V] [TRT] --------------- Timing Runner: PWN(241 + (Unnamed Layer* 36) [Shuffle], PRelu_42) (PointWiseV2)
[11/01/2021-22:22:58] [V] [TRT] Tactic: 0 Time: 0.616608
[11/01/2021-22:22:58] [V] [TRT] Tactic: 1 Time: 0.620712
[11/01/2021-22:22:58] [V] [TRT] Tactic: 2 Time: 0.62474
[11/01/2021-22:22:58] [V] [TRT] Tactic: 3 Time: 0.624544
[11/01/2021-22:22:59] [V] [TRT] Tactic: 4 Time: 0.628256
[11/01/2021-22:22:59] [V] [TRT] Tactic: 5 Time: 0.627736
[11/01/2021-22:22:59] [V] [TRT] Tactic: 6 Time: 0.621788
[11/01/2021-22:22:59] [V] [TRT] Tactic: 7 Time: 0.625716
[11/01/2021-22:22:59] [V] [TRT] Tactic: 8 Time: 0.624152
[11/01/2021-22:23:00] [V] [TRT] Tactic: 9 Time: 0.627396
[11/01/2021-22:23:00] [V] [TRT] Tactic: 28 Time: 0.617288
[11/01/2021-22:23:00] [V] [TRT] Fastest Tactic: 0 Time: 0.616608
[11/01/2021-22:23:00] [V] [TRT] --------------- Timing Runner: PWN(241 + (Unnamed Layer* 36) [Shuffle], PRelu_42) (PointWise)
[11/01/2021-22:23:00] [V] [TRT] Tactic: 128 Time: 1.0691
[11/01/2021-22:23:00] [V] [TRT] Tactic: 256 Time: 1.07779
[11/01/2021-22:23:00] [V] [TRT] Tactic: 512 Time: 1.1123
[11/01/2021-22:23:00] [V] [TRT] Fastest Tactic: 128 Time: 1.0691
[11/01/2021-22:23:00] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0
[11/01/2021-22:23:00] [V] [TRT] *************** Autotuning format combination: Float(31457280,1,30720,64) -> Float(62914560,1,61440,128) ***************
[11/01/2021-22:23:00] [V] [TRT] --------------- Timing Runner: PWN(241 + (Unnamed Layer* 36) [Shuffle], PRelu_42) (PointWiseV2)
[11/01/2021-22:23:00] [V] [TRT] Tactic: 0 Time: 0.617272
[11/01/2021-22:23:00] [V] [TRT] Tactic: 1 Time: 0.621312
[11/01/2021-22:23:00] [V] [TRT] Tactic: 2 Time: 0.6269
[11/01/2021-22:23:00] [V] [TRT] Tactic: 3 Time: 0.619852
[11/01/2021-22:23:00] [V] [TRT] Tactic: 4 Time: 0.626168
[11/01/2021-22:23:00] [V] [TRT] Tactic: 5 Time: 0.627616
[11/01/2021-22:23:00] [V] [TRT] Tactic: 6 Time: 0.616192
[11/01/2021-22:23:00] [V] [TRT] Tactic: 7 Time: 0.627388
[11/01/2021-22:23:00] [V] [TRT] Tactic: 8 Time: 0.629556
[11/01/2021-22:23:00] [V] [TRT] Tactic: 9 Time: 0.627612
[11/01/2021-22:23:00] [V] [TRT] Tactic: 28 Time: 0.618268
[11/01/2021-22:23:00] [V] [TRT] Fastest Tactic: 6 Time: 0.616192
[11/01/2021-22:23:00] [V] [TRT] --------------- Timing Runner: PWN(241 + (Unnamed Layer* 36) [Shuffle], PRelu_42) (PointWise)
[11/01/2021-22:23:00] [V] [TRT] Tactic: 128 Time: 0.961968
[11/01/2021-22:23:00] [V] [TRT] Tactic: 256 Time: 0.969704
[11/01/2021-22:23:00] [V] [TRT] Tactic: 512 Time: 1.00728
[11/01/2021-22:23:00] [V] [TRT] Fastest Tactic: 128 Time: 0.961968
[11/01/2021-22:23:00] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 6
[11/01/2021-22:23:00] [V] [TRT] *************** Autotuning format combination: Float(7864320,1:4,7680,16) -> Float(15728640,1:4,15360,32) ***************
[11/01/2021-22:23:00] [V] [TRT] --------------- Timing Runner: PWN(241 + (Unnamed Layer* 36) [Shuffle], PRelu_42) (PointWiseV2)
[11/01/2021-22:23:00] [V] [TRT] Tactic: 0 Time: 0.62786
[11/01/2021-22:23:00] [V] [TRT] Tactic: 1 Time: 0.628104
[11/01/2021-22:23:01] [V] [TRT] Tactic: 2 Time: 0.626848
[11/01/2021-22:23:01] [V] [TRT] Tactic: 3 Time: 0.62758
[11/01/2021-22:23:01] [V] [TRT] Tactic: 4 Time: 0.628044
[11/01/2021-22:23:01] [V] [TRT] Tactic: 5 Time: 0.627388
[11/01/2021-22:23:02] [V] [TRT] Tactic: 6 Time: 0.6592
[11/01/2021-22:23:02] [V] [TRT] Tactic: 7 Time: 0.628188
[11/01/2021-22:23:02] [V] [TRT] Tactic: 8 Time: 0.627084
[11/01/2021-22:23:02] [V] [TRT] Tactic: 9 Time: 0.64178
[11/01/2021-22:23:02] [V] [TRT] Tactic: 10 Time: 0.629452
[11/01/2021-22:23:03] [V] [TRT] Tactic: 11 Time: 0.628272
[11/01/2021-22:23:03] [V] [TRT] Tactic: 12 Time: 0.630844
[11/01/2021-22:23:03] [V] [TRT] Tactic: 13 Time: 0.627852
[11/01/2021-22:23:03] [V] [TRT] Tactic: 14 Time: 0.627868
[11/01/2021-22:23:04] [V] [TRT] Tactic: 15 Time: 0.627312
[11/01/2021-22:23:04] [V] [TRT] Tactic: 16 Time: 0.626576
[11/01/2021-22:23:04] [V] [TRT] Tactic: 17 Time: 0.630172
[11/01/2021-22:23:04] [V] [TRT] Tactic: 18 Time: 0.628464
[11/01/2021-22:23:04] [V] [TRT] Tactic: 19 Time: 0.627776
[11/01/2021-22:23:05] [V] [TRT] Tactic: 20 Time: 0.618256
[11/01/2021-22:23:05] [V] [TRT] Tactic: 21 Time: 0.620964
[11/01/2021-22:23:05] [V] [TRT] Tactic: 22 Time: 0.616708
[11/01/2021-22:23:05] [V] [TRT] Tactic: 23 Time: 0.633728
[11/01/2021-22:23:05] [V] [TRT] Tactic: 28 Time: 0.629828
[11/01/2021-22:23:06] [V] [TRT] Tactic: 29 Time: 0.628224
[11/01/2021-22:23:06] [V] [TRT] Tactic: 30 Time: 0.61946
[11/01/2021-22:23:06] [V] [TRT] Fastest Tactic: 22 Time: 0.616708
[11/01/2021-22:23:06] [V] [TRT] --------------- Timing Runner: PWN(241 + (Unnamed Layer* 36) [Shuffle], PRelu_42) (PointWise)
[11/01/2021-22:23:06] [V] [TRT] Tactic: 128 Time: 1.0647
[11/01/2021-22:23:06] [V] [TRT] Tactic: 256 Time: 1.072
[11/01/2021-22:23:06] [V] [TRT] Tactic: 512 Time: 1.10792
[11/01/2021-22:23:06] [V] [TRT] Tactic: -32 Time: 1.9671
[11/01/2021-22:23:06] [V] [TRT] Tactic: -64 Time: 1.37299
[11/01/2021-22:23:06] [V] [TRT] Tactic: -128 Time: 1.28776
[11/01/2021-22:23:06] [V] [TRT] Fastest Tactic: 128 Time: 1.0647
[11/01/2021-22:23:06] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 22
[11/01/2021-22:23:06] [V] [TRT] *************** Autotuning format combination: Float(983040,491520:32,480,1) -> Float(1966080,491520:32,480,1) ***************
[11/01/2021-22:23:06] [V] [TRT] --------------- Timing Runner: PWN(241 + (Unnamed Layer* 36) [Shuffle], PRelu_42) (PointWiseV2)
[11/01/2021-22:23:06] [V] [TRT] Tactic: 24 Time: 0.626256
[11/01/2021-22:23:06] [V] [TRT] Tactic: 25 Time: 0.624256
[11/01/2021-22:23:07] [V] [TRT] Tactic: 26 Time: 0.628256
[11/01/2021-22:23:07] [V] [TRT] Tactic: 27 Time: 0.628388
[11/01/2021-22:23:07] [V] [TRT] Tactic: 31 Time: 0.625236
[11/01/2021-22:23:07] [V] [TRT] Fastest Tactic: 25 Time: 0.624256
[11/01/2021-22:23:07] [V] [TRT] --------------- Timing Runner: PWN(241 + (Unnamed Layer* 36) [Shuffle], PRelu_42) (PointWise)
[11/01/2021-22:23:07] [V] [TRT] Tactic: 128 Time: 0.96458
[11/01/2021-22:23:07] [V] [TRT] Tactic: 256 Time: 0.97604
[11/01/2021-22:23:07] [V] [TRT] Tactic: 512 Time: 1.01041
[11/01/2021-22:23:07] [V] [TRT] Fastest Tactic: 128 Time: 0.96458
[11/01/2021-22:23:07] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 25
[11/01/2021-22:23:07] [V] [TRT] *************** Autotuning Reformat:Float(62914560,491520,480,1) -> Float(62914560,1,61440,128) ***************
[11/01/2021-22:23:07] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:07] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:07] [V] [TRT] Tactic: 1002 Time: 0.629504
[11/01/2021-22:23:07] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:07] [V] [TRT] Tactic: 0 Time: 1.03008
[11/01/2021-22:23:07] [V] [TRT] Fastest Tactic: 1002 Time: 0.629504
[11/01/2021-22:23:07] [V] [TRT] *************** Autotuning Reformat:Float(62914560,491520,480,1) -> Float(15728640,1:4,15360,32) ***************
[11/01/2021-22:23:07] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:07] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:07] [V] [TRT] Tactic: 1002 Time: 0.6313
[11/01/2021-22:23:07] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:07] [V] [TRT] Tactic: 0 Time: 1.0166
[11/01/2021-22:23:07] [V] [TRT] Fastest Tactic: 1002 Time: 0.6313
[11/01/2021-22:23:07] [V] [TRT] *************** Autotuning Reformat:Float(62914560,491520,480,1) -> Float(1966080,491520:32,480,1) ***************
[11/01/2021-22:23:07] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:07] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:07] [V] [TRT] Tactic: 1002 Time: 0.628944
[11/01/2021-22:23:07] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:07] [V] [TRT] Tactic: 0 Time: 4.70056
[11/01/2021-22:23:07] [V] [TRT] Fastest Tactic: 1002 Time: 0.628944
[11/01/2021-22:23:07] [V] [TRT] *************** Autotuning Reformat:Float(62914560,491520,480,1) -> Int8(15728640,491520:4,480,1) ***************
[11/01/2021-22:23:07] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:07] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:07] [V] [TRT] Tactic: 1002 Time: 0.38298
[11/01/2021-22:23:07] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:07] [V] [TRT] Tactic: 0 Time: 0.381844
[11/01/2021-22:23:07] [V] [TRT] Fastest Tactic: 0 Time: 0.381844
[11/01/2021-22:23:07] [V] [TRT] *************** Autotuning Reformat:Float(62914560,491520,480,1) -> Int8(1966080,491520:32,480,1) ***************
[11/01/2021-22:23:07] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:07] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:07] [V] [TRT] Tactic: 1002 Time: 0.382592
[11/01/2021-22:23:07] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:07] [V] [TRT] Tactic: 0 Time: 4.53534
[11/01/2021-22:23:07] [V] [TRT] Fastest Tactic: 1002 Time: 0.382592
[11/01/2021-22:23:07] [V] [TRT] *************** Autotuning Reformat:Float(62914560,1,61440,128) -> Float(62914560,491520,480,1) ***************
[11/01/2021-22:23:07] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:07] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:07] [V] [TRT] Tactic: 1002 Time: 0.624824
[11/01/2021-22:23:07] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:07] [V] [TRT] Tactic: 0 Time: 2.73845
[11/01/2021-22:23:07] [V] [TRT] Fastest Tactic: 1002 Time: 0.624824
[11/01/2021-22:23:07] [V] [TRT] *************** Autotuning Reformat:Float(62914560,1,61440,128) -> Float(15728640,1:4,15360,32) ***************
[11/01/2021-22:23:07] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:07] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:07] [V] [TRT] Tactic: 1002 Time: 0.625348
[11/01/2021-22:23:07] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:07] [V] [TRT] Tactic: 0 Time: 0.61888
[11/01/2021-22:23:07] [V] [TRT] Fastest Tactic: 0 Time: 0.61888
[11/01/2021-22:23:07] [V] [TRT] *************** Autotuning Reformat:Float(62914560,1,61440,128) -> Float(1966080,491520:32,480,1) ***************
[11/01/2021-22:23:07] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:07] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:07] [V] [TRT] Tactic: 1002 Time: 0.624384
[11/01/2021-22:23:07] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:07] [V] [TRT] Tactic: 0 Time: 7.77338
[11/01/2021-22:23:07] [V] [TRT] Fastest Tactic: 1002 Time: 0.624384
[11/01/2021-22:23:07] [V] [TRT] *************** Autotuning Reformat:Float(62914560,1,61440,128) -> Int8(15728640,491520:4,480,1) ***************
[11/01/2021-22:23:07] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:07] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:08] [V] [TRT] Tactic: 1002 Time: 0.386524
[11/01/2021-22:23:08] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:08] [V] [TRT] Tactic: 0 Time: 2.90838
[11/01/2021-22:23:08] [V] [TRT] Fastest Tactic: 1002 Time: 0.386524
[11/01/2021-22:23:08] [V] [TRT] *************** Autotuning Reformat:Float(62914560,1,61440,128) -> Int8(1966080,491520:32,480,1) ***************
[11/01/2021-22:23:08] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:08] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:08] [V] [TRT] Tactic: 1002 Time: 0.383936
[11/01/2021-22:23:08] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:08] [V] [TRT] Tactic: 0 Time: 7.08695
[11/01/2021-22:23:08] [V] [TRT] Fastest Tactic: 1002 Time: 0.383936
[11/01/2021-22:23:08] [V] [TRT] *************** Autotuning Reformat:Float(15728640,1:4,15360,32) -> Float(62914560,491520,480,1) ***************
[11/01/2021-22:23:08] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:08] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:08] [V] [TRT] Tactic: 1002 Time: 0.625544
[11/01/2021-22:23:08] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:08] [V] [TRT] Tactic: 0 Time: 2.73923
[11/01/2021-22:23:08] [V] [TRT] Fastest Tactic: 1002 Time: 0.625544
[11/01/2021-22:23:08] [V] [TRT] *************** Autotuning Reformat:Float(15728640,1:4,15360,32) -> Float(62914560,1,61440,128) ***************
[11/01/2021-22:23:08] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:08] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:08] [V] [TRT] Tactic: 1002 Time: 0.625184
[11/01/2021-22:23:08] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:08] [V] [TRT] Tactic: 0 Time: 0.616448
[11/01/2021-22:23:08] [V] [TRT] Fastest Tactic: 0 Time: 0.616448
[11/01/2021-22:23:08] [V] [TRT] *************** Autotuning Reformat:Float(15728640,1:4,15360,32) -> Float(1966080,491520:32,480,1) ***************
[11/01/2021-22:23:08] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:08] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:08] [V] [TRT] Tactic: 1002 Time: 0.62624
[11/01/2021-22:23:08] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:08] [V] [TRT] Tactic: 0 Time: 7.75876
[11/01/2021-22:23:08] [V] [TRT] Fastest Tactic: 1002 Time: 0.62624
[11/01/2021-22:23:08] [V] [TRT] *************** Autotuning Reformat:Float(15728640,1:4,15360,32) -> Int8(15728640,491520:4,480,1) ***************
[11/01/2021-22:23:08] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:08] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:08] [V] [TRT] Tactic: 1002 Time: 0.385868
[11/01/2021-22:23:08] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:08] [V] [TRT] Tactic: 0 Time: 2.9098
[11/01/2021-22:23:08] [V] [TRT] Fastest Tactic: 1002 Time: 0.385868
[11/01/2021-22:23:08] [V] [TRT] *************** Autotuning Reformat:Float(15728640,1:4,15360,32) -> Int8(1966080,491520:32,480,1) ***************
[11/01/2021-22:23:08] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:08] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:08] [V] [TRT] Tactic: 1002 Time: 0.385148
[11/01/2021-22:23:08] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:08] [V] [TRT] Tactic: 0 Time: 7.08774
[11/01/2021-22:23:08] [V] [TRT] Fastest Tactic: 1002 Time: 0.385148
[11/01/2021-22:23:08] [V] [TRT] *************** Autotuning Reformat:Float(1966080,491520:32,480,1) -> Float(62914560,491520,480,1) ***************
[11/01/2021-22:23:08] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:08] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:08] [V] [TRT] Tactic: 1002 Time: 0.625164
[11/01/2021-22:23:08] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:08] [V] [TRT] Tactic: 0 Time: 2.7678
[11/01/2021-22:23:08] [V] [TRT] Fastest Tactic: 1002 Time: 0.625164
[11/01/2021-22:23:08] [V] [TRT] *************** Autotuning Reformat:Float(1966080,491520:32,480,1) -> Float(62914560,1,61440,128) ***************
[11/01/2021-22:23:08] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:08] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:08] [V] [TRT] Tactic: 1002 Time: 0.627432
[11/01/2021-22:23:08] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:08] [V] [TRT] Tactic: 0 Time: 0.618172
[11/01/2021-22:23:08] [V] [TRT] Fastest Tactic: 0 Time: 0.618172
[11/01/2021-22:23:08] [V] [TRT] *************** Autotuning Reformat:Float(1966080,491520:32,480,1) -> Float(15728640,1:4,15360,32) ***************
[11/01/2021-22:23:08] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:08] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:08] [V] [TRT] Tactic: 1002 Time: 0.625864
[11/01/2021-22:23:08] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:08] [V] [TRT] Tactic: 0 Time: 0.621968
[11/01/2021-22:23:08] [V] [TRT] Fastest Tactic: 0 Time: 0.621968
[11/01/2021-22:23:08] [V] [TRT] *************** Autotuning Reformat:Float(1966080,491520:32,480,1) -> Int8(15728640,491520:4,480,1) ***************
[11/01/2021-22:23:08] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:08] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:08] [V] [TRT] Tactic: 1002 Time: 0.384192
[11/01/2021-22:23:08] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:08] [V] [TRT] Tactic: 0 Time: 2.93482
[11/01/2021-22:23:08] [V] [TRT] Fastest Tactic: 1002 Time: 0.384192
[11/01/2021-22:23:08] [V] [TRT] *************** Autotuning Reformat:Float(1966080,491520:32,480,1) -> Int8(1966080,491520:32,480,1) ***************
[11/01/2021-22:23:08] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:08] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:08] [V] [TRT] Tactic: 1002 Time: 0.38358
[11/01/2021-22:23:08] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:08] [V] [TRT] Tactic: 0 Time: 6.56892
[11/01/2021-22:23:08] [V] [TRT] Fastest Tactic: 1002 Time: 0.38358
[11/01/2021-22:23:08] [V] [TRT] *************** Autotuning Reformat:Float(62914560,491520,480,1) -> Int8(15728640,491520:4,480,1) ***************
[11/01/2021-22:23:08] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:08] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:08] [V] [TRT] Tactic: 1002 Time: 0.384128
[11/01/2021-22:23:08] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:08] [V] [TRT] Tactic: 0 Time: 0.505984
[11/01/2021-22:23:08] [V] [TRT] Fastest Tactic: 1002 Time: 0.384128
[11/01/2021-22:23:08] [V] [TRT] *************** Autotuning Reformat:Float(62914560,491520,480,1) -> Int8(1966080,491520:32,480,1) ***************
[11/01/2021-22:23:08] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:08] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:08] [V] [TRT] Tactic: 1002 Time: 0.382896
[11/01/2021-22:23:08] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:09] [V] [TRT] Tactic: 0 Time: 4.4977
[11/01/2021-22:23:09] [V] [TRT] Fastest Tactic: 1002 Time: 0.382896
[11/01/2021-22:23:09] [V] [TRT] *************** Autotuning Reformat:Float(62914560,1,61440,128) -> Float(62914560,491520,480,1) ***************
[11/01/2021-22:23:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:09] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:09] [V] [TRT] Tactic: 1002 Time: 0.632932
[11/01/2021-22:23:09] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:09] [V] [TRT] Tactic: 0 Time: 3.07126
[11/01/2021-22:23:09] [V] [TRT] Fastest Tactic: 1002 Time: 0.632932
[11/01/2021-22:23:09] [V] [TRT] *************** Autotuning Reformat:Float(62914560,1,61440,128) -> Int8(15728640,491520:4,480,1) ***************
[11/01/2021-22:23:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:09] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:09] [V] [TRT] Tactic: 1002 Time: 0.384436
[11/01/2021-22:23:09] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:09] [V] [TRT] Tactic: 0 Time: 3.29369
[11/01/2021-22:23:09] [V] [TRT] Fastest Tactic: 1002 Time: 0.384436
[11/01/2021-22:23:09] [V] [TRT] *************** Autotuning Reformat:Float(62914560,1,61440,128) -> Int8(1966080,491520:32,480,1) ***************
[11/01/2021-22:23:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:09] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:09] [V] [TRT] Tactic: 1002 Time: 0.3839
[11/01/2021-22:23:09] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:09] [V] [TRT] Tactic: 0 Time: 7.51919
[11/01/2021-22:23:09] [V] [TRT] Fastest Tactic: 1002 Time: 0.3839
[11/01/2021-22:23:09] [V] [TRT] *************** Autotuning Reformat:Float(15728640,1:4,15360,32) -> Float(62914560,491520,480,1) ***************
[11/01/2021-22:23:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:09] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:09] [V] [TRT] Tactic: 1002 Time: 0.632856
[11/01/2021-22:23:09] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:09] [V] [TRT] Tactic: 0 Time: 3.05775
[11/01/2021-22:23:09] [V] [TRT] Fastest Tactic: 1002 Time: 0.632856
[11/01/2021-22:23:09] [V] [TRT] *************** Autotuning Reformat:Float(15728640,1:4,15360,32) -> Int8(15728640,491520:4,480,1) ***************
[11/01/2021-22:23:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:09] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:09] [V] [TRT] Tactic: 1002 Time: 0.385416
[11/01/2021-22:23:09] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:09] [V] [TRT] Tactic: 0 Time: 3.29036
[11/01/2021-22:23:09] [V] [TRT] Fastest Tactic: 1002 Time: 0.385416
[11/01/2021-22:23:09] [V] [TRT] *************** Autotuning Reformat:Float(15728640,1:4,15360,32) -> Int8(1966080,491520:32,480,1) ***************
[11/01/2021-22:23:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:09] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:09] [V] [TRT] Tactic: 1002 Time: 0.38538
[11/01/2021-22:23:09] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:09] [V] [TRT] Tactic: 0 Time: 7.48365
[11/01/2021-22:23:09] [V] [TRT] Fastest Tactic: 1002 Time: 0.38538
[11/01/2021-22:23:09] [V] [TRT] *************** Autotuning Reformat:Float(1966080,491520:32,480,1) -> Float(62914560,491520,480,1) ***************
[11/01/2021-22:23:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:09] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:09] [V] [TRT] Tactic: 1002 Time: 0.629632
[11/01/2021-22:23:09] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:09] [V] [TRT] Tactic: 0 Time: 2.72839
[11/01/2021-22:23:09] [V] [TRT] Fastest Tactic: 1002 Time: 0.629632
[11/01/2021-22:23:09] [V] [TRT] *************** Autotuning Reformat:Float(1966080,491520:32,480,1) -> Int8(15728640,491520:4,480,1) ***************
[11/01/2021-22:23:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:09] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:09] [V] [TRT] Tactic: 1002 Time: 0.384208
[11/01/2021-22:23:09] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:09] [V] [TRT] Tactic: 0 Time: 2.93898
[11/01/2021-22:23:09] [V] [TRT] Fastest Tactic: 1002 Time: 0.384208
[11/01/2021-22:23:09] [V] [TRT] *************** Autotuning Reformat:Float(1966080,491520:32,480,1) -> Int8(1966080,491520:32,480,1) ***************
[11/01/2021-22:23:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:09] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:09] [V] [TRT] Tactic: 1002 Time: 0.38344
[11/01/2021-22:23:09] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:09] [V] [TRT] Tactic: 0 Time: 6.63382
[11/01/2021-22:23:09] [V] [TRT] Fastest Tactic: 1002 Time: 0.38344
[11/01/2021-22:23:09] [V] [TRT] *************** Autotuning Reformat:Int8(15728640,491520:4,480,1) -> Float(62914560,491520,480,1) ***************
[11/01/2021-22:23:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:09] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:09] [V] [TRT] Tactic: 1002 Time: 0.42808
[11/01/2021-22:23:09] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:09] [V] [TRT] Tactic: 0 Time: 0.581648
[11/01/2021-22:23:09] [V] [TRT] Fastest Tactic: 1002 Time: 0.42808
[11/01/2021-22:23:09] [V] [TRT] *************** Autotuning Reformat:Int8(15728640,491520:4,480,1) -> Int8(1966080,491520:32,480,1) ***************
[11/01/2021-22:23:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:09] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:09] [V] [TRT] Tactic: 1002 Time: 0.279552
[11/01/2021-22:23:09] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:09] [V] [TRT] Tactic: 0 Time: 0.16236
[11/01/2021-22:23:09] [V] [TRT] Fastest Tactic: 0 Time: 0.16236
[11/01/2021-22:23:09] [V] [TRT] *************** Autotuning Reformat:Int8(1966080,491520:32,480,1) -> Float(62914560,491520,480,1) ***************
[11/01/2021-22:23:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:09] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:09] [V] [TRT] Tactic: 1002 Time: 0.40048
[11/01/2021-22:23:09] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:09] [V] [TRT] Tactic: 0 Time: 2.72634
[11/01/2021-22:23:09] [V] [TRT] Fastest Tactic: 1002 Time: 0.40048
[11/01/2021-22:23:09] [V] [TRT] *************** Autotuning Reformat:Int8(1966080,491520:32,480,1) -> Int8(15728640,491520:4,480,1) ***************
[11/01/2021-22:23:09] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:09] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:09] [V] [TRT] Tactic: 1002 Time: 0.27924
[11/01/2021-22:23:09] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:09] [V] [TRT] Tactic: 0 Time: 0.683296
[11/01/2021-22:23:09] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:10] [V] [TRT] Tactic: 1 Time: 0.184768
[11/01/2021-22:23:10] [V] [TRT] Fastest Tactic: 1 Time: 0.184768
[11/01/2021-22:23:10] [V] [TRT] *************** Autotuning format combination: Float(62914560,491520,480,1) -> Float(7864320,122880,240,1) ***************
[11/01/2021-22:23:10] [V] [TRT] --------------- Timing Runner: MaxPool_43 (TiledPooling)
[11/01/2021-22:23:10] [V] [TRT] Tactic: 5505281 Time: 0.445016
[11/01/2021-22:23:10] [V] [TRT] Tactic: 5570817 Time: 0.3972
[11/01/2021-22:23:10] [V] [TRT] Tactic: 5636353 Time: 0.391904
[11/01/2021-22:23:10] [V] [TRT] Tactic: 5701889 Time: 0.389728
[11/01/2021-22:23:10] [V] [TRT] Tactic: 5767425 Time: 0.388488
[11/01/2021-22:23:10] [V] [TRT] Tactic: 5832961 Time: 0.389632
[11/01/2021-22:23:10] [V] [TRT] Tactic: 5898497 Time: 0.388316
[11/01/2021-22:23:10] [V] [TRT] Tactic: 5964033 Time: 0.388704
[11/01/2021-22:23:10] [V] [TRT] Tactic: 6029569 Time: 0.43974
[11/01/2021-22:23:10] [V] [TRT] Tactic: 6095105 Time: 0.415736
[11/01/2021-22:23:10] [V] [TRT] Tactic: 6160641 Time: 0.412612
[11/01/2021-22:23:10] [V] [TRT] Tactic: 6226177 Time: 0.411736
[11/01/2021-22:23:10] [V] [TRT] Tactic: 6291713 Time: 0.411956
[11/01/2021-22:23:10] [V] [TRT] Tactic: 6357249 Time: 0.413056
[11/01/2021-22:23:10] [V] [TRT] Tactic: 6422785 Time: 0.412236
[11/01/2021-22:23:10] [V] [TRT] Tactic: 6488321 Time: 0.411436
[11/01/2021-22:23:10] [V] [TRT] Fastest Tactic: 5898497 Time: 0.388316
[11/01/2021-22:23:10] [V] [TRT] --------------- Timing Runner: MaxPool_43 (CudnnPooling)
[11/01/2021-22:23:10] [V] [TRT] Tactic: -1 Time: 0.381852
[11/01/2021-22:23:10] [V] [TRT] Fastest Tactic: -1 Time: 0.381852
[11/01/2021-22:23:10] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnPooling Tactic: -1
[11/01/2021-22:23:10] [V] [TRT] *************** Autotuning format combination: Int8(15728640,491520:4,480,1) -> Int8(1966080,122880:4,240,1) ***************
[11/01/2021-22:23:10] [V] [TRT] --------------- Timing Runner: MaxPool_43 (TiledPooling)
[11/01/2021-22:23:10] [V] [TRT] Tactic: 5505281 Time: 0.12702
[11/01/2021-22:23:10] [V] [TRT] Tactic: 5570817 Time: 0.10112
[11/01/2021-22:23:10] [V] [TRT] Tactic: 5636353 Time: 0.100216
[11/01/2021-22:23:10] [V] [TRT] Tactic: 5701889 Time: 0.099688
[11/01/2021-22:23:10] [V] [TRT] Tactic: 5767425 Time: 0.100224
[11/01/2021-22:23:10] [V] [TRT] Tactic: 5832961 Time: 0.09996
[11/01/2021-22:23:10] [V] [TRT] Tactic: 5898497 Time: 0.101108
[11/01/2021-22:23:10] [V] [TRT] Tactic: 5964033 Time: 0.101108
[11/01/2021-22:23:10] [V] [TRT] Tactic: 6029569 Time: 0.115016
[11/01/2021-22:23:10] [V] [TRT] Tactic: 6095105 Time: 0.09836
[11/01/2021-22:23:10] [V] [TRT] Tactic: 6160641 Time: 0.0985
[11/01/2021-22:23:10] [V] [TRT] Tactic: 6226177 Time: 0.099108
[11/01/2021-22:23:10] [V] [TRT] Tactic: 6291713 Time: 0.0992
[11/01/2021-22:23:10] [V] [TRT] Tactic: 6357249 Time: 0.100416
[11/01/2021-22:23:10] [V] [TRT] Tactic: 6422785 Time: 0.100096
[11/01/2021-22:23:10] [V] [TRT] Tactic: 6488321 Time: 0.100648
[11/01/2021-22:23:10] [V] [TRT] Fastest Tactic: 6095105 Time: 0.09836
[11/01/2021-22:23:10] [V] [TRT] --------------- Timing Runner: MaxPool_43 (CudaPooling)
[11/01/2021-22:23:10] [V] [TRT] Tactic: -3 Time: 0.0995
[11/01/2021-22:23:10] [V] [TRT] Fastest Tactic: -3 Time: 0.0995
[11/01/2021-22:23:10] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: TiledPooling Tactic: 6095105
[11/01/2021-22:23:10] [V] [TRT] *************** Autotuning format combination: Int8(1966080,491520:32,480,1) -> Int8(245760,122880:32,240,1) ***************
[11/01/2021-22:23:10] [V] [TRT] --------------- Timing Runner: MaxPool_43 (TiledPooling)
[11/01/2021-22:23:10] [V] [TRT] TiledPooling has no valid tactics for this config, skipping
[11/01/2021-22:23:10] [V] [TRT] --------------- Timing Runner: MaxPool_43 (CudaPooling)
[11/01/2021-22:23:10] [V] [TRT] Tactic: -4 Time: 0.0995
[11/01/2021-22:23:10] [V] [TRT] Fastest Tactic: -4 Time: 0.0995
[11/01/2021-22:23:10] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudaPooling Tactic: -4
[11/01/2021-22:23:10] [V] [TRT] *************** Autotuning Reformat:Float(7864320,122880,240,1) -> Float(7864320,1,15360,64) ***************
[11/01/2021-22:23:10] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:10] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:10] [V] [TRT] Tactic: 1002 Time: 0.16094
[11/01/2021-22:23:10] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:10] [V] [TRT] Tactic: 0 Time: 0.176516
[11/01/2021-22:23:10] [V] [TRT] Fastest Tactic: 1002 Time: 0.16094
[11/01/2021-22:23:10] [V] [TRT] *************** Autotuning Reformat:Float(7864320,122880,240,1) -> Float(1966080,1:4,3840,16) ***************
[11/01/2021-22:23:10] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:10] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:10] [V] [TRT] Tactic: 1002 Time: 0.160792
[11/01/2021-22:23:10] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:10] [V] [TRT] Tactic: 0 Time: 0.178052
[11/01/2021-22:23:10] [V] [TRT] Fastest Tactic: 1002 Time: 0.160792
[11/01/2021-22:23:10] [V] [TRT] *************** Autotuning Reformat:Float(7864320,122880,240,1) -> Float(245760,122880:32,240,1) ***************
[11/01/2021-22:23:10] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:10] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:10] [V] [TRT] Tactic: 1002 Time: 0.161152
[11/01/2021-22:23:10] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:10] [V] [TRT] Tactic: 0 Time: 1.16093
[11/01/2021-22:23:10] [V] [TRT] Fastest Tactic: 1002 Time: 0.161152
[11/01/2021-22:23:10] [V] [TRT] *************** Autotuning Reformat:Float(7864320,122880,240,1) -> Int8(1966080,122880:4,240,1) ***************
[11/01/2021-22:23:10] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:10] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:10] [V] [TRT] Tactic: 1002 Time: 0.100464
[11/01/2021-22:23:10] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:10] [V] [TRT] Tactic: 0 Time: 0.098516
[11/01/2021-22:23:10] [V] [TRT] Fastest Tactic: 0 Time: 0.098516
[11/01/2021-22:23:10] [V] [TRT] *************** Autotuning Reformat:Float(7864320,122880,240,1) -> Int8(245760,122880:32,240,1) ***************
[11/01/2021-22:23:10] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:10] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:10] [V] [TRT] Tactic: 1002 Time: 0.09984
[11/01/2021-22:23:10] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:10] [V] [TRT] Tactic: 0 Time: 0.854952
[11/01/2021-22:23:10] [V] [TRT] Fastest Tactic: 1002 Time: 0.09984
[11/01/2021-22:23:10] [V] [TRT] *************** Autotuning Reformat:Int8(1966080,122880:4,240,1) -> Float(7864320,122880,240,1) ***************
[11/01/2021-22:23:10] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:10] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:10] [V] [TRT] Tactic: 1002 Time: 0.11132
[11/01/2021-22:23:10] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:10] [V] [TRT] Tactic: 0 Time: 0.101532
[11/01/2021-22:23:10] [V] [TRT] Fastest Tactic: 0 Time: 0.101532
[11/01/2021-22:23:10] [V] [TRT] *************** Autotuning Reformat:Int8(1966080,122880:4,240,1) -> Float(7864320,1,15360,64) ***************
[11/01/2021-22:23:10] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:10] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:10] [V] [TRT] Tactic: 1002 Time: 0.103304
[11/01/2021-22:23:10] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:10] [V] [TRT] Tactic: 0 Time: 0.134288
[11/01/2021-22:23:10] [V] [TRT] Fastest Tactic: 1002 Time: 0.103304
[11/01/2021-22:23:10] [V] [TRT] *************** Autotuning Reformat:Int8(1966080,122880:4,240,1) -> Float(1966080,1:4,3840,16) ***************
[11/01/2021-22:23:10] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:10] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:10] [V] [TRT] Tactic: 1002 Time: 0.112788
[11/01/2021-22:23:10] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:10] [V] [TRT] Tactic: 0 Time: 0.147512
[11/01/2021-22:23:10] [V] [TRT] Fastest Tactic: 1002 Time: 0.112788
[11/01/2021-22:23:10] [V] [TRT] *************** Autotuning Reformat:Int8(1966080,122880:4,240,1) -> Float(245760,122880:32,240,1) ***************
[11/01/2021-22:23:10] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:10] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:10] [V] [TRT] Tactic: 1002 Time: 0.103628
[11/01/2021-22:23:10] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:10] [V] [TRT] Tactic: 0 Time: 1.04728
[11/01/2021-22:23:10] [V] [TRT] Fastest Tactic: 1002 Time: 0.103628
[11/01/2021-22:23:10] [V] [TRT] *************** Autotuning Reformat:Int8(1966080,122880:4,240,1) -> Int8(245760,122880:32,240,1) ***************
[11/01/2021-22:23:10] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:10] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:10] [V] [TRT] Tactic: 1002 Time: 0.073728
[11/01/2021-22:23:10] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:10] [V] [TRT] Tactic: 0 Time: 0.04254
[11/01/2021-22:23:10] [V] [TRT] Fastest Tactic: 0 Time: 0.04254
[11/01/2021-22:23:10] [V] [TRT] *************** Autotuning Reformat:Int8(245760,122880:32,240,1) -> Float(7864320,122880,240,1) ***************
[11/01/2021-22:23:10] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:10] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:10] [V] [TRT] Tactic: 1002 Time: 0.105256
[11/01/2021-22:23:10] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:10] [V] [TRT] Tactic: 0 Time: 0.569856
[11/01/2021-22:23:10] [V] [TRT] Fastest Tactic: 1002 Time: 0.105256
[11/01/2021-22:23:10] [V] [TRT] *************** Autotuning Reformat:Int8(245760,122880:32,240,1) -> Float(7864320,1,15360,64) ***************
[11/01/2021-22:23:10] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:10] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:10] [V] [TRT] Tactic: 1002 Time: 0.102556
[11/01/2021-22:23:10] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:10] [V] [TRT] Tactic: 0 Time: 0.129304
[11/01/2021-22:23:10] [V] [TRT] Fastest Tactic: 1002 Time: 0.102556
[11/01/2021-22:23:10] [V] [TRT] *************** Autotuning Reformat:Int8(245760,122880:32,240,1) -> Float(1966080,1:4,3840,16) ***************
[11/01/2021-22:23:10] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:10] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:10] [V] [TRT] Tactic: 1002 Time: 0.105088
[11/01/2021-22:23:10] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:10] [V] [TRT] Tactic: 0 Time: 0.146432
[11/01/2021-22:23:10] [V] [TRT] Fastest Tactic: 1002 Time: 0.105088
[11/01/2021-22:23:10] [V] [TRT] *************** Autotuning Reformat:Int8(245760,122880:32,240,1) -> Float(245760,122880:32,240,1) ***************
[11/01/2021-22:23:10] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:10] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:10] [V] [TRT] Tactic: 1002 Time: 0.103956
[11/01/2021-22:23:10] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:10] [V] [TRT] Tactic: 0 Time: 1.63494
[11/01/2021-22:23:10] [V] [TRT] Fastest Tactic: 1002 Time: 0.103956
[11/01/2021-22:23:10] [V] [TRT] *************** Autotuning Reformat:Int8(245760,122880:32,240,1) -> Int8(1966080,122880:4,240,1) ***************
[11/01/2021-22:23:10] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:10] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:10] [V] [TRT] Tactic: 1002 Time: 0.073728
[11/01/2021-22:23:10] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:10] [V] [TRT] Tactic: 0 Time: 0.143284
[11/01/2021-22:23:10] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:10] [V] [TRT] Tactic: 1 Time: 0.051252
[11/01/2021-22:23:10] [V] [TRT] Fastest Tactic: 1 Time: 0.051252
[11/01/2021-22:23:10] [V] [TRT] *************** Autotuning Reformat:Float(7864320,122880,240,1) -> Float(7864320,1,15360,64) ***************
[11/01/2021-22:23:10] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:10] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:10] [V] [TRT] Tactic: 1002 Time: 0.160924
[11/01/2021-22:23:10] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 0 Time: 0.176456
[11/01/2021-22:23:11] [V] [TRT] Fastest Tactic: 1002 Time: 0.160924
[11/01/2021-22:23:11] [V] [TRT] *************** Autotuning Reformat:Float(7864320,122880,240,1) -> Float(1966080,1:4,3840,16) ***************
[11/01/2021-22:23:11] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 1002 Time: 0.160964
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 0 Time: 0.178244
[11/01/2021-22:23:11] [V] [TRT] Fastest Tactic: 1002 Time: 0.160964
[11/01/2021-22:23:11] [V] [TRT] *************** Autotuning Reformat:Float(7864320,122880,240,1) -> Int8(1966080,122880:4,240,1) ***************
[11/01/2021-22:23:11] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 1002 Time: 0.100372
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 0 Time: 0.098728
[11/01/2021-22:23:11] [V] [TRT] Fastest Tactic: 0 Time: 0.098728
[11/01/2021-22:23:11] [V] [TRT] *************** Autotuning Reformat:Float(7864320,122880,240,1) -> Int8(245760,122880:32,240,1) ***************
[11/01/2021-22:23:11] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 1002 Time: 0.100128
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 0 Time: 0.850816
[11/01/2021-22:23:11] [V] [TRT] Fastest Tactic: 1002 Time: 0.100128
[11/01/2021-22:23:11] [V] [TRT] *************** Autotuning Reformat:Float(7864320,1,15360,64) -> Float(7864320,122880,240,1) ***************
[11/01/2021-22:23:11] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 1002 Time: 0.16128
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 0 Time: 0.688128
[11/01/2021-22:23:11] [V] [TRT] Fastest Tactic: 1002 Time: 0.16128
[11/01/2021-22:23:11] [V] [TRT] *************** Autotuning Reformat:Float(7864320,1,15360,64) -> Float(1966080,1:4,3840,16) ***************
[11/01/2021-22:23:11] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 1002 Time: 0.160612
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 0 Time: 0.156604
[11/01/2021-22:23:11] [V] [TRT] Fastest Tactic: 0 Time: 0.156604
[11/01/2021-22:23:11] [V] [TRT] *************** Autotuning Reformat:Float(7864320,1,15360,64) -> Int8(1966080,122880:4,240,1) ***************
[11/01/2021-22:23:11] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 1002 Time: 0.100648
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 0 Time: 0.728244
[11/01/2021-22:23:11] [V] [TRT] Fastest Tactic: 1002 Time: 0.100648
[11/01/2021-22:23:11] [V] [TRT] *************** Autotuning Reformat:Float(7864320,1,15360,64) -> Int8(245760,122880:32,240,1) ***************
[11/01/2021-22:23:11] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 1002 Time: 0.100392
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 0 Time: 1.7606
[11/01/2021-22:23:11] [V] [TRT] Fastest Tactic: 1002 Time: 0.100392
[11/01/2021-22:23:11] [V] [TRT] *************** Autotuning Reformat:Float(1966080,1:4,3840,16) -> Float(7864320,122880,240,1) ***************
[11/01/2021-22:23:11] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 1002 Time: 0.161024
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 0 Time: 0.69166
[11/01/2021-22:23:11] [V] [TRT] Fastest Tactic: 1002 Time: 0.161024
[11/01/2021-22:23:11] [V] [TRT] *************** Autotuning Reformat:Float(1966080,1:4,3840,16) -> Float(7864320,1,15360,64) ***************
[11/01/2021-22:23:11] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 1002 Time: 0.159744
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 0 Time: 0.156672
[11/01/2021-22:23:11] [V] [TRT] Fastest Tactic: 0 Time: 0.156672
[11/01/2021-22:23:11] [V] [TRT] *************** Autotuning Reformat:Float(1966080,1:4,3840,16) -> Int8(1966080,122880:4,240,1) ***************
[11/01/2021-22:23:11] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 1002 Time: 0.101816
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 0 Time: 0.728992
[11/01/2021-22:23:11] [V] [TRT] Fastest Tactic: 1002 Time: 0.101816
[11/01/2021-22:23:11] [V] [TRT] *************** Autotuning Reformat:Float(1966080,1:4,3840,16) -> Int8(245760,122880:32,240,1) ***************
[11/01/2021-22:23:11] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 1002 Time: 0.10184
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 0 Time: 1.75501
[11/01/2021-22:23:11] [V] [TRT] Fastest Tactic: 1002 Time: 0.10184
[11/01/2021-22:23:11] [V] [TRT] *************** Autotuning Reformat:Float(245760,122880:32,240,1) -> Float(7864320,122880,240,1) ***************
[11/01/2021-22:23:11] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 1002 Time: 0.162576
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 0 Time: 0.685108
[11/01/2021-22:23:11] [V] [TRT] Fastest Tactic: 1002 Time: 0.162576
[11/01/2021-22:23:11] [V] [TRT] *************** Autotuning Reformat:Float(245760,122880:32,240,1) -> Float(7864320,1,15360,64) ***************
[11/01/2021-22:23:11] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 1002 Time: 0.159872
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 0 Time: 0.157044
[11/01/2021-22:23:11] [V] [TRT] Fastest Tactic: 0 Time: 0.157044
[11/01/2021-22:23:11] [V] [TRT] *************** Autotuning Reformat:Float(245760,122880:32,240,1) -> Float(1966080,1:4,3840,16) ***************
[11/01/2021-22:23:11] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 1002 Time: 0.159812
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 0 Time: 0.159372
[11/01/2021-22:23:11] [V] [TRT] Fastest Tactic: 0 Time: 0.159372
[11/01/2021-22:23:11] [V] [TRT] *************** Autotuning Reformat:Float(245760,122880:32,240,1) -> Int8(1966080,122880:4,240,1) ***************
[11/01/2021-22:23:11] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 1002 Time: 0.100592
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 0 Time: 0.723688
[11/01/2021-22:23:11] [V] [TRT] Fastest Tactic: 1002 Time: 0.100592
[11/01/2021-22:23:11] [V] [TRT] *************** Autotuning Reformat:Float(245760,122880:32,240,1) -> Int8(245760,122880:32,240,1) ***************
[11/01/2021-22:23:11] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 1002 Time: 0.100608
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 0 Time: 1.63395
[11/01/2021-22:23:11] [V] [TRT] Fastest Tactic: 1002 Time: 0.100608
[11/01/2021-22:23:11] [V] [TRT] *************** Autotuning Reformat:Int8(1966080,122880:4,240,1) -> Float(7864320,122880,240,1) ***************
[11/01/2021-22:23:11] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 1002 Time: 0.111496
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 0 Time: 0.10036
[11/01/2021-22:23:11] [V] [TRT] Fastest Tactic: 0 Time: 0.10036
[11/01/2021-22:23:11] [V] [TRT] *************** Autotuning Reformat:Int8(1966080,122880:4,240,1) -> Float(7864320,1,15360,64) ***************
[11/01/2021-22:23:11] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 1002 Time: 0.1028
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 0 Time: 0.13416
[11/01/2021-22:23:11] [V] [TRT] Fastest Tactic: 1002 Time: 0.1028
[11/01/2021-22:23:11] [V] [TRT] *************** Autotuning Reformat:Int8(1966080,122880:4,240,1) -> Float(1966080,1:4,3840,16) ***************
[11/01/2021-22:23:11] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 1002 Time: 0.113692
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 0 Time: 0.147672
[11/01/2021-22:23:11] [V] [TRT] Fastest Tactic: 1002 Time: 0.113692
[11/01/2021-22:23:11] [V] [TRT] *************** Autotuning Reformat:Int8(1966080,122880:4,240,1) -> Int8(245760,122880:32,240,1) ***************
[11/01/2021-22:23:11] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 1002 Time: 0.073728
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 0 Time: 0.042416
[11/01/2021-22:23:11] [V] [TRT] Fastest Tactic: 0 Time: 0.042416
[11/01/2021-22:23:11] [V] [TRT] *************** Autotuning Reformat:Int8(245760,122880:32,240,1) -> Float(7864320,122880,240,1) ***************
[11/01/2021-22:23:11] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 1002 Time: 0.105472
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 0 Time: 0.567584
[11/01/2021-22:23:11] [V] [TRT] Fastest Tactic: 1002 Time: 0.105472
[11/01/2021-22:23:11] [V] [TRT] *************** Autotuning Reformat:Int8(245760,122880:32,240,1) -> Float(7864320,1,15360,64) ***************
[11/01/2021-22:23:11] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 1002 Time: 0.102872
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 0 Time: 0.129324
[11/01/2021-22:23:11] [V] [TRT] Fastest Tactic: 1002 Time: 0.102872
[11/01/2021-22:23:11] [V] [TRT] *************** Autotuning Reformat:Int8(245760,122880:32,240,1) -> Float(1966080,1:4,3840,16) ***************
[11/01/2021-22:23:11] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 1002 Time: 0.105276
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 0 Time: 0.146296
[11/01/2021-22:23:11] [V] [TRT] Fastest Tactic: 1002 Time: 0.105276
[11/01/2021-22:23:11] [V] [TRT] *************** Autotuning Reformat:Int8(245760,122880:32,240,1) -> Int8(1966080,122880:4,240,1) ***************
[11/01/2021-22:23:11] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 1002 Time: 0.07368
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 0 Time: 0.143928
[11/01/2021-22:23:11] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:11] [V] [TRT] Tactic: 1 Time: 0.051244
[11/01/2021-22:23:11] [V] [TRT] Fastest Tactic: 1 Time: 0.051244
[11/01/2021-22:23:11] [V] [TRT] *************** Autotuning format combination: Float(7864320,122880,240,1) -> Float(7864320,122880,240,1) ***************
[11/01/2021-22:23:11] [V] [TRT] --------------- Timing Runner: Conv_44 (CudaDepthwiseConvolution)
[11/01/2021-22:23:11] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[11/01/2021-22:23:11] [V] [TRT] --------------- Timing Runner: Conv_44 (FusedConvActConvolution)
[11/01/2021-22:23:11] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[11/01/2021-22:23:11] [V] [TRT] --------------- Timing Runner: Conv_44 (CudnnConvolution)
[11/01/2021-22:23:11] [V] [TRT] Tactic: 0 Time: 1.75308
[11/01/2021-22:23:11] [V] [TRT] Tactic: 1 Time: 0.961004
[11/01/2021-22:23:11] [V] [TRT] Tactic: 2 Time: 2.47581
[11/01/2021-22:23:11] [V] [TRT] Tactic: 5 Time: 8.26853
[11/01/2021-22:23:11] [V] [TRT] Tactic: 6 Time: 0.695016
[11/01/2021-22:23:11] [V] [TRT] Tactic: 56 Time: 1.75976
[11/01/2021-22:23:11] [V] [TRT] Tactic: 57 Time: 0.968352
[11/01/2021-22:23:11] [V] [TRT] Tactic: 58 Time: 2.4815
[11/01/2021-22:23:11] [V] [TRT] Tactic: 61 Time: 8.33505
[11/01/2021-22:23:11] [V] [TRT] Tactic: 62 Time: 0.693368
[11/01/2021-22:23:11] [V] [TRT] Tactic: 112 Time: 1.76014
[11/01/2021-22:23:11] [V] [TRT] Tactic: 113 Time: 0.877464
[11/01/2021-22:23:12] [V] [TRT] Tactic: 114 Time: 2.48941
[11/01/2021-22:23:12] [V] [TRT] Tactic: 117 Time: 8.36743
[11/01/2021-22:23:12] [V] [TRT] Tactic: 118 Time: 0.694076
[11/01/2021-22:23:12] [V] [TRT] Fastest Tactic: 62 Time: 0.693368
[11/01/2021-22:23:12] [V] [TRT] --------------- Timing Runner: Conv_44 (CaskConvolution)
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_scudnn_128x64_relu_small_nn_v1 Tactic: 4549827808004681195
[11/01/2021-22:23:12] [V] [TRT] Tactic: 4549827808004681195 Time: 0.743496
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_scudnn_128x128_relu_small_nn_v1 Tactic: 5779835512569528575
[11/01/2021-22:23:12] [V] [TRT] Tactic: 5779835512569528575 Time: 1.31089
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_scudnn_128x128_relu_xregs_large_nn_v1 Tactic: 6053873026024413720
[11/01/2021-22:23:12] [V] [TRT] Tactic: 6053873026024413720 Time: 1.39982
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_scudnn_128x64_relu_xregs_large_nn_v1 Tactic: 6767548733843469815
[11/01/2021-22:23:12] [V] [TRT] Tactic: 6767548733843469815 Time: 0.71574
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_scudnn_128x32_relu_small_nn_v1 Tactic: -6313876406580483184
[11/01/2021-22:23:12] [V] [TRT] Tactic: -6313876406580483184 Time: 0.826652
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_scudnn_128x128_relu_medium_nn_v1 Tactic: -1123676555321336786
[11/01/2021-22:23:12] [V] [TRT] Tactic: -1123676555321336786 Time: 1.35404
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_scudnn_128x64_relu_medium_nn_v1 Tactic: -701551393537224327
[11/01/2021-22:23:12] [V] [TRT] Tactic: -701551393537224327 Time: 0.766212
[11/01/2021-22:23:12] [V] [TRT] Fastest Tactic: 6767548733843469815 Time: 0.71574
[11/01/2021-22:23:12] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 62
[11/01/2021-22:23:12] [V] [TRT] *************** Autotuning format combination: Float(7864320,1,15360,64) -> Float(7864320,1,15360,64) ***************
[11/01/2021-22:23:12] [V] [TRT] --------------- Timing Runner: Conv_44 (CudnnConvolution)
[11/01/2021-22:23:12] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[11/01/2021-22:23:12] [V] [TRT] --------------- Timing Runner: Conv_44 (CaskConvolution)
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 2860655430572478466
[11/01/2021-22:23:12] [V] [TRT] Tactic: 2860655430572478466 Time: 0.739104
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4474630279712975759
[11/01/2021-22:23:12] [V] [TRT] Tactic: 4474630279712975759 Time: 0.813952
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 4479823862704990365
[11/01/2021-22:23:12] [V] [TRT] Tactic: 4479823862704990365 Time: 0.803068
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4696204239951173149
[11/01/2021-22:23:12] [V] [TRT] Tactic: 4696204239951173149 Time: 0.748448
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 5778138195697110003
[11/01/2021-22:23:12] [V] [TRT] Tactic: 5778138195697110003 Time: 1.32112
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 7155825427510256858
[11/01/2021-22:23:12] [V] [TRT] Tactic: 7155825427510256858 Time: 1.27771
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 8918020581761223752
[11/01/2021-22:23:12] [V] [TRT] Tactic: 8918020581761223752 Time: 1.26381
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: -4756382386362004279
[11/01/2021-22:23:12] [V] [TRT] Tactic: -4756382386362004279 Time: 0.734392
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: -3855385237722507464
[11/01/2021-22:23:12] [V] [TRT] Tactic: -3855385237722507464 Time: 1.3219
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: -2809379259463049391
[11/01/2021-22:23:12] [V] [TRT] Tactic: -2809379259463049391 Time: 1.32031
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: -504296718212024303
[11/01/2021-22:23:12] [V] [TRT] Tactic: -504296718212024303 Time: 1.26491
[11/01/2021-22:23:12] [V] [TRT] Fastest Tactic: -4756382386362004279 Time: 0.734392
[11/01/2021-22:23:12] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -4756382386362004279
[11/01/2021-22:23:12] [V] [TRT] *************** Autotuning format combination: Float(1966080,1:4,3840,16) -> Float(1966080,1:4,3840,16) ***************
[11/01/2021-22:23:12] [V] [TRT] --------------- Timing Runner: Conv_44 (CudnnConvolution)
[11/01/2021-22:23:12] [V] [TRT] CudnnConvolution has no valid tactics for this config, skipping
[11/01/2021-22:23:12] [V] [TRT] --------------- Timing Runner: Conv_44 (CaskConvolution)
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 2860655430572478466
[11/01/2021-22:23:12] [V] [TRT] Tactic: 2860655430572478466 Time: 0.739488
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4474630279712975759
[11/01/2021-22:23:12] [V] [TRT] Tactic: 4474630279712975759 Time: 0.814156
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_scudnn_128x32_sliced1x4_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: 4479823862704990365
[11/01/2021-22:23:12] [V] [TRT] Tactic: 4479823862704990365 Time: 0.803256
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 4696204239951173149
[11/01/2021-22:23:12] [V] [TRT] Tactic: 4696204239951173149 Time: 0.742668
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_scudnn_128x128_relu_exp_small_nhwc_tn_v1 Tactic: 5778138195697110003
[11/01/2021-22:23:12] [V] [TRT] Tactic: 5778138195697110003 Time: 1.3067
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: 7155825427510256858
[11/01/2021-22:23:12] [V] [TRT] Tactic: 7155825427510256858 Time: 1.27364
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: 7342025736444949634
[11/01/2021-22:23:12] [V] [TRT] Tactic: 7342025736444949634 Time: 0.938092
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_medium_nhwc_tn_v1 Tactic: 8918020581761223752
[11/01/2021-22:23:12] [V] [TRT] Tactic: 8918020581761223752 Time: 1.26329
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8_t1r3s3 Tactic: -7377458734869418330
[11/01/2021-22:23:12] [V] [TRT] Tactic: -7377458734869418330 Time: 0.927632
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_f32f32_tf32f32_f32_nhwckrsc_nhwc_tilesize128x128x16_stage4_warpsize2x2x1_g1_tensor16x8x8 Tactic: -5457304872213719461
[11/01/2021-22:23:12] [V] [TRT] Tactic: -5457304872213719461 Time: 0.93516
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_scudnn_128x64_sliced1x2_ldg4_relu_exp_large_nhwc_tn_v1 Tactic: -4756382386362004279
[11/01/2021-22:23:12] [V] [TRT] Tactic: -4756382386362004279 Time: 0.733872
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_scudnn_128x128_relu_exp_large_nhwc_tn_v1 Tactic: -3855385237722507464
[11/01/2021-22:23:12] [V] [TRT] Tactic: -3855385237722507464 Time: 1.32134
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_scudnn_128x128_relu_exp_medium_nhwc_tn_v1 Tactic: -2809379259463049391
[11/01/2021-22:23:12] [V] [TRT] Tactic: -2809379259463049391 Time: 1.33381
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_scudnn_128x128_ldg4_relu_exp_small_nhwc_tn_v1 Tactic: -504296718212024303
[11/01/2021-22:23:12] [V] [TRT] Tactic: -504296718212024303 Time: 1.2644
[11/01/2021-22:23:12] [V] [TRT] Fastest Tactic: -4756382386362004279 Time: 0.733872
[11/01/2021-22:23:12] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -4756382386362004279
[11/01/2021-22:23:12] [V] [TRT] *************** Autotuning format combination: Int8(1966080,122880:4,240,1) -> Float(7864320,122880,240,1) ***************
[11/01/2021-22:23:12] [V] [TRT] --------------- Timing Runner: Conv_44 (CudaDepthwiseConvolution)
[11/01/2021-22:23:12] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[11/01/2021-22:23:12] [V] [TRT] --------------- Timing Runner: Conv_44 (CaskConvolution)
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: 1332468635798226953
[11/01/2021-22:23:12] [V] [TRT] Tactic: 1332468635798226953 Time: 0.54748
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: 1508480131241957639
[11/01/2021-22:23:12] [V] [TRT] Tactic: 1508480131241957639 Time: 0.439368
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: 1947019689364377201
[11/01/2021-22:23:12] [V] [TRT] Tactic: 1947019689364377201 Time: 0.229632
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: 3239257003214966313
[11/01/2021-22:23:12] [V] [TRT] Tactic: 3239257003214966313 Time: 0.439324
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: 5592640619112287921
[11/01/2021-22:23:12] [V] [TRT] Tactic: 5592640619112287921 Time: 0.235392
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 7621465827583909090
[11/01/2021-22:23:12] [V] [TRT] Tactic: 7621465827583909090 Time: 0.230812
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: -5576936487443445631
[11/01/2021-22:23:12] [V] [TRT] Tactic: -5576936487443445631 Time: 0.232372
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: -2297737319934264721
[11/01/2021-22:23:12] [V] [TRT] Tactic: -2297737319934264721 Time: 0.249404
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: -1425085658556684465
[11/01/2021-22:23:12] [V] [TRT] Tactic: -1425085658556684465 Time: 0.24512
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -108011214168778087
[11/01/2021-22:23:12] [V] [TRT] Tactic: -108011214168778087 Time: 0.247236
[11/01/2021-22:23:12] [V] [TRT] Fastest Tactic: 1947019689364377201 Time: 0.229632
[11/01/2021-22:23:12] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 1947019689364377201
[11/01/2021-22:23:12] [V] [TRT] *************** Autotuning format combination: Int8(1966080,122880:4,240,1) -> Int8(1966080,122880:4,240,1) ***************
[11/01/2021-22:23:12] [V] [TRT] --------------- Timing Runner: Conv_44 (CudaDepthwiseConvolution)
[11/01/2021-22:23:12] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[11/01/2021-22:23:12] [V] [TRT] --------------- Timing Runner: Conv_44 (FusedConvActConvolution)
[11/01/2021-22:23:12] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[11/01/2021-22:23:12] [V] [TRT] --------------- Timing Runner: Conv_44 (CaskConvolution)
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_nn_v1 Tactic: 175853789719975416
[11/01/2021-22:23:12] [V] [TRT] Tactic: 175853789719975416 Time: 0.237984
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_nn_v1 Tactic: 2171150287007712632
[11/01/2021-22:23:12] [V] [TRT] Tactic: 2171150287007712632 Time: 0.229376
[11/01/2021-22:23:12] [V] [TRT] Conv_44 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 2234457234705232274
[11/01/2021-22:23:13] [V] [TRT] Tactic: 2234457234705232274 Time: 0.223388
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_nn_v1 Tactic: 5834048089706882838
[11/01/2021-22:23:13] [V] [TRT] Tactic: 5834048089706882838 Time: 0.224792
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: -8626990807754934295
[11/01/2021-22:23:13] [V] [TRT] Tactic: -8626990807754934295 Time: 0.236108
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_nn_v1 Tactic: -7303593854972602201
[11/01/2021-22:23:13] [V] [TRT] Tactic: -7303593854972602201 Time: 0.222656
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_nn_v1 Tactic: -6585664687867083638
[11/01/2021-22:23:13] [V] [TRT] Tactic: -6585664687867083638 Time: 0.4269
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_xregs_large_nn_v1 Tactic: -3730012925709297561
[11/01/2021-22:23:13] [V] [TRT] Tactic: -3730012925709297561 Time: 0.220436
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_xregs_large_nn_v1 Tactic: -2277259417488004546
[11/01/2021-22:23:13] [V] [TRT] Tactic: -2277259417488004546 Time: 0.497252
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_nn_v1 Tactic: -683636008127039856
[11/01/2021-22:23:13] [V] [TRT] Tactic: -683636008127039856 Time: 0.42674
[11/01/2021-22:23:13] [V] [TRT] Fastest Tactic: -3730012925709297561 Time: 0.220436
[11/01/2021-22:23:13] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -3730012925709297561
[11/01/2021-22:23:13] [V] [TRT] *************** Autotuning format combination: Int8(1966080,122880:4,240,1) -> Int8(245760,122880:32,240,1) ***************
[11/01/2021-22:23:13] [V] [TRT] --------------- Timing Runner: Conv_44 (CaskConvolution)
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_xregs_large_c32_nn_v1 Tactic: 984309058095623735
[11/01/2021-22:23:13] [V] [TRT] Tactic: 984309058095623735 Time: 0.220416
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 1100922622480907544
[11/01/2021-22:23:13] [V] [TRT] Tactic: 1100922622480907544 Time: 0.235696
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_xregs_large_c32_nn_v1 Tactic: 3238312825609165543
[11/01/2021-22:23:13] [V] [TRT] Tactic: 3238312825609165543 Time: 0.493524
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_medium_c32_nn_v1 Tactic: 3606311198834416176
[11/01/2021-22:23:13] [V] [TRT] Tactic: 3606311198834416176 Time: 0.225144
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_small_c32_nn_v1 Tactic: 4325765560739862899
[11/01/2021-22:23:13] [V] [TRT] Tactic: 4325765560739862899 Time: 0.426284
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x128_relu_medium_c32_nn_v1 Tactic: -4255737803793506479
[11/01/2021-22:23:13] [V] [TRT] Tactic: -4255737803793506479 Time: 0.4264
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_small_c32_nn_v1 Tactic: -3958182351168863467
[11/01/2021-22:23:13] [V] [TRT] Tactic: -3958182351168863467 Time: 0.222312
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_medium_c32_nn_v1 Tactic: -3111968753064955248
[11/01/2021-22:23:13] [V] [TRT] Tactic: -3111968753064955248 Time: 0.228992
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: -1492575840277333548
[11/01/2021-22:23:13] [V] [TRT] Tactic: -1492575840277333548 Time: 0.237724
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_small_c32_nn_v1 Tactic: -868495160148524802
[11/01/2021-22:23:13] [V] [TRT] Tactic: -868495160148524802 Time: 0.223404
[11/01/2021-22:23:13] [V] [TRT] Fastest Tactic: 984309058095623735 Time: 0.220416
[11/01/2021-22:23:13] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 984309058095623735
[11/01/2021-22:23:13] [V] [TRT] *************** Autotuning format combination: Int8(245760,122880:32,240,1) -> Float(245760,122880:32,240,1) ***************
[11/01/2021-22:23:13] [V] [TRT] --------------- Timing Runner: Conv_44 (CaskConvolution)
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 1011019097971850911
[11/01/2021-22:23:13] [V] [TRT] Tactic: 1011019097971850911 Time: 0.130996
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 1071114551801767124
[11/01/2021-22:23:13] [V] [TRT] Tactic: 1071114551801767124 Time: 0.18146
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 2623576043214044314
[11/01/2021-22:23:13] [V] [TRT] Tactic: 2623576043214044314 Time: 0.156836
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 3281631721811475881
[11/01/2021-22:23:13] [V] [TRT] Tactic: 3281631721811475881 Time: 0.110464
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 4551754795416974366
[11/01/2021-22:23:13] [V] [TRT] Tactic: 4551754795416974366 Time: 0.15566
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 4925112190271421402
[11/01/2021-22:23:13] [V] [TRT] Tactic: 4925112190271421402 Time: 0.1586
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 5012796702462679112
[11/01/2021-22:23:13] [V] [TRT] Tactic: 5012796702462679112 Time: 0.186496
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 5041593333398049019
[11/01/2021-22:23:13] [V] [TRT] Tactic: 5041593333398049019 Time: 0.138196
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 5166018662410176512
[11/01/2021-22:23:13] [V] [TRT] Tactic: 5166018662410176512 Time: 0.18286
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 6191867932654611882
[11/01/2021-22:23:13] [V] [TRT] Tactic: 6191867932654611882 Time: 0.175504
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 6556170942941957134
[11/01/2021-22:23:13] [V] [TRT] Tactic: 6556170942941957134 Time: 0.149664
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 6852868042694587230
[11/01/2021-22:23:13] [V] [TRT] Tactic: 6852868042694587230 Time: 0.112632
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 8399092794516815300
[11/01/2021-22:23:13] [V] [TRT] Tactic: 8399092794516815300 Time: 0.287008
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -9132922677633967263
[11/01/2021-22:23:13] [V] [TRT] Tactic: -9132922677633967263 Time: 0.19772
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -7988637803896331454
[11/01/2021-22:23:13] [V] [TRT] Tactic: -7988637803896331454 Time: 0.145472
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: -7865001268126363229
[11/01/2021-22:23:13] [V] [TRT] Tactic: -7865001268126363229 Time: 0.1239
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -7606074703023778034
[11/01/2021-22:23:13] [V] [TRT] Tactic: -7606074703023778034 Time: 0.116784
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: -7413564913826321357
[11/01/2021-22:23:13] [V] [TRT] Tactic: -7413564913826321357 Time: 0.131584
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -7282232519526877434
[11/01/2021-22:23:13] [V] [TRT] Tactic: -7282232519526877434 Time: 0.181524
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -5942379529065248478
[11/01/2021-22:23:13] [V] [TRT] Tactic: -5942379529065248478 Time: 0.12918
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -5603587790314027122
[11/01/2021-22:23:13] [V] [TRT] Tactic: -5603587790314027122 Time: 0.116224
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: -5334776871777565833
[11/01/2021-22:23:13] [V] [TRT] Tactic: -5334776871777565833 Time: 0.188832
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: -5157868397078537095
[11/01/2021-22:23:13] [V] [TRT] Tactic: -5157868397078537095 Time: 0.180588
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -5100834417027499764
[11/01/2021-22:23:13] [V] [TRT] Tactic: -5100834417027499764 Time: 0.142076
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -3365360067423513506
[11/01/2021-22:23:13] [V] [TRT] Tactic: -3365360067423513506 Time: 0.15022
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -2194148180068068313
[11/01/2021-22:23:13] [V] [TRT] Tactic: -2194148180068068313 Time: 0.190228
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -1782593837177056527
[11/01/2021-22:23:13] [V] [TRT] Tactic: -1782593837177056527 Time: 0.135672
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: -1610768292520086910
[11/01/2021-22:23:13] [V] [TRT] Tactic: -1610768292520086910 Time: 0.131484
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: -1573035963956198975
[11/01/2021-22:23:13] [V] [TRT] Tactic: -1573035963956198975 Time: 0.283056
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_fp32_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -1558762241666006941
[11/01/2021-22:23:13] [V] [TRT] Tactic: -1558762241666006941 Time: 0.156416
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: -1365353082499976145
[11/01/2021-22:23:13] [V] [TRT] Tactic: -1365353082499976145 Time: 0.119448
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_fp32_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -621838502160440068
[11/01/2021-22:23:13] [V] [TRT] Tactic: -621838502160440068 Time: 0.129328
[11/01/2021-22:23:13] [V] [TRT] Fastest Tactic: 3281631721811475881 Time: 0.110464
[11/01/2021-22:23:13] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 3281631721811475881
[11/01/2021-22:23:13] [V] [TRT] *************** Autotuning format combination: Int8(245760,122880:32,240,1) -> Int8(245760,122880:32,240,1) ***************
[11/01/2021-22:23:13] [V] [TRT] --------------- Timing Runner: Conv_44 (CudaGroupConvolution)
[11/01/2021-22:23:13] [V] [TRT] CudaGroupConvolution has no valid tactics for this config, skipping
[11/01/2021-22:23:13] [V] [TRT] --------------- Timing Runner: Conv_44 (CudaDepthwiseConvolution)
[11/01/2021-22:23:13] [V] [TRT] CudaDepthwiseConvolution has no valid tactics for this config, skipping
[11/01/2021-22:23:13] [V] [TRT] --------------- Timing Runner: Conv_44 (FusedConvActConvolution)
[11/01/2021-22:23:13] [V] [TRT] FusedConvActConvolution has no valid tactics for this config, skipping
[11/01/2021-22:23:13] [V] [TRT] --------------- Timing Runner: Conv_44 (CaskConvolution)
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_epifadd Tactic: 177040020707947851
[11/01/2021-22:23:13] [V] [TRT] Tactic: 177040020707947851 Time: 0.13446
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: 1550399266192842845
[11/01/2021-22:23:13] [V] [TRT] Tactic: 1550399266192842845 Time: 0.139768
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 1572887561103143487
[11/01/2021-22:23:13] [V] [TRT] Tactic: 1572887561103143487 Time: 0.101704
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 2325023763229477890
[11/01/2021-22:23:13] [V] [TRT] Tactic: 2325023763229477890 Time: 0.090336
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_int8_i8816cudnn_int8_128x128_ldg16_relu_medium_nt_v1 Tactic: 2985940154541537814
[11/01/2021-22:23:13] [V] [TRT] Tactic: 2985940154541537814 Time: 0.135168
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 3284282970967328046
[11/01/2021-22:23:13] [V] [TRT] Tactic: 3284282970967328046 Time: 0.147528
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 3401614690060226673
[11/01/2021-22:23:13] [V] [TRT] Tactic: 3401614690060226673 Time: 0.122196
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 3512426920013359699
[11/01/2021-22:23:13] [V] [TRT] Tactic: 3512426920013359699 Time: 0.08652
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x128_ldg16_relu_medium_nt_v1 Tactic: 3899284354987683408
[11/01/2021-22:23:13] [V] [TRT] Tactic: 3899284354987683408 Time: 0.164556
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 4042202769383439184
[11/01/2021-22:23:13] [V] [TRT] Tactic: 4042202769383439184 Time: 0.185488
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_large_nt_v1 Tactic: 4182625619810185112
[11/01/2021-22:23:13] [V] [TRT] Tactic: 4182625619810185112 Time: 0.084096
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_epifadd Tactic: 4259547356717612415
[11/01/2021-22:23:13] [V] [TRT] Tactic: 4259547356717612415 Time: 0.110276
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_small_nt_v1 Tactic: 4717285412741024953
[11/01/2021-22:23:13] [V] [TRT] Tactic: 4717285412741024953 Time: 0.080528
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 4734519122557206480
[11/01/2021-22:23:13] [V] [TRT] Tactic: 4734519122557206480 Time: 0.274792
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_epifadd Tactic: 5121596860264626879
[11/01/2021-22:23:13] [V] [TRT] Tactic: 5121596860264626879 Time: 0.273648
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 5136656982162849059
[11/01/2021-22:23:13] [V] [TRT] Tactic: 5136656982162849059 Time: 0.14838
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: 5158259316594207439
[11/01/2021-22:23:13] [V] [TRT] Tactic: 5158259316594207439 Time: 0.1871
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 5966973378912044513
[11/01/2021-22:23:13] [V] [TRT] Tactic: 5966973378912044513 Time: 0.088084
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 6004789655466615912
[11/01/2021-22:23:13] [V] [TRT] Tactic: 6004789655466615912 Time: 0.102956
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 6146901278630392829
[11/01/2021-22:23:13] [V] [TRT] Tactic: 6146901278630392829 Time: 0.269684
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_epifadd Tactic: 6434020722187266170
[11/01/2021-22:23:13] [V] [TRT] Tactic: 6434020722187266170 Time: 0.150564
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 6781129591847482048
[11/01/2021-22:23:13] [V] [TRT] Tactic: 6781129591847482048 Time: 0.100644
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 7191893591576074000
[11/01/2021-22:23:13] [V] [TRT] Tactic: 7191893591576074000 Time: 0.1293
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 7438984192263206338
[11/01/2021-22:23:13] [V] [TRT] Tactic: 7438984192263206338 Time: 0.172592
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: 7504901284678552178
[11/01/2021-22:23:13] [V] [TRT] Tactic: 7504901284678552178 Time: 0.154736
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 8096257414008860171
[11/01/2021-22:23:13] [V] [TRT] Tactic: 8096257414008860171 Time: 0.094796
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: 9143438935315839085
[11/01/2021-22:23:13] [V] [TRT] Tactic: 9143438935315839085 Time: 0.123252
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: -9165697322068360861
[11/01/2021-22:23:13] [V] [TRT] Tactic: -9165697322068360861 Time: 0.151152
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_small_nt_v1 Tactic: -9118785798277698619
[11/01/2021-22:23:13] [V] [TRT] Tactic: -9118785798277698619 Time: 0.076952
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: -8263994888336646547
[11/01/2021-22:23:13] [V] [TRT] Tactic: -8263994888336646547 Time: 0.154668
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -8205948405243401049
[11/01/2021-22:23:13] [V] [TRT] Tactic: -8205948405243401049 Time: 0.135608
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: -7992068592656168418
[11/01/2021-22:23:13] [V] [TRT] Tactic: -7992068592656168418 Time: 0.094548
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_epifadd Tactic: -7842775553137511386
[11/01/2021-22:23:13] [V] [TRT] Tactic: -7842775553137511386 Time: 0.090036
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: -7683887278997527517
[11/01/2021-22:23:13] [V] [TRT] Tactic: -7683887278997527517 Time: 0.139616
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_int8_i8816cudnn_int8_128x128_ldg16_relu_small_nt_v1 Tactic: -6400348606759295499
[11/01/2021-22:23:13] [V] [TRT] Tactic: -6400348606759295499 Time: 0.13172
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x128_ldg16_relu_small_nt_v1 Tactic: -5980889159865208399
[11/01/2021-22:23:13] [V] [TRT] Tactic: -5980889159865208399 Time: 0.163028
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_medium_nt_v1 Tactic: -5766140806760372989
[11/01/2021-22:23:13] [V] [TRT] Tactic: -5766140806760372989 Time: 0.079492
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: -5709079507616090666
[11/01/2021-22:23:13] [V] [TRT] Tactic: -5709079507616090666 Time: 0.150784
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: -5698636014239116282
[11/01/2021-22:23:13] [V] [TRT] Tactic: -5698636014239116282 Time: 0.267924
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -4933563390723451692
[11/01/2021-22:23:13] [V] [TRT] Tactic: -4933563390723451692 Time: 0.08654
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_medium_nt_v1 Tactic: -4516822589357530549
[11/01/2021-22:23:13] [V] [TRT] Tactic: -4516822589357530549 Time: 0.082304
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_epifadd Tactic: -3413217501222406256
[11/01/2021-22:23:13] [V] [TRT] Tactic: -3413217501222406256 Time: 0.147516
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -3238475748440751107
[11/01/2021-22:23:13] [V] [TRT] Tactic: -3238475748440751107 Time: 0.172032
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: -3182884991006484042
[11/01/2021-22:23:13] [V] [TRT] Tactic: -3182884991006484042 Time: 0.087956
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -3173468756112541306
[11/01/2021-22:23:13] [V] [TRT] Tactic: -3173468756112541306 Time: 0.126508
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x128_ldg16_relu_large_nt_v1 Tactic: -2917455979290586480
[11/01/2021-22:23:13] [V] [TRT] Tactic: -2917455979290586480 Time: 0.164164
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_int8_i8816cudnn_int8_128x128_ldg16_relu_large_nt_v1 Tactic: -2571022005763160364
[11/01/2021-22:23:13] [V] [TRT] Tactic: -2571022005763160364 Time: 0.141176
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: -2083778562631872334
[11/01/2021-22:23:13] [V] [TRT] Tactic: -2083778562631872334 Time: 0.099824
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -1546787387293556842
[11/01/2021-22:23:13] [V] [TRT] Tactic: -1546787387293556842 Time: 0.150784
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: -1498626619443284096
[11/01/2021-22:23:13] [V] [TRT] Tactic: -1498626619443284096 Time: 0.111876
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: -1283580231568512025
[11/01/2021-22:23:13] [V] [TRT] Tactic: -1283580231568512025 Time: 0.152208
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_epifadd Tactic: -1173968681844185579
[11/01/2021-22:23:13] [V] [TRT] Tactic: -1173968681844185579 Time: 0.154228
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: -762222380308749469
[11/01/2021-22:23:13] [V] [TRT] Tactic: -762222380308749469 Time: 0.091668
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_epifadd Tactic: -556794153877490941
[11/01/2021-22:23:13] [V] [TRT] Tactic: -556794153877490941 Time: 0.090384
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: -516725800067794372
[11/01/2021-22:23:13] [V] [TRT] Tactic: -516725800067794372 Time: 0.147828
[11/01/2021-22:23:13] [V] [TRT] Conv_44 Set Tactic Name: ampere_int8_i8816cudnn_int8_256x64_ldg16_relu_singleBuffer_large_nt_v1 Tactic: -428104331444385564
[11/01/2021-22:23:13] [V] [TRT] Tactic: -428104331444385564 Time: 0.08092
[11/01/2021-22:23:13] [V] [TRT] Fastest Tactic: -9118785798277698619 Time: 0.076952
[11/01/2021-22:23:13] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -9118785798277698619
[11/01/2021-22:23:13] [V] [TRT] *************** Autotuning Reformat:Float(7864320,122880,240,1) -> Int8(7864320:32,122880,240,1) ***************
[11/01/2021-22:23:13] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:13] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:13] [V] [TRT] Tactic: 1002 Time: 1.43698
[11/01/2021-22:23:13] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:13] [V] [TRT] Tactic: 0 Time: 1.13058
[11/01/2021-22:23:13] [V] [TRT] Fastest Tactic: 0 Time: 1.13058
[11/01/2021-22:23:13] [V] [TRT] *************** Autotuning Reformat:Float(7864320,1,15360,64) -> Float(7864320,122880,240,1) ***************
[11/01/2021-22:23:13] [V] [TRT] *************** Autotuning Reformat:Float(7864320,1,15360,64) -> Int8(7864320:32,122880,240,1) ***************
[11/01/2021-22:23:13] [V] [TRT] *************** Autotuning Reformat:Float(1966080,1:4,3840,16) -> Float(7864320,122880,240,1) ***************
[11/01/2021-22:23:13] [V] [TRT] *************** Autotuning Reformat:Float(1966080,1:4,3840,16) -> Int8(7864320:32,122880,240,1) ***************
[11/01/2021-22:23:13] [V] [TRT] *************** Autotuning Reformat:Float(245760,122880:32,240,1) -> Float(7864320,122880,240,1) ***************
[11/01/2021-22:23:13] [V] [TRT] *************** Autotuning Reformat:Float(245760,122880:32,240,1) -> Int8(7864320:32,122880,240,1) ***************
[11/01/2021-22:23:13] [V] [TRT] *************** Autotuning Reformat:Int8(1966080,122880:4,240,1) -> Float(7864320,122880,240,1) ***************
[11/01/2021-22:23:13] [V] [TRT] *************** Autotuning Reformat:Int8(1966080,122880:4,240,1) -> Int8(7864320:32,122880,240,1) ***************
[11/01/2021-22:23:13] [V] [TRT] *************** Autotuning Reformat:Int8(245760,122880:32,240,1) -> Float(7864320,122880,240,1) ***************
[11/01/2021-22:23:13] [V] [TRT] *************** Autotuning Reformat:Int8(245760,122880:32,240,1) -> Int8(7864320:32,122880,240,1) ***************
[11/01/2021-22:23:13] [V] [TRT] *************** Autotuning Reformat:Int8(7864320:32,122880,240,1) -> Float(7864320,122880,240,1) ***************
[11/01/2021-22:23:13] [V] [TRT] --------------- Timing Runner: Optimizer Reformat (Reformat)
[11/01/2021-22:23:13] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:13] [V] [TRT] Tactic: 1002 Time: 3.02782
[11/01/2021-22:23:13] [V] [TRT] Setting a default quantization params because quantization data is missing for 
[11/01/2021-22:23:13] [V] [TRT] Tactic: 0 Time: 0.684388
[11/01/2021-22:23:13] [V] [TRT] Fastest Tactic: 0 Time: 0.684388
[11/01/2021-22:23:13] [V] [TRT] *************** Autotuning format combination: Float(7864320,122880,240,1) -> Float(7864320,122880,240,1) ***************
[11/01/2021-22:23:13] [V] [TRT] --------------- Timing Runner: InstanceNormalization_47 (PluginV2)
[11/01/2021-22:23:13] [V] [TRT] Tactic: 0 Time: 0.199076
[11/01/2021-22:23:13] [V] [TRT] Fastest Tactic: 0 Time: 0.199076
[11/01/2021-22:23:13] [V] [TRT] >>>>>>>>>>>>>>> Chose Runner Type: PluginV2 Tactic: 0
[11/01/2021-22:23:13] [V] [TRT] *************** Autotuning format combination: Int8(7864320:32,122880,240,1) -> Int8(7864320:32,122880,240,1) ***************
[11/01/2021-22:23:13] [V] [TRT] --------------- Timing Runner: InstanceNormalization_47 (PluginV2)
[11/01/2021-22:23:13] [E] [TRT] instanceNormalizationPlugin.cu (292) - Cudnn Error in enqueue: 3 (CUDNN_STATUS_BAD_PARAM)
terminate called after throwing an instance of 'nvinfer1::plugin::CudnnError'
  what():  std::exception
Aborted (core dumped)